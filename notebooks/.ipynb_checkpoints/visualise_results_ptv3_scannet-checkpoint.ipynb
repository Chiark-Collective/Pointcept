{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d61c53-d178-48cf-81d3-7e2410c8dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing as ty\n",
    "from pathlib import Path\n",
    "\n",
    "import laspy\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pyntcloud import PyntCloud\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46d3b4d8-e014-4790-8be9-cc91f523afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to read PCD data - this needs to be implemented\n",
    "def read_pcd(file_path):\n",
    "    cloud = PyntCloud.from_file(\n",
    "        file_path\n",
    "    )\n",
    "    return cloud.points\n",
    "\n",
    "def create_pc_dataset(pc_dict: dict[str, ty.Any]) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Create an Xarray dataset from a point cloud dictionary.\n",
    "\n",
    "    Args:\n",
    "        pc_dict (Dict[str, Any]): A dictionary containing point cloud data with the following keys:\n",
    "            - 'coord': The point coordinates as a 2D numpy array of shape (num_points, 3).\n",
    "            - 'color': The point colors as a 2D numpy array of shape (num_points, 3).\n",
    "            - 'normal': The point normals as a 2D numpy array of shape (num_points, 3).\n",
    "            - 'semantic_gt20': The semantic labels (20 classes) as a 1D numpy array of shape (num_points,).\n",
    "            - 'semantic_gt200': The semantic labels (200 classes) as a 1D numpy array of shape (num_points,).\n",
    "            - 'instance_gt': The instance labels as a 1D numpy array of shape (num_points,).\n",
    "            - 'scene_id': The scene ID as a string.\n",
    "\n",
    "    Returns:\n",
    "        xr.Dataset: An Xarray dataset representing the point cloud data.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If any of the required keys are missing from the input dictionary.\n",
    "        ValueError: If the input arrays have inconsistent shapes or if the 'scene_id' is not a string.\n",
    "\n",
    "    Example:\n",
    "        pc_dict = {\n",
    "            'coord': np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]),\n",
    "            'color': np.array([[255, 0, 0], [0, 255, 0]]),\n",
    "            'normal': np.array([[0.0, 0.0, 1.0], [0.0, 1.0, 0.0]]),\n",
    "            'semantic_gt20': np.array([1, 2]),\n",
    "            'semantic_gt200': np.array([10, 20]),\n",
    "            'instance_gt': np.array([1, 1]),\n",
    "            'scene_id': 'scene_0001'\n",
    "        }\n",
    "        pc_dataset = create_pc_dataset(pc_dict)\n",
    "    \"\"\"\n",
    "    # Check if all required keys are present in the input dictionary\n",
    "    required_keys = [\n",
    "        'coord', 'color', 'normal', 'semantic_gt20', 'semantic_gt200', \n",
    "        'instance_gt', 'scene_id'\n",
    "    ]\n",
    "    for key in required_keys:\n",
    "        if key not in pc_dict:\n",
    "            raise KeyError(f\"Missing required key '{key}' in the input dict\")\n",
    "\n",
    "    # Check if the input arrays have consistent shapes\n",
    "    num_points = len(pc_dict['coord'])\n",
    "    for key in ['color', 'normal']:\n",
    "        if pc_dict[key].shape != (num_points, 3):\n",
    "            raise ValueError(f\"Array '{key}' has an inconsistent shape. Expected ({num_points}, 3).\")\n",
    "    for key in ['semantic_gt20', 'semantic_gt200', 'instance_gt']:\n",
    "        if pc_dict[key].shape != (num_points,):\n",
    "            raise ValueError(f\"Array '{key}' has an inconsistent shape. Expected ({num_points},).\")\n",
    "\n",
    "    # Create an Xarray dataset from the point cloud dictionary\n",
    "    pc_dataset = xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            coord=(['point', 'coord_dim'], pc_dict['coord']),\n",
    "            color=(['point', 'color_dim'], pc_dict['color']),\n",
    "            normal=(['point', 'normal_dim'], pc_dict['normal']),\n",
    "            semantic_gt20=(['point'], pc_dict['semantic_gt20']),\n",
    "            semantic_gt200=(['point'], pc_dict['semantic_gt200']),\n",
    "            instance_gt=(['point'], pc_dict['instance_gt'])\n",
    "        ),\n",
    "        coords=dict(\n",
    "            point=np.arange(num_points),\n",
    "            coord_dim=['x', 'y', 'z'],\n",
    "            color_dim=['r', 'g', 'b'],\n",
    "            normal_dim=['nx', 'ny', 'nz']\n",
    "        ),\n",
    "        attrs=dict(scene_id=pc_dict['scene_id'])\n",
    "    )\n",
    "\n",
    "    return pc_dataset\n",
    "\n",
    "\n",
    "# Define the mappings from Polars to PCD data types\n",
    "POLARS_PCD_TYPE_MAPPINGS = [\n",
    "    (pl.Float32, ('F', 4)),\n",
    "    (pl.Float64, ('F', 8)),\n",
    "    (pl.UInt8, ('U', 1)),\n",
    "    (pl.UInt16, ('U', 2)),\n",
    "    (pl.UInt32, ('U', 4)),\n",
    "    (pl.UInt64, ('U', 8)),\n",
    "    (pl.Int16, ('I', 2)),\n",
    "    (pl.Int32, ('I', 4)),\n",
    "    (pl.Int64, ('I', 8)),\n",
    "]\n",
    "POLARS_TYPE_TO_PCD_TYPE = {dtype: mapping for dtype, mapping in POLARS_PCD_TYPE_MAPPINGS}\n",
    "\n",
    "\n",
    "def write_pcd(filename, data, metadata=None, batch_size=500000, binary=False):\n",
    "    \"\"\"\n",
    "    Writes a PCD file from a Polars DataFrame or LazyFrame in batches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        Path to the output PCD file.\n",
    "    data: Polars DataFrame or LazyFrame\n",
    "        DataFrame or LazyFrame containing the point cloud data.\n",
    "    metadata: dict, optional\n",
    "        Dictionary containing PCD metadata. If not provided, it will be\n",
    "        generated from the data.\n",
    "    batch_size: int, optional\n",
    "        Size of each batch to be processed.\n",
    "    binary: bool, optional\n",
    "        If True, writes the PCD file in binary format. Otherwise, writes in ASCII format.\n",
    "    \"\"\"\n",
    "    # Check if data is a LazyFrame\n",
    "    is_lazy = isinstance(data, pl.LazyFrame)\n",
    "\n",
    "    # Rename columns if they exist\n",
    "    # rename_map = {'X': 'x', 'Y': 'y', 'Z': 'z'}\n",
    "    # data = data.rename(rename_map)\n",
    "\n",
    "    # Fill nans for eigenentropy\n",
    "    if is_lazy:\n",
    "        data = data.with_columns([\n",
    "            pl.col(\"x\").cast(pl.Float32),  # open3d seems to req single precision float\n",
    "            pl.col(\"y\").cast(pl.Float32),\n",
    "            pl.col(\"z\").cast(pl.Float32)\n",
    "        ])\n",
    "        data = data.select(pl.exclude(pl.String))\n",
    "    else:\n",
    "        data = data.with_columns([\n",
    "            pl.col(\"x\").cast(pl.Float32),  # open3d seems to req single precision float\n",
    "            pl.col(\"y\").cast(pl.Float32),\n",
    "            pl.col(\"z\").cast(pl.Float32)\n",
    "        ])\n",
    "        data = data.select(pl.exclude(pl.Utf8))\n",
    "\n",
    "    if metadata is None:\n",
    "        n_rows = data.height if not is_lazy else data.select(pl.col(\"*\")).count().item()\n",
    "        # Generate metadata from the first batch of data\n",
    "        first_batch = data.limit(batch_size).collect() if is_lazy else data.head(batch_size)\n",
    "        metadata = {\n",
    "            'version': '.7',\n",
    "            'fields': first_batch.columns,\n",
    "            'size': [POLARS_TYPE_TO_PCD_TYPE[dtype][1] for dtype in first_batch.dtypes],\n",
    "            'type': [POLARS_TYPE_TO_PCD_TYPE[dtype][0] for dtype in first_batch.dtypes],\n",
    "            'count': [1] * len(first_batch.columns),\n",
    "            'width': n_rows,\n",
    "            'height': 1,\n",
    "            'viewpoint': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "            'points': n_rows,\n",
    "            'data': 'binary' if binary else 'ascii'\n",
    "        }\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        # Write metadata\n",
    "        f.write(f\"VERSION {metadata['version']}\\n\".encode())\n",
    "        f.write(f\"FIELDS {' '.join(metadata['fields'])}\\n\".encode())\n",
    "        f.write(f\"SIZE {' '.join(map(str, metadata['size']))}\\n\".encode())\n",
    "        f.write(f\"TYPE {' '.join(metadata['type'])}\\n\".encode())\n",
    "        f.write(f\"COUNT {' '.join(map(str, metadata['count']))}\\n\".encode())\n",
    "        f.write(f\"WIDTH {metadata['width']}\\n\".encode())\n",
    "        f.write(f\"HEIGHT {metadata['height']}\\n\".encode())\n",
    "        f.write(f\"VIEWPOINT {' '.join(map(str, metadata['viewpoint']))}\\n\".encode())\n",
    "        f.write(f\"POINTS {metadata['points']}\\n\".encode())\n",
    "        f.write(f\"DATA {metadata['data']}\\n\".encode())\n",
    "\n",
    "        # Process and write data in batches\n",
    "        for start_row in range(0, metadata[\"points\"], batch_size):\n",
    "            batch_df = data.slice(start_row, batch_size).collect() if is_lazy else data[start_row:start_row+batch_size]\n",
    "            if binary:\n",
    "                # Write batch in binary format\n",
    "                data_buffer = batch_df.to_pandas().to_records(index=False).tobytes()\n",
    "                f.write(data_buffer)\n",
    "            else:\n",
    "                # Write batch in ASCII format\n",
    "                np.savetxt(f, batch_df.to_numpy(), fmt=' '.join(['%s'] * batch_df.width))\n",
    "\n",
    "    return filename\n",
    "\n",
    "def convert_pcd_to_las(\n",
    "    pcd_file_path: Path,\n",
    "    las_file_path: Path,\n",
    "    pred_col: str = \"pred\",\n",
    "    gt_col: str = \"gt\"\n",
    ") -> Path:\n",
    "    # Read the PCD file\n",
    "    pcd_data = read_pcd(str(pcd_file_path))\n",
    "\n",
    "    # Create a new LAS file header\n",
    "    header = laspy.LasHeader(version=\"1.4\", point_format=2)\n",
    "    \n",
    "    # Add extra dimensions for predicted_class and predicted_proba\n",
    "    header.add_extra_dim(laspy.ExtraBytesParams(name=\"predicted_class\", type=\"f\"))\n",
    "    header.add_extra_dim(laspy.ExtraBytesParams(name=\"gt_class\", type=\"f\"))\n",
    "    \n",
    "    # Create an empty LAS file with the header\n",
    "    las = laspy.LasData(header)\n",
    "\n",
    "    # Fill the LAS file with data\n",
    "    las.x, las.y, las.z = pcd_data['x'], pcd_data['y'], pcd_data['z']\n",
    "    if \"intensity\" in pcd_data.columns:\n",
    "        las.intensity = pcd_data['intensity']\n",
    "    las.red, las.green, las.blue = pcd_data[\"r\"], pcd_data[\"g\"], pcd_data[\"b\"]\n",
    "\n",
    "    # Set values for the extra dimensions\n",
    "    las.predicted_class = pcd_data[pred_col]\n",
    "    las.gt_class = pcd_data[gt_col]\n",
    "\n",
    "    # Write the LAS file to disk\n",
    "    las.write(las_file_path)\n",
    "    return las_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb3dd6-64fc-4b88-a567-042ae8fb37fe",
   "metadata": {},
   "source": [
    "Load the scenes and the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a572b0be-0e74-4008-b4b3-d0339689d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = Path(\"..\")\n",
    "\n",
    "results_dir = (\n",
    "    repo_root / \"exp/scannet/semseg-pt-v3m1-1-ppt-extreme/result\"\n",
    ")\n",
    "scenes_dir = (\n",
    "    repo_root / \"data/scannet/val\"\n",
    ")\n",
    "\n",
    "predictions = {\n",
    "    path.stem.rstrip(\"_pred\"): np.load(path) \n",
    "    for path in results_dir.glob(\"*.npy\")\n",
    "}\n",
    "scenes = {\n",
    "    path.stem: torch.load(path) \n",
    "    for path in scenes_dir.glob(\"*.pth\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76408e2f-9e66-432b-9821-24710901387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa', 'table', 'door',\n",
    "    'window', 'bookshelf', 'picture', 'counter', 'desk', 'curtain',\n",
    "    'refridgerator', 'shower curtain', 'toilet', 'sink', 'bathtub',\n",
    "    'otherfurniture'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743b9bc-22cc-49d2-9920-a5d8a4b2e98a",
   "metadata": {},
   "source": [
    "Write everything out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "637712e6-5361-465b-b70a-8f3f99f5254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write: scene0144_00.las\n",
      "Write: scene0131_00.las\n",
      "Write: scene0671_00.las\n",
      "Write: scene0559_00.las\n",
      "Write: scene0633_01.las\n",
      "Write: scene0500_00.las\n",
      "Write: scene0696_02.las\n",
      "Write: scene0685_00.las\n",
      "Write: scene0574_02.las\n",
      "Write: scene0164_00.las\n",
      "Write: scene0606_00.las\n",
      "Write: scene0249_00.las\n",
      "Write: scene0207_00.las\n",
      "Write: scene0645_00.las\n",
      "Write: scene0356_00.las\n",
      "Write: scene0095_01.las\n",
      "Write: scene0695_03.las\n",
      "Write: scene0081_02.las\n",
      "Write: scene0131_01.las\n",
      "Write: scene0426_01.las\n",
      "Write: scene0435_02.las\n",
      "Write: scene0153_00.las\n",
      "Write: scene0193_00.las\n",
      "Write: scene0196_00.las\n",
      "Write: scene0435_00.las\n",
      "Write: scene0575_00.las\n",
      "Write: scene0334_01.las\n",
      "Write: scene0686_00.las\n",
      "Write: scene0357_00.las\n",
      "Write: scene0389_00.las\n",
      "Write: scene0651_01.las\n",
      "Write: scene0217_00.las\n",
      "Write: scene0496_00.las\n",
      "Write: scene0423_01.las\n",
      "Write: scene0334_02.las\n",
      "Write: scene0063_00.las\n",
      "Write: scene0423_00.las\n",
      "Write: scene0378_00.las\n",
      "Write: scene0494_00.las\n",
      "Write: scene0599_00.las\n",
      "Write: scene0629_00.las\n",
      "Write: scene0575_02.las\n",
      "Write: scene0670_01.las\n",
      "Write: scene0574_01.las\n",
      "Write: scene0193_01.las\n",
      "Write: scene0046_02.las\n",
      "Write: scene0704_01.las\n",
      "Write: scene0427_00.las\n",
      "Write: scene0164_01.las\n",
      "Write: scene0025_01.las\n",
      "Write: scene0208_00.las\n",
      "Write: scene0474_04.las\n",
      "Write: scene0621_00.las\n",
      "Write: scene0169_00.las\n",
      "Write: scene0084_00.las\n",
      "Write: scene0086_01.las\n",
      "Write: scene0382_00.las\n",
      "Write: scene0591_02.las\n",
      "Write: scene0187_00.las\n",
      "Write: scene0382_01.las\n",
      "Write: scene0553_00.las\n",
      "Write: scene0207_02.las\n",
      "Write: scene0693_00.las\n",
      "Write: scene0553_01.las\n",
      "Write: scene0591_00.las\n",
      "Write: scene0645_01.las\n",
      "Write: scene0406_01.las\n",
      "Write: scene0064_00.las\n",
      "Write: scene0696_01.las\n",
      "Write: scene0702_00.las\n",
      "Write: scene0474_05.las\n",
      "Write: scene0644_00.las\n",
      "Write: scene0377_00.las\n",
      "Write: scene0095_00.las\n",
      "Write: scene0338_00.las\n",
      "Write: scene0164_02.las\n",
      "Write: scene0591_01.las\n",
      "Write: scene0700_02.las\n",
      "Write: scene0670_00.las\n",
      "Write: scene0231_02.las\n",
      "Write: scene0695_00.las\n",
      "Write: scene0256_01.las\n",
      "Write: scene0050_01.las\n",
      "Write: scene0351_00.las\n",
      "Write: scene0609_00.las\n",
      "Write: scene0599_01.las\n",
      "Write: scene0558_02.las\n",
      "Write: scene0535_00.las\n",
      "Write: scene0377_02.las\n",
      "Write: scene0678_02.las\n",
      "Write: scene0221_00.las\n",
      "Write: scene0316_00.las\n",
      "Write: scene0144_01.las\n",
      "Write: scene0300_00.las\n",
      "Write: scene0203_02.las\n",
      "Write: scene0609_03.las\n",
      "Write: scene0655_02.las\n",
      "Write: scene0559_01.las\n",
      "Write: scene0149_00.las\n",
      "Write: scene0222_01.las\n",
      "Write: scene0334_00.las\n",
      "Write: scene0693_02.las\n",
      "Write: scene0689_00.las\n",
      "Write: scene0423_02.las\n",
      "Write: scene0629_01.las\n",
      "Write: scene0246_00.las\n",
      "Write: scene0693_01.las\n",
      "Write: scene0406_00.las\n",
      "Write: scene0207_01.las\n",
      "Write: scene0342_00.las\n",
      "Write: scene0355_01.las\n",
      "Write: scene0651_02.las\n",
      "Write: scene0430_00.las\n",
      "Write: scene0664_02.las\n",
      "Write: scene0608_00.las\n",
      "Write: scene0441_00.las\n",
      "Write: scene0652_00.las\n",
      "Write: scene0488_01.las\n",
      "Write: scene0549_00.las\n",
      "Write: scene0616_00.las\n",
      "Write: scene0088_02.las\n",
      "Write: scene0329_02.las\n",
      "Write: scene0697_00.las\n",
      "Write: scene0607_01.las\n",
      "Write: scene0598_02.las\n",
      "Write: scene0593_00.las\n",
      "Write: scene0300_01.las\n",
      "Write: scene0701_02.las\n",
      "Write: scene0329_01.las\n",
      "Write: scene0046_00.las\n",
      "Write: scene0314_00.las\n",
      "Write: scene0558_01.las\n",
      "Write: scene0426_02.las\n",
      "Write: scene0655_00.las\n",
      "Write: scene0578_01.las\n",
      "Write: scene0648_01.las\n",
      "Write: scene0088_01.las\n",
      "Write: scene0500_01.las\n",
      "Write: scene0354_00.las\n",
      "Write: scene0653_00.las\n",
      "Write: scene0169_01.las\n",
      "Write: scene0307_02.las\n",
      "Write: scene0618_00.las\n",
      "Write: scene0307_00.las\n",
      "Write: scene0356_02.las\n",
      "Write: scene0647_00.las\n",
      "Write: scene0686_02.las\n",
      "Write: scene0086_00.las\n",
      "Write: scene0593_01.las\n",
      "Write: scene0461_00.las\n",
      "Write: scene0088_00.las\n",
      "Write: scene0458_01.las\n",
      "Write: scene0256_02.las\n",
      "Write: scene0607_00.las\n",
      "Write: scene0081_01.las\n",
      "Write: scene0353_01.las\n",
      "Write: scene0643_00.las\n",
      "Write: scene0695_02.las\n",
      "Write: scene0084_02.las\n",
      "Write: scene0595_00.las\n",
      "Write: scene0030_01.las\n",
      "Write: scene0356_01.las\n",
      "Write: scene0616_01.las\n",
      "Write: scene0015_00.las\n",
      "Write: scene0307_01.las\n",
      "Write: scene0606_02.las\n",
      "Write: scene0549_01.las\n",
      "Write: scene0697_01.las\n",
      "Write: scene0064_01.las\n",
      "Write: scene0353_00.las\n",
      "Write: scene0351_01.las\n",
      "Write: scene0664_01.las\n",
      "Write: scene0426_03.las\n",
      "Write: scene0278_01.las\n",
      "Write: scene0696_00.las\n",
      "Write: scene0665_00.las\n",
      "Write: scene0377_01.las\n",
      "Write: scene0645_02.las\n",
      "Write: scene0633_00.las\n",
      "Write: scene0574_00.las\n",
      "Write: scene0100_01.las\n",
      "Write: scene0146_02.las\n",
      "Write: scene0077_00.las\n",
      "Write: scene0153_01.las\n",
      "Write: scene0701_00.las\n",
      "Write: scene0146_00.las\n",
      "Write: scene0222_00.las\n",
      "Write: scene0278_00.las\n",
      "Write: scene0474_00.las\n",
      "Write: scene0685_02.las\n",
      "Write: scene0575_01.las\n",
      "Write: scene0277_02.las\n",
      "Write: scene0648_00.las\n",
      "Write: scene0050_02.las\n",
      "Write: scene0088_03.las\n",
      "Write: scene0355_00.las\n",
      "Write: scene0474_03.las\n",
      "Write: scene0629_02.las\n",
      "Write: scene0553_02.las\n",
      "Write: scene0304_00.las\n",
      "Write: scene0329_00.las\n",
      "Write: scene0426_00.las\n",
      "Write: scene0353_02.las\n",
      "Write: scene0490_00.las\n",
      "Write: scene0660_00.las\n",
      "Write: scene0474_01.las\n",
      "Write: scene0518_00.las\n",
      "Write: scene0338_01.las\n",
      "Write: scene0568_00.las\n",
      "Write: scene0684_01.las\n",
      "Write: scene0131_02.las\n",
      "Write: scene0139_00.las\n",
      "Write: scene0651_00.las\n",
      "Write: scene0671_01.las\n",
      "Write: scene0019_00.las\n",
      "Write: scene0343_00.las\n",
      "Write: scene0146_01.las\n",
      "Write: scene0432_01.las\n",
      "Write: scene0081_00.las\n",
      "Write: scene0046_01.las\n",
      "Write: scene0697_03.las\n",
      "Write: scene0609_02.las\n",
      "Write: scene0430_01.las\n",
      "Write: scene0462_00.las\n",
      "Write: scene0552_01.las\n",
      "Write: scene0100_02.las\n",
      "Write: scene0599_02.las\n",
      "Write: scene0700_00.las\n",
      "Write: scene0256_00.las\n",
      "Write: scene0580_01.las\n",
      "Write: scene0011_01.las\n",
      "Write: scene0412_01.las\n",
      "Write: scene0025_00.las\n",
      "Write: scene0164_03.las\n",
      "Write: scene0658_00.las\n",
      "Write: scene0187_01.las\n",
      "Write: scene0655_01.las\n",
      "Write: scene0025_02.las\n",
      "Write: scene0231_00.las\n",
      "Write: scene0686_01.las\n",
      "Write: scene0653_01.las\n",
      "Write: scene0702_01.las\n",
      "Write: scene0690_01.las\n",
      "Write: scene0663_00.las\n",
      "Write: scene0663_02.las\n",
      "Write: scene0030_02.las\n",
      "Write: scene0203_00.las\n",
      "Write: scene0277_01.las\n",
      "Write: scene0558_00.las\n",
      "Write: scene0550_00.las\n",
      "Write: scene0328_00.las\n",
      "Write: scene0378_01.las\n",
      "Write: scene0583_01.las\n",
      "Write: scene0699_00.las\n",
      "Write: scene0704_00.las\n",
      "Write: scene0690_00.las\n",
      "Write: scene0432_00.las\n",
      "Write: scene0606_01.las\n",
      "Write: scene0221_01.las\n",
      "Write: scene0565_00.las\n",
      "Write: scene0086_02.las\n",
      "Write: scene0665_01.las\n",
      "Write: scene0701_01.las\n",
      "Write: scene0684_00.las\n",
      "Write: scene0277_00.las\n",
      "Write: scene0458_00.las\n",
      "Write: scene0702_02.las\n",
      "Write: scene0488_00.las\n",
      "Write: scene0552_00.las\n",
      "Write: scene0678_00.las\n",
      "Write: scene0257_00.las\n",
      "Write: scene0700_01.las\n",
      "Write: scene0011_00.las\n",
      "Write: scene0338_02.las\n",
      "Write: scene0251_00.las\n",
      "Write: scene0608_02.las\n",
      "Write: scene0412_00.las\n",
      "Write: scene0608_01.las\n",
      "Write: scene0435_03.las\n",
      "Write: scene0527_00.las\n",
      "Write: scene0678_01.las\n",
      "Write: scene0050_00.las\n",
      "Write: scene0559_02.las\n",
      "Write: scene0598_01.las\n",
      "Write: scene0583_02.las\n",
      "Write: scene0077_01.las\n",
      "Write: scene0474_02.las\n",
      "Write: scene0568_01.las\n",
      "Write: scene0019_01.las\n",
      "Write: scene0100_00.las\n",
      "Write: scene0583_00.las\n",
      "Write: scene0203_01.las\n",
      "Write: scene0580_00.las\n",
      "Write: scene0695_01.las\n",
      "Write: scene0647_01.las\n",
      "Write: scene0578_02.las\n",
      "Write: scene0231_01.las\n",
      "Write: scene0685_01.las\n",
      "Write: scene0609_01.las\n",
      "Write: scene0414_00.las\n",
      "Write: scene0406_02.las\n",
      "Write: scene0578_00.las\n",
      "Write: scene0084_01.las\n",
      "Write: scene0664_00.las\n",
      "Write: scene0598_00.las\n",
      "Write: scene0030_00.las\n",
      "Write: scene0378_02.las\n",
      "Write: scene0568_02.las\n",
      "Write: scene0663_01.las\n",
      "Write: scene0357_01.las\n",
      "Write: scene0697_02.las\n",
      "Write: scene0435_01.las\n"
     ]
    }
   ],
   "source": [
    "for scene_id, pred in predictions.items():\n",
    "    points = scenes[scene_id]\n",
    "    labels = points[\"semantic_gt20\"]\n",
    "    ds = create_pc_dataset(points)\n",
    "    ds = ds.assign_coords(classes_scannet=xr.DataArray(np.array(classes), dims=[\"classes_scannet\"]))\n",
    "    ds[\"pred\"] =((\"point\",), pred)\n",
    "    ds[\"color\"] = ds[\"color\"].astype(np.uint8)\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            ds[\"coord\"].to_pandas(),\n",
    "            ds[\"color\"].to_pandas(),\n",
    "            ds[\"pred\"].to_pandas().rename(\"pred\"),\n",
    "            ds[\"semantic_gt20\"].to_pandas().rename(\"gt\")\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    lf = pl.from_pandas(df)\n",
    "    pcd_out = f\"{scene_id}.pcd\"\n",
    "    las_out = f\"{scene_id}.las\"\n",
    "    write_pcd(pcd_out, lf, binary=True)\n",
    "    print(f\"Write: {las_out}\")\n",
    "    convert_pcd_to_las(pcd_out, las_out)\n",
    "    os.remove(pcd_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc8ddf-db02-4aa8-bdfa-812fa1e017a4",
   "metadata": {},
   "source": [
    "Confusion matrix stuff\n",
    "\n",
    "need to figure out labelling first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e90cba4-2d72-46cc-bb6b-17b7b5e6ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df = pd.DataFrame(dict(pred=pred, gt=labels))\n",
    "    \n",
    "    # Extract the predicted and ground truth labels from the DataFrame\n",
    "    y_pred = df['pred']\n",
    "    y_true = df['gt']\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))  # Adjust the figure size as needed\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    \n",
    "    # Add a color bar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Define the class names\n",
    "    #classes = class_name\n",
    "    classes = [\n",
    "        'wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa', 'table', 'door',\n",
    "        'window', 'bookshelf', 'picture', 'counter', 'desk', 'curtain',\n",
    "        'refridgerator', 'shower curtain', 'toilet', 'sink', 'bathtub',\n",
    "        'otherfurniture'\n",
    "    ]\n",
    "    \n",
    "    # Set the tick labels and positions\n",
    "    ax.set(xticks=np.arange(len(classes)),\n",
    "           yticks=np.arange(len(classes)),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           xlabel='Predicted Labels',\n",
    "           ylabel='True Labels')\n",
    "    \n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Loop over data dimensions and create text annotations\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    \n",
    "    # Adjust the layout and display the plot\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbd15ce0-d301-4dba-b34c-e8184d6eef1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_output.pcd'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
