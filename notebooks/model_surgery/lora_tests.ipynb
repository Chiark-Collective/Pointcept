{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97aa1631-bdf1-446f-8a7a-12eb0de74245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-06 17:53:23,934 INFO test.py line 41 131730] => Loading config ...\n",
      "[2024-09-06 17:53:23,935 INFO test.py line 48 131730] => Building model ...\n",
      "[2024-09-06 17:53:26,779 INFO test.py line 61 131730] Num params: 97447088\n",
      "[2024-09-06 17:53:27,016 INFO test.py line 68 131730] Loading weight at: ../../models/PointTransformerV3/scannet-semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth\n",
      "[2024-09-06 17:53:27,580 INFO test.py line 84 131730] => Loaded weight '../../models/PointTransformerV3/scannet-semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth' (epoch 94)\n",
      "[2024-09-06 17:53:27,584 INFO test.py line 53 131730] => Building test dataset & dataloader ...\n",
      "[2024-09-06 17:53:27,586 INFO scannet.py line 72 131730] Totally 0 x 1 samples in val set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "import logging\n",
    "import typing as ty\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "import graphviz\n",
    "import minlora\n",
    "from minlora import (\n",
    "    LoRAParametrization,\n",
    "    add_lora,\n",
    "    merge_lora,\n",
    "    remove_lora\n",
    ")\n",
    "from minlora.utils import get_params_by_name, name_is_lora\n",
    "from minlora.model import add_lora_by_name, apply_lora\n",
    "from torch.optim import AdamW\n",
    "from spconv.pytorch.conv import SubMConv3d\n",
    "graphviz.set_jupyter_format('svg')\n",
    "from lora_pytorch import LoRA\n",
    "assert torch.cuda.is_available()\n",
    "from torchview import draw_graph\n",
    "from torchviz import make_dot\n",
    "from graphviz import Digraph\n",
    "\n",
    "from pointcept.engines.defaults import (\n",
    "    default_argument_parser,\n",
    "    default_config_parser,\n",
    "    default_setup,\n",
    ")\n",
    "from pointcept.engines.test import TESTERS\n",
    "from pointcept.engines.launch import launch\n",
    "from pointcept.engines.test import TesterBase, SemSegTester\n",
    "from pointcept.models.point_prompt_training import PDNorm\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "repo_root = Path(\"../..\")\n",
    "\n",
    "\n",
    "class WeightFreezer:\n",
    "    \"\"\"\n",
    "    Utility class for conditional, invertible freezing/unfreezing of model \n",
    "    weights with state tracking\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module) -> None:\n",
    "        self.model = model\n",
    "        self.original_states = {}\n",
    "        self._store_initial_states()\n",
    "        \n",
    "    def _store_initial_states(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            self.original_states[name] = param.requires_grad\n",
    "\n",
    "    def freeze_if(self, filter_fn: ty.Callable[[str, nn.Parameter], bool] | None) -> None:\n",
    "        filter_fn = filter_fn or (lambda n, p: True)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if filter_fn(name, param):\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def freeze_all(self) -> None:\n",
    "        return self.freeze_if(filter_fn=None)\n",
    "\n",
    "    def unfreeze_if(\n",
    "        self,\n",
    "        filter_fn: ty.Callable[[str, nn.Parameter], bool] | None, \n",
    "        hard: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Defaults to restoring to original state if the filter_fn returns True,\n",
    "        meaning if the initial model had certain parameters frozen, these will \n",
    "        faithfully still be frozen. Setting hard=True overrides this and unfreezes\n",
    "        irrespective of the initial state.\n",
    "        \"\"\"\n",
    "        filter_fn = filter_fn or (lambda n, p: True)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if filter_fn(name, param):\n",
    "                if hard:\n",
    "                    param.requires_grad = True\n",
    "                else:\n",
    "                    param.requires_grad = self.original_states.get(name, True)\n",
    "\n",
    "    def unfreeze_all(self, hard: bool = False) -> None:\n",
    "        return self.unfreeze_if(filter_fn=None, hard=hard)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.requires_grad = self.original_states.get(name, True)\n",
    "\n",
    "    def print_frozen_status(self, print_unfrozen: bool = False) -> None:\n",
    "        for name, param in self.model.named_parameters():\n",
    "            state = \"unfrozen\" if param.requires_grad else \"frozen\"\n",
    "            if state == \"unfrozen\" and not print_unfrozen:\n",
    "                continue\n",
    "            print(f\"{name}: {state}\")\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return dict(\n",
    "        trainable=sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        frozen=sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    )\n",
    "\n",
    "\n",
    "def named_trainable_parameters(model):\n",
    "    return dict(\n",
    "        trainable=[n for n, p in model.named_parameters() if p.requires_grad],\n",
    "        frozen=[n for n, p in model.named_parameters() if not p.requires_grad]\n",
    "    )\n",
    "\n",
    "def is_lora(name: str, value: nn.Parameter) -> bool:\n",
    "    return name_is_lora(name)\n",
    "\n",
    "\n",
    "def filter_named_params(\n",
    "    model: nn.Module,\n",
    "    filter_fn: ty.Callable[[str, nn.Parameter], bool] | None\n",
    ") -> tuple[str, nn.Parameter]:\n",
    "    \"\"\"\n",
    "    generator which returns (parameter_name, weight tensor)\n",
    "    for all tensors whose names match the filter function\n",
    "    \"\"\"\n",
    "    for n, p in model.named_parameters():\n",
    "        if filter_fn is None or filter_fn(n, p):\n",
    "            yield n, p\n",
    "\n",
    "\n",
    "get_named_lora_params = partial(filter_named_params, filter_fn=is_lora)\n",
    "get_named_non_lora_params = partial(filter_named_params, filter_fn=(lambda x: not is_lora(x)))\n",
    "\n",
    "\n",
    "def count_lora_parameters(model):\n",
    "    \"\"\"use minlora directly\"\"\"\n",
    "    return sum(p.numel() for p in minlora.get_lora_params(model))\n",
    "\n",
    "\n",
    "def count_lora_params_manual(model):\n",
    "    \"\"\"just looking at weight tensor names manually as a cross check\"\"\"\n",
    "    return sum(p.numel() for n, p in get_named_lora_params(model))\n",
    "\n",
    "\n",
    "def assert_lora_trainable(model):\n",
    "    for param in minlora.get_lora_params(model):\n",
    "        assert param.requires_grad\n",
    "\n",
    "\n",
    "def configure_adamw_lora(\n",
    "    model,\n",
    "    weight_decay: float = 0.05,\n",
    "    learning_rate: float = 0.005,\n",
    "    betas: tuple[float, float] = (0.9, 0.999),\n",
    "    device_type: str = \"cuda\"\n",
    ") -> torch.optim.AdamW:\n",
    "    \"\"\"\n",
    "    Create an AdamW optimiser which targets only LoRA parameters during\n",
    "    gradient descent\n",
    "    \"\"\"\n",
    "    # apply weight decay to all lora params\n",
    "    optim_groups = [\n",
    "        {\"params\": list(minlora.get_lora_params(model)) , \"weight_decay\": weight_decay},\n",
    "        # could also add biases for fine-tuning,\n",
    "        # {\"params\": minlora.get_bias_params(model), \"weight_decay\": 0.0}, # bias params don't get weight decay\n",
    "    ]\n",
    "\n",
    "    def parameter_count(optim_groups):\n",
    "        n = sum(p.numel() for group in optim_groups for p in group[\"params\"])\n",
    "        if n < 1e6:\n",
    "            return f\"{n/1e3:.1f}k\"\n",
    "        else:\n",
    "            return f\"{n/1e6:.1f}M\"\n",
    "\n",
    "    logger.info(f\"Optimizing {parameter_count(optim_groups)} parameters\")\n",
    "\n",
    "    # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n",
    "    use_fused = (device_type == \"cuda\") and (\"fused\" in inspect.signature(torch.optim.AdamW).parameters)\n",
    "    logger.info(f\"Using fused AdamW: {use_fused}\")\n",
    "    extra_args = dict(fused=True) if use_fused else dict()\n",
    "    return torch.optim.AdamW(\n",
    "        optim_groups,\n",
    "        lr=learning_rate,\n",
    "        betas=betas,\n",
    "        **extra_args\n",
    "    )\n",
    "\n",
    "\n",
    "def total_optimized_params(optimizer: torch.optim.Optimizer) -> int:\n",
    "    tot = 0\n",
    "    for param_group in optimizer.param_groups:\n",
    "        for param in param_group[\"params\"]:\n",
    "            tot += param.numel()\n",
    "    return tot\n",
    "\n",
    "    \n",
    "def create_spoofed_input(batch_size=2, num_points=1000, n_classes=5, num_features=6, device='cpu'):\n",
    "    return {\n",
    "        'coord': torch.rand(num_points * batch_size, num_features, device=device),\n",
    "        'feat': torch.rand(num_points * batch_size, num_features, device=device),\n",
    "        'grid_coord': torch.randint(0, 100, (num_points * batch_size, 3), device=device),\n",
    "        'batch': torch.arange(batch_size, device=device).repeat_interleave(num_points),\n",
    "        'offset': torch.tensor([num_points * i for i in range(1, batch_size + 1)], device=device),\n",
    "        'condition': ['ScanNet'] * batch_size,\n",
    "        'grid_size': torch.tensor([0.01], device=device),\n",
    "        'segment': torch.randint(low=0, high=n_classes-1, size=(num_points * batch_size,), device=device)\n",
    "    }\n",
    "\n",
    "\n",
    "def patch_cfg(cfg: dict, repo_root: Path = repo_root) -> dict:\n",
    "    cfg = cfg.copy()\n",
    "    cfg[\"my_data_root\"] = repo_root / cfg[\"my_data_root\"]\n",
    "    cfg[\"weight\"] = repo_root / cfg[\"weight\"]\n",
    "    cfg[\"batch_size_test_per_gpu\"] = 1\n",
    "    return cfg\n",
    "\n",
    "\n",
    "repo_root = Path(\"../..\")\n",
    "cfg_file = Path(\"../../test/custom-ppt-config.py\"); assert cfg_file.exists\n",
    "device = \"cuda\"\n",
    "\n",
    "args = default_argument_parser().parse_args(args=[\"--config-file\", f\"{cfg_file}\"])\n",
    "cfg = default_config_parser(args.config_file, args.options); cfg = patch_cfg(cfg)\n",
    "\n",
    "tester = TESTERS.build(dict(type=cfg.test.type, cfg=cfg))\n",
    "model = tester.model\n",
    "model.to(device)\n",
    "\n",
    "# make this once at start, otherwise i gotta make it a singleton to \n",
    "# avoid subsequent runs redefining the \"initial state\"\n",
    "wf = WeightFreezer(model) \n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7334aa9-fc21-4a47-816f-a87500e18f7e",
   "metadata": {},
   "source": [
    "# Inject new normalisation layers for new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c39bd6-0fa5-4d47-acfb-9ba313a64b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_ppt_model_conditions(\n",
    "    model: nn.Module,\n",
    "    new_conditions: list[str],\n",
    "    condition_mapping: dict[str, str] = None\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Expands a trained PPT model to handle new conditions (datasets). The appropriate \n",
    "    normalisation layers are either copied from the trained norm layers corresponding \n",
    "    to existing datasets or are initialised randomly (as specified by condition_mapping).\n",
    "    \n",
    "    Args:\n",
    "    - model: The trained PPT model\n",
    "    - new_conditions: List of new condition names to add\n",
    "    - condition_mapping: dict mapping new conditions to existing ones for weight initialisation\n",
    "    \n",
    "    Returns:\n",
    "    - Updated model with expanded normalisation layers\n",
    "    \"\"\"\n",
    "    if condition_mapping is None:\n",
    "        condition_mapping = {}\n",
    "    for condition in new_conditions:\n",
    "        if condition not in condition_mapping:\n",
    "            condition_mapping[condition] = None\n",
    "\n",
    "    original_conditions = model.conditions\n",
    "    model.conditions = tuple(list(original_conditions) + new_conditions)\n",
    "\n",
    "    def expand_pdnorm(pdnorm):\n",
    "        if isinstance(pdnorm, PDNorm) and pdnorm.decouple:\n",
    "            first_norm = pdnorm.norm[0]\n",
    "            if isinstance(first_norm, nn.BatchNorm1d):\n",
    "                new_norm_func = lambda: type(first_norm)(\n",
    "                    first_norm.num_features,\n",
    "                    eps=first_norm.eps,\n",
    "                    momentum=first_norm.momentum,\n",
    "                    affine=first_norm.affine,\n",
    "                    track_running_stats=first_norm.track_running_stats\n",
    "                )\n",
    "            elif isinstance(first_norm, nn.LayerNorm):\n",
    "                new_norm_func = lambda: type(first_norm)(\n",
    "                    first_norm.normalized_shape,\n",
    "                    eps=first_norm.eps,\n",
    "                    elementwise_affine=first_norm.elementwise_affine\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported normalization type: {type(first_norm)}\")\n",
    "\n",
    "            new_norms = [new_norm_func() for _ in new_conditions]\n",
    "            \n",
    "            for i, condition in enumerate(new_conditions):\n",
    "                if condition_mapping[condition] in original_conditions:\n",
    "                    source_idx = original_conditions.index(condition_mapping[condition])\n",
    "                    new_norms[i].weight.data.copy_(pdnorm.norm[source_idx].weight.data)\n",
    "                    new_norms[i].bias.data.copy_(pdnorm.norm[source_idx].bias.data)\n",
    "                    if isinstance(new_norms[i], nn.BatchNorm1d):\n",
    "                        new_norms[i].running_mean.copy_(pdnorm.norm[source_idx].running_mean)\n",
    "                        new_norms[i].running_var.copy_(pdnorm.norm[source_idx].running_var)\n",
    "                else:\n",
    "                    # Initialize with random values\n",
    "                    nn.init.normal_(new_norms[i].weight, mean=1.0, std=0.02)\n",
    "                    nn.init.zeros_(new_norms[i].bias)\n",
    "            \n",
    "            pdnorm.norm.extend(new_norms)\n",
    "            pdnorm.conditions = model.conditions\n",
    "        return pdnorm\n",
    "\n",
    "    def update_norm_layers(module):\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, PDNorm):\n",
    "                setattr(module, name, expand_pdnorm(child))\n",
    "            else:\n",
    "                update_norm_layers(child)\n",
    "\n",
    "    update_norm_layers(model)\n",
    "\n",
    "    old_embed = model.embedding_table\n",
    "    new_embed = nn.Embedding(len(model.conditions), old_embed.embedding_dim)\n",
    "    nn.init.normal_(new_embed.weight, mean=0.0, std=0.02)\n",
    "    new_embed.weight.data[:len(original_conditions)] = old_embed.weight.data\n",
    "    \n",
    "    for i, condition in enumerate(new_conditions):\n",
    "        new_idx = len(original_conditions) + i\n",
    "        if condition_mapping.get(condition) in original_conditions:\n",
    "            source_idx = original_conditions.index(condition_mapping[condition])\n",
    "            new_embed.weight.data[new_idx] = old_embed.weight.data[source_idx]\n",
    "    \n",
    "    model.embedding_table = new_embed\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ca0b975-2896-4f36-bf81-d758141a0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pointcept.models.point_prompt_training import PDNorm\n",
    "\n",
    "def test_ppt_model_expansion(model, new_conditions=[\"NewDataset1\", \"NewDataset2\"], device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Test function to verify the correctness of PDNorm expansion in a PPT model.\n",
    "    \n",
    "    Args:\n",
    "    - model: The original PPT model\n",
    "    - new_conditions: List of new conditions to add (default: [\"NewDataset1\", \"NewDataset2\"])\n",
    "    - device: The device to run the test on (default: \"cuda\")\n",
    "    \n",
    "    Returns:\n",
    "    - None, but raises AssertionError if any test fails\n",
    "    \"\"\"\n",
    "    # Ensure the model is on the specified device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Setup\n",
    "    original_conditions = model.conditions\n",
    "    condition_mapping = {\n",
    "        \"NewDataset1\": \"ScanNet\",  # Copy from ScanNet\n",
    "        \"NewDataset2\": None  # Random initialization\n",
    "    }\n",
    "    \n",
    "    # Store original embedding weights\n",
    "    original_embedding_weights = model.embedding_table.weight.clone()\n",
    "    \n",
    "    # Expand the model\n",
    "    expanded_model = expand_ppt_model_conditions(model, new_conditions, condition_mapping)\n",
    "    expanded_model = expanded_model.to(device)\n",
    "    \n",
    "    # Helper function to check if tensors are close\n",
    "    def tensors_close(a, b, rtol=1e-5, atol=1e-8):\n",
    "        return torch.allclose(a.to(device), b.to(device), rtol=rtol, atol=atol)\n",
    "    \n",
    "    # Test embedding table\n",
    "    assert expanded_model.embedding_table.weight.shape[0] == len(original_conditions) + len(new_conditions), \"Embedding table size mismatch\"\n",
    "    assert tensors_close(expanded_model.embedding_table.weight[:len(original_conditions)], original_embedding_weights), \"Original embeddings changed\"\n",
    "    assert tensors_close(\n",
    "        expanded_model.embedding_table.weight[len(original_conditions)], \n",
    "        original_embedding_weights[original_conditions.index(\"ScanNet\")]\n",
    "    ), \"NewDataset1 embedding not copied correctly\"\n",
    "    \n",
    "    def check_pdnorm_layers(module, prefix=''):\n",
    "        for name, child in module.named_children():\n",
    "            full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "            if isinstance(child, PDNorm):\n",
    "                assert len(child.norm) == len(original_conditions) + len(new_conditions), f\"PDNorm {full_name} size mismatch\"\n",
    "                \n",
    "                # Get the corresponding PDNorm from the original model\n",
    "                original_pdnorm = model\n",
    "                for part in full_name.split('.'):\n",
    "                    original_pdnorm = getattr(original_pdnorm, part)\n",
    "                \n",
    "                # Check parameters of original conditions\n",
    "                for i, condition in enumerate(original_conditions):\n",
    "                    assert tensors_close(child.norm[i].weight, original_pdnorm.norm[i].weight), f\"Weight mismatch for {condition} in {full_name}\"\n",
    "                    assert tensors_close(child.norm[i].bias, original_pdnorm.norm[i].bias), f\"Bias mismatch for {condition} in {full_name}\"\n",
    "                    assert child.norm[i].eps == original_pdnorm.norm[i].eps, f\"Eps mismatch for {condition} in {full_name}\"\n",
    "                \n",
    "                # Check parameters of new conditions\n",
    "                scannet_idx = original_conditions.index(\"ScanNet\")\n",
    "                new_dataset1_idx = len(original_conditions)\n",
    "                new_dataset2_idx = len(original_conditions) + 1\n",
    "                \n",
    "                # NewDataset1 should be copied from ScanNet\n",
    "                if not tensors_close(child.norm[new_dataset1_idx].weight, child.norm[scannet_idx].weight):\n",
    "                    print(f\"NewDataset1 weight: {child.norm[new_dataset1_idx].weight}\")\n",
    "                    print(f\"ScanNet weight: {child.norm[scannet_idx].weight}\")\n",
    "                    raise AssertionError(f\"NewDataset1 weight not copied correctly in {full_name}\")\n",
    "                \n",
    "                if not tensors_close(child.norm[new_dataset1_idx].bias, child.norm[scannet_idx].bias):\n",
    "                    print(f\"NewDataset1 bias: {child.norm[new_dataset1_idx].bias}\")\n",
    "                    print(f\"ScanNet bias: {child.norm[scannet_idx].bias}\")\n",
    "                    raise AssertionError(f\"NewDataset1 bias not copied correctly in {full_name}\")\n",
    "                \n",
    "                # NewDataset2 should be randomly initialized\n",
    "                assert not tensors_close(child.norm[new_dataset2_idx].weight, child.norm[scannet_idx].weight, rtol=1e-3, atol=1e-3), f\"NewDataset2 weight should not match ScanNet in {full_name}\"\n",
    "                \n",
    "                # Check that NewDataset2 is properly initialized\n",
    "                assert torch.allclose(child.norm[new_dataset2_idx].weight.mean(), torch.tensor(1.0, device=device), rtol=1e-1), f\"NewDataset2 weight not properly initialized in {full_name}\"\n",
    "                assert torch.allclose(child.norm[new_dataset2_idx].bias.mean(), torch.tensor(0.0, device=device), rtol=1e-1), f\"NewDataset2 bias not properly initialized in {full_name}\"\n",
    "                \n",
    "                # Check eps and other parameters\n",
    "                for i in range(len(child.norm)):\n",
    "                    assert child.norm[i].eps == child.norm[0].eps, f\"Eps mismatch in {full_name} for layer {i}\"\n",
    "                    if isinstance(child.norm[i], nn.BatchNorm1d):\n",
    "                        assert child.norm[i].momentum == child.norm[0].momentum, f\"Momentum mismatch in {full_name} for layer {i}\"\n",
    "                        assert child.norm[i].affine == child.norm[0].affine, f\"Affine mismatch in {full_name} for layer {i}\"\n",
    "                        assert child.norm[i].track_running_stats == child.norm[0].track_running_stats, f\"Track_running_stats mismatch in {full_name} for layer {i}\"\n",
    "                    elif isinstance(child.norm[i], nn.LayerNorm):\n",
    "                        assert child.norm[i].elementwise_affine == child.norm[0].elementwise_affine, f\"Elementwise_affine mismatch in {full_name} for layer {i}\"\n",
    "            else:\n",
    "                check_pdnorm_layers(child, full_name)\n",
    "    \n",
    "    # Run the recursive check\n",
    "    check_pdnorm_layers(expanded_model)\n",
    "    \n",
    "    print(\"All tests passed successfully!\")\n",
    "\n",
    "# Usage:\n",
    "# test_ppt_model_expansion(your_model, device=\"cuda\")  # or \"cpu\" if preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44aaa4c-5d13-4cf9-8679-48e5a04f602a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "test_ppt_model_expansion(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb4eabb6-854e-449f-8aa0-baf2fbad2b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assume 'trained_model' is your PPT model trained on the original datasets\n",
    "\n",
    "# Define new conditions and mapping\n",
    "new_conditions = [\"NewDataset1\", \"NewDataset2\"]\n",
    "condition_mapping = {\n",
    "    \"NewDataset1\": \"ScanNet\",  # Initialize NewDataset1 with ScanNet's parameters\n",
    "    \"NewDataset2\": None  # Initialize NewDataset2 with default initialization\n",
    "}\n",
    "\n",
    "# Expand the model\n",
    "expanded_model = expand_ppt_model_conditions(model, new_conditions, condition_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86febb45-09b8-4346-bdd8-e511429b23d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointPromptTraining(\n",
       "  (backbone): PointTransformerV3(\n",
       "    (embedding): Embedding(\n",
       "      (stem): PointSequential(\n",
       "        (conv): SubMConv3d(6, 48, kernel_size=[5, 5, 5], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.Native)\n",
       "        (norm): PDNorm(\n",
       "          (norm): ModuleList(\n",
       "            (0-8): 9 x BatchNorm1d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (enc): PointSequential(\n",
       "      (enc0): PointSequential(\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "            (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=48, out_features=192, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=192, out_features=48, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "            (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=48, out_features=192, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=192, out_features=48, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.018)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=48, out_features=144, bias=True)\n",
       "            (proj): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=48, out_features=192, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=192, out_features=48, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.035)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (enc1): PointSequential(\n",
       "        (down): SerializedPooling(\n",
       "          (proj): Linear(in_features=48, out_features=96, bias=True)\n",
       "          (norm): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (act): PointSequential(\n",
       "            (0): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.053)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.071)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.088)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (enc2): PointSequential(\n",
       "        (down): SerializedPooling(\n",
       "          (proj): Linear(in_features=96, out_features=192, bias=True)\n",
       "          (norm): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (act): PointSequential(\n",
       "            (0): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.106)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.124)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.141)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (enc3): PointSequential(\n",
       "        (down): SerializedPooling(\n",
       "          (proj): Linear(in_features=192, out_features=384, bias=True)\n",
       "          (norm): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (act): PointSequential(\n",
       "            (0): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.159)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.176)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.194)\n",
       "          )\n",
       "        )\n",
       "        (block3): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.212)\n",
       "          )\n",
       "        )\n",
       "        (block4): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.229)\n",
       "          )\n",
       "        )\n",
       "        (block5): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.247)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (enc4): PointSequential(\n",
       "        (down): SerializedPooling(\n",
       "          (proj): Linear(in_features=384, out_features=512, bias=True)\n",
       "          (norm): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (act): PointSequential(\n",
       "            (0): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(512, 512, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.265)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(512, 512, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.282)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(512, 512, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.300)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dec): PointSequential(\n",
       "      (dec3): PointSequential(\n",
       "        (up): SerializedUnpooling(\n",
       "          (proj): PointSequential(\n",
       "            (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (proj_skip): PointSequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.300)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.273)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.245)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dec2): PointSequential(\n",
       "        (up): SerializedUnpooling(\n",
       "          (proj): PointSequential(\n",
       "            (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (proj_skip): PointSequential(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.218)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.191)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.164)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dec1): PointSequential(\n",
       "        (up): SerializedUnpooling(\n",
       "          (proj): PointSequential(\n",
       "            (0): Linear(in_features=192, out_features=96, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (proj_skip): PointSequential(\n",
       "            (0): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.136)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.109)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.082)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dec0): PointSequential(\n",
       "        (up): SerializedUnpooling(\n",
       "          (proj): PointSequential(\n",
       "            (0): Linear(in_features=96, out_features=64, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (proj_skip): PointSequential(\n",
       "            (0): Linear(in_features=48, out_features=64, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.055)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.027)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-8): 9 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (embedding_table): Embedding(9, 256)\n",
       "  (proj_head): Linear(in_features=64, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4c725-2926-43b7-aed2-9f348530cc91",
   "metadata": {},
   "source": [
    "# Visualise netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f17df69-85bb-4481-9c82-d4b36bd058b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece78c4e-df11-4a38-9175-75309a337b1b",
   "metadata": {},
   "source": [
    "Now install netron and open this file:\n",
    "\n",
    "```bash\n",
    "snap install netron\n",
    "snap run netron\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92870c6-b661-422a-8542-a3b7f0e4bac7",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf183b0f-3960-4586-ace2-567cb7365ded",
   "metadata": {},
   "source": [
    "### minlora implementation\n",
    "\n",
    "Quick test run to see things look reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27230259-4488-4bb1-a727-34fe57fab909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Optimizing 3.3M parameters\n",
      "INFO:__main__:Using fused AdamW: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params before LoRA: {'trainable': 97447088, 'frozen': 1}\n",
      "logit_scale: frozen\n",
      "freezing all weights\n",
      "# params after freezing: {'trainable': 0, 'frozen': 97447089}\n",
      "applying LoRA adapters\n",
      "# params after LoRA: 3314890\n",
      "performing cross checks\n",
      "restoring initial model state\n",
      "# trainable params after removing lora: {'trainable': 97447088, 'frozen': 1}\n",
      "logit_scale: frozen\n"
     ]
    }
   ],
   "source": [
    "# lora adapter hyperparameters\n",
    "lora_hparams = dict(\n",
    "    lora_dropout_p = 0.0,\n",
    "    rank=10,\n",
    "    lora_alpha = 64\n",
    ")\n",
    "\n",
    "# optimizer hyperparameters\n",
    "weight_decay = 0.05\n",
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999#0.95\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "lora_config = {\n",
    "    torch.nn.Embedding: {\n",
    "        \"weight\": partial(LoRAParametrization.from_embedding, **lora_hparams),\n",
    "    },\n",
    "    torch.nn.Linear: {\n",
    "        \"weight\": partial(LoRAParametrization.from_linear, **lora_hparams),\n",
    "    },\n",
    "    SubMConv3d: {\n",
    "        \"weight\": partial(LoRAParametrization.from_sparseconv3d, **lora_hparams),\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"# params before LoRA:\", count_trainable_parameters(model))\n",
    "wf.print_frozen_status()\n",
    "print(\"freezing all weights\")\n",
    "wf.freeze_all()\n",
    "print(\"# params after freezing:\", count_trainable_parameters(model))\n",
    "print(\"applying LoRA adapters\")\n",
    "minlora.add_lora(model, lora_config=lora_config)\n",
    "\n",
    "lora_trainable_params = count_trainable_parameters(model)[\"trainable\"]\n",
    "print(\"# params after LoRA:\", lora_trainable_params)\n",
    "\n",
    "# create AdamW optimizer (for LoRA weights only)\n",
    "optimizer = configure_adamw_lora(\n",
    "    model,\n",
    "    weight_decay,\n",
    "    learning_rate,\n",
    "    (beta1, beta2),\n",
    "    device_type\n",
    ")\n",
    "\n",
    "print(\"performing cross checks\")\n",
    "# check all lora parameters trainable\n",
    "assert_lora_trainable(model)\n",
    "# check manual lora parameter counting against minlora to check that it matches\n",
    "assert count_lora_parameters(model) == count_lora_params_manual(model)\n",
    "# cross check with lora params with gradient enabled\n",
    "assert total_optimized_params(optimizer) == lora_trainable_params\n",
    "\n",
    "print(\"restoring initial model state\")\n",
    "# remove adapters and unfreeze weights to original state\n",
    "minlora.remove_lora(model)\n",
    "wf.unfreeze_all()\n",
    "\n",
    "print(\"# trainable params after removing lora:\", count_trainable_parameters(model))\n",
    "wf.print_frozen_status()\n",
    "\n",
    "# if use_lora:\n",
    "#     optimizer = configure_optimizers_lora(model, weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# else:\n",
    "#     optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# if init_from == 'resume':\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb1be65-6820-4942-805b-93db4e80ffc1",
   "metadata": {},
   "source": [
    "## Test fwd/backward passes with LoRA\n",
    "\n",
    "Now freeze + apply LoRA again and test a bit on dummy data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f91bc39-0427-4150-9e37-b647d78d051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wf.freeze_all()\n",
    "minlora.add_lora(model, lora_config=lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205909ba-1de1-4e2f-854d-e760b47e7561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** First Pass ***\n",
      "Initial gradients: A: 0/195, B: 194/195\n",
      "Trainable parameters with gradients: 808,320\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "Total A matrices without gradients: 195\n",
      "Total B matrices without gradients: 1\n",
      "\n",
      "Gradients after step 1:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 2:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 3:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 4:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 5:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n"
     ]
    }
   ],
   "source": [
    "def inspect_lora_gradients(model, x, num_steps=5):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    def check_grads():\n",
    "        a_no_grad, b_no_grad = [], []\n",
    "        a_with_grad, b_with_grad = 0, 0\n",
    "        total_a, total_b = 0, 0\n",
    "        trainable_params_with_grad = 0\n",
    "        frozen_params = 0\n",
    "        total_params = 0\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            total_params += param.numel()\n",
    "            if not param.requires_grad:\n",
    "                frozen_params += param.numel()\n",
    "            elif param.grad is not None and torch.any(param.grad != 0):\n",
    "                trainable_params_with_grad += param.numel()\n",
    "\n",
    "            if 'lora_A' in name:\n",
    "                total_a += 1\n",
    "                if param.grad is None or torch.all(param.grad == 0):\n",
    "                    a_no_grad.append(name)\n",
    "                else:\n",
    "                    a_with_grad += 1\n",
    "            elif 'lora_B' in name:\n",
    "                total_b += 1\n",
    "                if param.grad is None or torch.all(param.grad == 0):\n",
    "                    b_no_grad.append(name)\n",
    "                else:\n",
    "                    b_with_grad += 1\n",
    "\n",
    "        return (a_with_grad, b_with_grad, total_a, total_b, a_no_grad, b_no_grad, \n",
    "                trainable_params_with_grad, frozen_params, total_params)\n",
    "\n",
    "    # Initial forward and backward pass\n",
    "    y = model(x)\n",
    "    loss = y[\"loss\"].sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    results = check_grads()\n",
    "    (\n",
    "        a_grad,\n",
    "        b_grad,\n",
    "        total_a,\n",
    "        total_b,\n",
    "        a_no_grad,\n",
    "        b_no_grad,\n",
    "        trainable_grad,\n",
    "        frozen,\n",
    "        total\n",
    "    ) = results\n",
    "\n",
    "    print(\"*** First Pass ***\")\n",
    "    print(f\"Initial gradients: A: {a_grad}/{total_a}, B: {b_grad}/{total_b}\")\n",
    "    print(f\"Trainable parameters with gradients: {trainable_grad:,}\")\n",
    "    print(f\"Frozen parameters: {frozen:,}\")\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    if a_no_grad:\n",
    "        print(f\"Total A matrices without gradients: {len(a_no_grad)}\")\n",
    "    if b_no_grad:\n",
    "        print(f\"Total B matrices without gradients: {len(b_no_grad)}\")\n",
    "\n",
    "    # Perform several optimization steps\n",
    "    for i in range(num_steps):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y = model(x)\n",
    "        loss = y[\"loss\"].sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        results = check_grads()\n",
    "        a_grad, b_grad, total_a, total_b, a_no_grad, b_no_grad, trainable_grad, frozen, total = results\n",
    "\n",
    "        print(f\"\\nGradients after step {i+1}:\")\n",
    "        print(f\"A: {a_grad}/{total_a}, B: {b_grad}/{total_b}\")\n",
    "        print(f\"Trainable parameters with gradients: {trainable_grad:,}\")\n",
    "        print(f\"Frozen parameters: {frozen:,}\")\n",
    "        print(f\"Total parameters: {total:,}\")\n",
    "        if a_no_grad:\n",
    "            print(f\"A matrices without gradients: {a_no_grad}\")\n",
    "        if b_no_grad:\n",
    "            print(f\"B matrices without gradients: {b_no_grad}\")\n",
    "            \n",
    "X = create_spoofed_input(device=\"cuda\", batch_size=16)\n",
    "inspect_lora_gradients(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff3c868-354c-4251-9d3c-de175ac71b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module backbone.enc.enc0.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module proj_head:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "def showlora(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d, nn.MultiheadAttention)):\n",
    "            print(f\"Module {name}:\")\n",
    "            if hasattr(module, 'parametrizations'):\n",
    "                for param_name, param in module.parametrizations.items():\n",
    "                    print(f\"  - {param_name} LoRA parameters:\")\n",
    "                    for lora_name, lora_param in param.named_parameters():\n",
    "                        print(f\"    - {lora_name}: device = {lora_param.device}\")\n",
    "            elif isinstance(module, nn.MultiheadAttention):\n",
    "                if hasattr(module.out_proj, 'parametrizations'):\n",
    "                    for param_name, param in module.out_proj.parametrizations.items():\n",
    "                        print(f\"  - out_proj.{param_name} LoRA parameters:\")\n",
    "                        for lora_name, lora_param in param.named_parameters():\n",
    "                            print(f\"    - {lora_name}: device = {lora_param.device}\")\n",
    "\n",
    "showlora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dd82726-a2ce-4fc5-bcb2-54535a6d7460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, \"model_minlora.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228ea067-0f72-4c94-9aec-c7bf789e9749",
   "metadata": {},
   "source": [
    "RuntimeError: Serialization of parametrized modules is only supported through state_dict(). See:\n",
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training\n",
    "\n",
    "No biggie, just can't visualise this using netron as a consequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4c1c1-4922-4059-b19b-d2d14969eb85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
