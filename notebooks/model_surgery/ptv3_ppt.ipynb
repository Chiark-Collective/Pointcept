{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97aa1631-bdf1-446f-8a7a-12eb0de74245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-27 17:32:46,017 INFO test.py line 41 33468] => Loading config ...\n",
      "[2024-08-27 17:32:46,018 INFO test.py line 48 33468] => Building model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proj_head shape says Linear(in_features=64, out_features=512, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-27 17:32:49,630 INFO test.py line 61 33468] Num params: 97447088\n",
      "[2024-08-27 17:32:49,862 INFO test.py line 68 33468] Loading weight at: ../../models/PointTransformerV3/scannet-semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth\n",
      "[2024-08-27 17:32:51,405 INFO test.py line 84 33468] => Loaded weight '../../models/PointTransformerV3/scannet-semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth' (epoch 94)\n",
      "[2024-08-27 17:32:51,409 INFO test.py line 53 33468] => Building test dataset & dataloader ...\n",
      "[2024-08-27 17:32:51,411 INFO scannet.py line 72 33468] Totally 0 x 1 samples in val set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DITCHING CLASS EMBEDDING\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('svg')\n",
    "from lora_pytorch import LoRA\n",
    "assert torch.cuda.is_available()\n",
    "from torchview import draw_graph\n",
    "from torchviz import make_dot\n",
    "from graphviz import Digraph\n",
    "\n",
    "from pointcept.engines.defaults import (\n",
    "    default_argument_parser,\n",
    "    default_config_parser,\n",
    "    default_setup,\n",
    ")\n",
    "from pointcept.engines.test import TESTERS\n",
    "from pointcept.engines.launch import launch\n",
    "from pointcept.engines.test import TesterBase, SemSegTester\n",
    "\n",
    "repo_root = Path(\"../..\")\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def create_spoofed_input(batch_size=2, num_points=1000, n_classes=5, num_features=6, device='cpu'):\n",
    "    return {\n",
    "        'coord': torch.rand(num_points * batch_size, num_features, device=device),\n",
    "        'feat': torch.rand(num_points * batch_size, num_features, device=device),\n",
    "        'grid_coord': torch.randint(0, 100, (num_points * batch_size, 3), device=device),\n",
    "        'batch': torch.arange(batch_size, device=device).repeat_interleave(num_points),\n",
    "        'offset': torch.tensor([num_points * i for i in range(1, batch_size + 1)], device=device),\n",
    "        'condition': ['ScanNet'] * batch_size,\n",
    "        'grid_size': torch.tensor([0.01], device=device),\n",
    "        'segment': torch.randint(low=0, high=n_classes-1, size=(num_points * batch_size,), device=device)\n",
    "    }\n",
    "\n",
    "\n",
    "def patch_cfg(cfg: dict, repo_root: Path = repo_root) -> dict:\n",
    "    cfg = cfg.copy()\n",
    "    cfg[\"my_data_root\"] = repo_root / cfg[\"my_data_root\"]\n",
    "    cfg[\"weight\"] = repo_root / cfg[\"weight\"]\n",
    "    cfg[\"batch_size_test_per_gpu\"] = 1\n",
    "    return cfg\n",
    "\n",
    "\n",
    "repo_root = Path(\"../..\")\n",
    "cfg_file = Path(\"../../test/custom-ppt-config.py\"); assert cfg_file.exists\n",
    "device = \"cuda\"\n",
    "\n",
    "args = default_argument_parser().parse_args(args=[\"--config-file\", f\"{cfg_file}\"])\n",
    "cfg = default_config_parser(args.config_file, args.options); cfg = patch_cfg(cfg)\n",
    "\n",
    "tester = TESTERS.build(dict(type=cfg.test.type, cfg=cfg))\n",
    "model = tester.model\n",
    "model.to(device)\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1f83027-8dcf-48b2-b1fc-296fc9fda1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(m))\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "for m in model.named_modules():\n",
    "    print(len(m))\n",
    "    print(m[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4c725-2926-43b7-aed2-9f348530cc91",
   "metadata": {},
   "source": [
    "# Visualise netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f17df69-85bb-4481-9c82-d4b36bd058b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece78c4e-df11-4a38-9175-75309a337b1b",
   "metadata": {},
   "source": [
    "Now install netron and open this file:\n",
    "\n",
    "```bash\n",
    "snap install netron\n",
    "snap run netron\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92870c6-b661-422a-8542-a3b7f0e4bac7",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac3631-feea-49f9-9dc2-d8ac19297e51",
   "metadata": {},
   "source": [
    "### lora-pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf019940-a66c-402d-ae71-f164b94cf3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bare model:  110759388\n",
      "lora: 13312300\n"
     ]
    }
   ],
   "source": [
    "# lora_model = LoRA.from_module(model, rank=50)\n",
    "# print(\"bare model: \", count_trainable_parameters(model))\n",
    "# print(\"lora:\", count_trainable_parameters(lora_model))\n",
    "# torch.save(model, \"model_lora.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf183b0f-3960-4586-ace2-567cb7365ded",
   "metadata": {},
   "source": [
    "### minlora implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "249031c8-7288-4f6a-ade9-1c0c76ad785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA applied to blocks.0.attention.out_proj: False\n",
      "LoRA applied to blocks.0.ffn.0: True\n",
      "LoRA applied to blocks.0.ffn.2: True\n",
      "LoRA applied to blocks.0.conv: True\n",
      "LoRA applied to blocks.1.attention.out_proj: False\n",
      "LoRA applied to blocks.1.ffn.0: True\n",
      "LoRA applied to blocks.1.ffn.2: True\n",
      "LoRA applied to blocks.1.conv: True\n",
      "LoRA applied to blocks.2.attention.out_proj: False\n",
      "LoRA applied to blocks.2.ffn.0: True\n",
      "LoRA applied to blocks.2.ffn.2: True\n",
      "LoRA applied to blocks.2.conv: True\n",
      "LoRA applied to final_layer: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from minlora import add_lora, LoRAParametrization\n",
    "\n",
    "# Example custom architecture\n",
    "class CustomBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(dim, 8)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 4*dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4*dim, dim)\n",
    "        )\n",
    "        self.conv = nn.Conv2d(dim, dim, 3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.attention(x, x, x)[0] + x\n",
    "        x = self.ffn(x) + x\n",
    "        x = x.permute(0, 2, 1).unsqueeze(-1)  # reshape for 2D conv\n",
    "        x = self.conv(x).squeeze(-1).permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, dim, num_blocks):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([CustomBlock(dim) for _ in range(num_blocks)])\n",
    "        self.final_layer = nn.Linear(dim, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return self.final_layer(x)\n",
    "\n",
    "# Custom LoRA configuration\n",
    "custom_lora_config = {\n",
    "    nn.Linear: {\n",
    "        \"weight\": LoRAParametrization.from_linear\n",
    "    },\n",
    "    # nn.MultiheadAttention: {\n",
    "    #     \"out_proj.weight\": LoRAParametrization.from_linear\n",
    "    # },\n",
    "    nn.Conv2d: {\n",
    "        \"weight\": LoRAParametrization.from_conv2d\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create and apply LoRA\n",
    "model = CustomModel(dim=256, num_blocks=3)\n",
    "add_lora(model, lora_config=custom_lora_config)\n",
    "\n",
    "# Verify LoRA application\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (nn.Linear, nn.Conv2d)) or (isinstance(module, nn.MultiheadAttention) and 'weight' in name):\n",
    "        print(f\"LoRA applied to {name}: {hasattr(module, 'parametrizations')}\")\n",
    "\n",
    "# # Training loop (pseudo-code)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = criterion(model(batch), targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317bc98-49b3-45eb-b9f7-7881112a0ae3",
   "metadata": {},
   "source": [
    "#### for PPT+PTvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b67844-b7be-4072-be00-36475b883e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import minlora\n",
    "from minlora import (\n",
    "    LoRAParametrization,\n",
    "    add_lora,\n",
    "    merge_lora,\n",
    "    remove_lora\n",
    ")\n",
    "from minlora.model import add_lora_by_name, apply_lora\n",
    "\n",
    "\n",
    "from spconv.pytorch.conv import SubMConv3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27230259-4488-4bb1-a727-34fe57fab909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before LoRA: 97447088\n",
      "after LoRA: 3314890\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "def configure_optimizers_lora(self, weight_decay, learning_rate, betas, device_type):\n",
    "    # we apply weight decay to all lora params\n",
    "    optim_groups = [\n",
    "        # note: .get_lora_params() returns a generator\n",
    "        # we need to wrap it in a list so we can consume it twice\n",
    "        {\"params\": list(minlora.get_lora_params(self)) , \"weight_decay\": weight_decay},\n",
    "        # you can also add biases for fine-tuning,\n",
    "        # but I want to make sure lora alone works\n",
    "        # {\"params\": minlora.get_bias_params(self), \"weight_decay\": 0.0}, # bias params don't get weight decay\n",
    "    ]\n",
    "\n",
    "    def parameter_count(optim_groups):\n",
    "        n = sum(p.numel() for group in optim_groups for p in group[\"params\"])\n",
    "        if n < 1e6:\n",
    "            return f\"{n/1e3:.1f}k\"\n",
    "        else:\n",
    "            return f\"{n/1e6:.1f}M\"\n",
    "\n",
    "    print(f\"optimizing {parameter_count(optim_groups)} parameters\")\n",
    "\n",
    "    # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n",
    "    use_fused = (device_type == \"cuda\") and (\"fused\" in inspect.signature(torch.optim.AdamW).parameters)\n",
    "    print(f\"using fused AdamW: {use_fused}\")\n",
    "    extra_args = dict(fused=True) if use_fused else dict()\n",
    "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "lora_hparams = dict(\n",
    "    lora_dropout_p = 0.0,\n",
    "    rank=10,\n",
    "    lora_alpha = 64\n",
    ")\n",
    "\n",
    "lora_config = {\n",
    "    torch.nn.Embedding: {\n",
    "        \"weight\": partial(LoRAParametrization.from_embedding, **lora_hparams),\n",
    "    },\n",
    "    torch.nn.Linear: {\n",
    "        \"weight\": partial(LoRAParametrization.from_linear, **lora_hparams),\n",
    "    },\n",
    "    SubMConv3d: {\n",
    "        \"weight\": partial(LoRAParametrization.from_sparseconv3d, **lora_hparams),\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"before LoRA:\", count_trainable_parameters(model))\n",
    "\n",
    "def freeze_non_lora_params(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if True:#'lora' not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "freeze_non_lora_params(model)\n",
    "\n",
    "minlora.add_lora(model, lora_config=lora_config)\n",
    "print(\"after LoRA:\", count_trainable_parameters(model))\n",
    "# if use_lora:\n",
    "#     optimizer = configure_optimizers_lora(model, weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# else:\n",
    "#     optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# if init_from == 'resume':\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7004cf3-0a9f-4b45-b92a-d8d5725d5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = create_spoofed_input(device=\"cuda\", batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1be3af30-b71e-406f-932b-2bcc52f3b746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])=\n",
      "feat.shape=torch.Size([16000, 512])\n",
      "self.class_embedding.shape=torch.Size([13, 512])\n",
      "sim.shape=torch.Size([16000, 13])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(4.7081, device='cuda:0', grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544ececb-6201-434a-81fa-130646d1e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizing 3.3M parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "optimizer = configure_optimizers_lora(model, weight_decay, learning_rate, (beta1, beta2), device_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9e934-62e0-4b3f-bffe-4c7a2d05e074",
   "metadata": {},
   "source": [
    "test backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d35664-7b6a-4cc0-b199-3dd4d138c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])=\n",
      "feat.shape=torch.Size([16000, 512])\n",
      "self.class_embedding.shape=torch.Size([13, 512])\n",
      "sim.shape=torch.Size([16000, 13])\n"
     ]
    }
   ],
   "source": [
    "loss = model(X)\n",
    "\n",
    "loss[\"loss\"].backward()\n",
    "optimizer.step()\n",
    "#optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac5a9e93-435c-4b13-b531-709996eab8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4582c4b7-2f5a-4d6c-a8b0-965af4a508f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff3c868-354c-4251-9d3c-de175ac71b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module backbone.enc.enc0.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module proj_head:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "def showlora(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d, nn.MultiheadAttention)):\n",
    "            print(f\"Module {name}:\")\n",
    "            if hasattr(module, 'parametrizations'):\n",
    "                for param_name, param in module.parametrizations.items():\n",
    "                    print(f\"  - {param_name} LoRA parameters:\")\n",
    "                    for lora_name, lora_param in param.named_parameters():\n",
    "                        print(f\"    - {lora_name}: device = {lora_param.device}\")\n",
    "            elif isinstance(module, nn.MultiheadAttention):\n",
    "                if hasattr(module.out_proj, 'parametrizations'):\n",
    "                    for param_name, param in module.out_proj.parametrizations.items():\n",
    "                        print(f\"  - out_proj.{param_name} LoRA parameters:\")\n",
    "                        for lora_name, lora_param in param.named_parameters():\n",
    "                            print(f\"    - {lora_name}: device = {lora_param.device}\")\n",
    "\n",
    "showlora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd82726-a2ce-4fc5-bcb2-54535a6d7460",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Serialization of parametrized modules is only supported through state_dict(). See:\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_minlora.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/serialization.py:629\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 629\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/serialization.py:841\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    839\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m    840\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m--> 841\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    843\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/utils/parametrize.py:305\u001b[0m, in \u001b[0;36m_inject_new_class.<locals>.getstate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSerialization of parametrized modules is only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported through state_dict(). See:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/tutorials/beginner/saving_loading_models.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Serialization of parametrized modules is only supported through state_dict(). See:\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"model_minlora.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7011af-0d97-46b4-94a0-e4b2707e46c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba4a57-ab4c-47d7-b9bf-bb32267bab67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8478a-94cd-41cc-bb66-168c02e22551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca56452-a63a-41a3-ad42-a7afc0df6d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd04c52-fdca-464d-943c-3373fc528132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef7074-15d7-463d-942b-c6f1facae827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18bd2de-5b12-45c3-9f35-abcdc24c64b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516088c-9a06-457e-8b55-df4ca94f2d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df44cae0-ba50-4ca3-ad1a-3e8cdd5cd0d1",
   "metadata": {},
   "source": [
    "### custom implementation (claude, unchecked but it runs lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56f4925-2b1d-4961-afc5-6c98cf7a39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.scale = 0.01\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.lora_A.T @ self.lora_B.T) * self.scale\n",
    "\n",
    "class AdaptiveLoRAWrapper(nn.Module):\n",
    "    def __init__(self, base_layer, rank=4):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        if hasattr(base_layer, 'weight'):\n",
    "            weight = base_layer.weight\n",
    "            in_features, out_features = weight.shape[1], weight.shape[0]\n",
    "        elif hasattr(base_layer, 'in_features') and hasattr(base_layer, 'out_features'):\n",
    "            in_features, out_features = base_layer.in_features, base_layer.out_features\n",
    "        else:\n",
    "            raise ValueError(f\"Unable to determine in_features and out_features for {type(base_layer)}\")\n",
    "        self.lora = LoRALayer(in_features, out_features, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_layer(x) + self.lora(x)\n",
    "\n",
    "def get_in_out_features(layer):\n",
    "    if hasattr(layer, 'in_features') and hasattr(layer, 'out_features'):\n",
    "        return layer.in_features, layer.out_features\n",
    "    elif hasattr(layer, 'weight'):\n",
    "        return layer.weight.shape[1], layer.weight.shape[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to determine in_features and out_features for {type(layer)}\")\n",
    "\n",
    "class LoRAQKV(nn.Module):\n",
    "    def __init__(self, qkv_layer, rank=4):\n",
    "        super().__init__()\n",
    "        self.qkv_layer = qkv_layer\n",
    "        in_features, out_features = get_in_out_features(qkv_layer)\n",
    "        self.lora = LoRALayer(in_features, out_features, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.qkv_layer(x) + self.lora(x)\n",
    "\n",
    "def apply_lora_to_ptv3(model, rank=4):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, SerializedAttention):\n",
    "            module.qkv = LoRAQKV(module.qkv, rank)\n",
    "            module.proj = AdaptiveLoRAWrapper(module.proj, rank)\n",
    "        elif isinstance(module, MLP):\n",
    "            module.fc1 = AdaptiveLoRAWrapper(module.fc1, rank)\n",
    "            module.fc2 = AdaptiveLoRAWrapper(module.fc2, rank)\n",
    "\n",
    "def apply_lora_to_ppt(model, rank=4):\n",
    "    # Apply LoRA to PT-v3 backbone\n",
    "    apply_lora_to_ptv3(model.backbone, rank)\n",
    "    \n",
    "    # Apply LoRA to the projection head\n",
    "    model.proj_head = AdaptiveLoRAWrapper(model.proj_head, rank)\n",
    "\n",
    "    def freeze_non_lora_params(model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'lora' not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    freeze_non_lora_params(model)\n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "# ppt_model = PointPromptTraining(...)\n",
    "# ppt_model_with_lora = apply_lora_to_ppt(ppt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63cb43c1-07d4-4dbd-8750-996c067d5c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_model_with_lora = apply_lora_to_ppt(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e371a72-40b1-4b14-b25f-849320d726a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointcept.models.point_transformer_v3 import SerializedAttention, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49b62fda-35af-416b-bb18-00942c4fa9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453888"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_parameters(ppt_model_with_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b67cec-fe12-465d-92e6-b09f227082d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97979580"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d061f4e-b415-4cce-b5ce-68fb2f8b1a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
