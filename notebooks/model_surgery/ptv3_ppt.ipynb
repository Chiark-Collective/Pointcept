{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97aa1631-bdf1-446f-8a7a-12eb0de74245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-02 15:38:58,385 INFO test.py line 41 32766] => Loading config ...\n",
      "[2024-09-02 15:38:58,386 INFO test.py line 48 32766] => Building model ...\n",
      "[2024-09-02 15:39:00,996 INFO test.py line 61 32766] Num params: 97447088\n",
      "[2024-09-02 15:39:01,197 INFO test.py line 68 32766] Loading weight at: ../../models/PointTransformerV3/scannet-semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth\n",
      "[2024-09-02 15:39:01,853 INFO test.py line 84 32766] => Loaded weight '../../models/PointTransformerV3/scannet-semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth' (epoch 94)\n",
      "[2024-09-02 15:39:01,857 INFO test.py line 53 32766] => Building test dataset & dataloader ...\n",
      "[2024-09-02 15:39:01,859 INFO scannet.py line 72 32766] Totally 0 x 1 samples in val set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DITCHING CLASS EMBEDDING\n",
      "loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('svg')\n",
    "from lora_pytorch import LoRA\n",
    "assert torch.cuda.is_available()\n",
    "from torchview import draw_graph\n",
    "from torchviz import make_dot\n",
    "from graphviz import Digraph\n",
    "\n",
    "from pointcept.engines.defaults import (\n",
    "    default_argument_parser,\n",
    "    default_config_parser,\n",
    "    default_setup,\n",
    ")\n",
    "from pointcept.engines.test import TESTERS\n",
    "from pointcept.engines.launch import launch\n",
    "from pointcept.engines.test import TesterBase, SemSegTester\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "repo_root = Path(\"../..\")\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def create_spoofed_input(batch_size=2, num_points=1000, n_classes=5, num_features=6, device='cpu'):\n",
    "    return {\n",
    "        'coord': torch.rand(num_points * batch_size, num_features, device=device),\n",
    "        'feat': torch.rand(num_points * batch_size, num_features, device=device),\n",
    "        'grid_coord': torch.randint(0, 100, (num_points * batch_size, 3), device=device),\n",
    "        'batch': torch.arange(batch_size, device=device).repeat_interleave(num_points),\n",
    "        'offset': torch.tensor([num_points * i for i in range(1, batch_size + 1)], device=device),\n",
    "        'condition': ['ScanNet'] * batch_size,\n",
    "        'grid_size': torch.tensor([0.01], device=device),\n",
    "        'segment': torch.randint(low=0, high=n_classes-1, size=(num_points * batch_size,), device=device)\n",
    "    }\n",
    "\n",
    "\n",
    "def patch_cfg(cfg: dict, repo_root: Path = repo_root) -> dict:\n",
    "    cfg = cfg.copy()\n",
    "    cfg[\"my_data_root\"] = repo_root / cfg[\"my_data_root\"]\n",
    "    cfg[\"weight\"] = repo_root / cfg[\"weight\"]\n",
    "    cfg[\"batch_size_test_per_gpu\"] = 1\n",
    "    return cfg\n",
    "\n",
    "\n",
    "repo_root = Path(\"../..\")\n",
    "cfg_file = Path(\"../../test/custom-ppt-config.py\"); assert cfg_file.exists\n",
    "device = \"cuda\"\n",
    "\n",
    "args = default_argument_parser().parse_args(args=[\"--config-file\", f\"{cfg_file}\"])\n",
    "cfg = default_config_parser(args.config_file, args.options); cfg = patch_cfg(cfg)\n",
    "\n",
    "tester = TESTERS.build(dict(type=cfg.test.type, cfg=cfg))\n",
    "model = tester.model\n",
    "model.to(device)\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4c725-2926-43b7-aed2-9f348530cc91",
   "metadata": {},
   "source": [
    "# Visualise netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f17df69-85bb-4481-9c82-d4b36bd058b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece78c4e-df11-4a38-9175-75309a337b1b",
   "metadata": {},
   "source": [
    "Now install netron and open this file:\n",
    "\n",
    "```bash\n",
    "snap install netron\n",
    "snap run netron\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92870c6-b661-422a-8542-a3b7f0e4bac7",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac3631-feea-49f9-9dc2-d8ac19297e51",
   "metadata": {},
   "source": [
    "### lora-pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf019940-a66c-402d-ae71-f164b94cf3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bare model:  110759388\n",
      "lora: 13312300\n"
     ]
    }
   ],
   "source": [
    "# lora_model = LoRA.from_module(model, rank=50)\n",
    "# print(\"bare model: \", count_trainable_parameters(model))\n",
    "# print(\"lora:\", count_trainable_parameters(lora_model))\n",
    "# torch.save(model, \"model_lora.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf183b0f-3960-4586-ace2-567cb7365ded",
   "metadata": {},
   "source": [
    "### minlora implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317bc98-49b3-45eb-b9f7-7881112a0ae3",
   "metadata": {},
   "source": [
    "#### for PPT+PTvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b67844-b7be-4072-be00-36475b883e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import minlora\n",
    "from minlora import (\n",
    "    LoRAParametrization,\n",
    "    add_lora,\n",
    "    merge_lora,\n",
    "    remove_lora\n",
    ")\n",
    "from minlora.model import add_lora_by_name, apply_lora\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from spconv.pytorch.conv import SubMConv3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27230259-4488-4bb1-a727-34fe57fab909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before LoRA: 97447088\n",
      "after LoRA: 3314890\n"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "def configure_optimizers_lora(\n",
    "    model,\n",
    "    weight_decay: float = 0.05,\n",
    "    learning_rate: float = 0.005,\n",
    "    betas: tuple[float, float] = (0.9, 0.999),\n",
    "    device_type: str = \"cuda\"\n",
    "):\n",
    "    # we apply weight decay to all lora params\n",
    "    optim_groups = [\n",
    "        # note: .get_lora_params() returns a generator\n",
    "        # we need to wrap it in a list so we can consume it twice\n",
    "        {\"params\": list(minlora.get_lora_params(model)) , \"weight_decay\": weight_decay},\n",
    "        # you can also add biases for fine-tuning,\n",
    "        # but I want to make sure lora alone works\n",
    "        # {\"params\": minlora.get_bias_params(model), \"weight_decay\": 0.0}, # bias params don't get weight decay\n",
    "    ]\n",
    "\n",
    "    def parameter_count(optim_groups):\n",
    "        n = sum(p.numel() for group in optim_groups for p in group[\"params\"])\n",
    "        if n < 1e6:\n",
    "            return f\"{n/1e3:.1f}k\"\n",
    "        else:\n",
    "            return f\"{n/1e6:.1f}M\"\n",
    "\n",
    "    logger.info(f\"optimizing {parameter_count(optim_groups)} parameters\")\n",
    "\n",
    "    # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n",
    "    use_fused = (device_type == \"cuda\") and (\"fused\" in inspect.signature(torch.optim.AdamW).parameters)\n",
    "    logger.info(f\"using fused AdamW: {use_fused}\")\n",
    "    extra_args = dict(fused=True) if use_fused else dict()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups,\n",
    "        lr=learning_rate,\n",
    "        betas=betas,\n",
    "        **extra_args\n",
    "    )\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "lora_hparams = dict(\n",
    "    lora_dropout_p = 0.0,\n",
    "    rank=10,\n",
    "    lora_alpha = 64\n",
    ")\n",
    "\n",
    "lora_config = {\n",
    "    torch.nn.Embedding: {\n",
    "        \"weight\": partial(LoRAParametrization.from_embedding, **lora_hparams),\n",
    "    },\n",
    "    torch.nn.Linear: {\n",
    "        \"weight\": partial(LoRAParametrization.from_linear, **lora_hparams),\n",
    "    },\n",
    "    SubMConv3d: {\n",
    "        \"weight\": partial(LoRAParametrization.from_sparseconv3d, **lora_hparams),\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"before LoRA:\", count_trainable_parameters(model))\n",
    "\n",
    "def freeze_non_lora_params(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lora' not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "def unfreeze_all_params(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "freeze_non_lora_params(model)\n",
    "\n",
    "minlora.add_lora(model, lora_config=lora_config)\n",
    "print(\"after LoRA:\", count_trainable_parameters(model))\n",
    "# if use_lora:\n",
    "#     optimizer = configure_optimizers_lora(model, weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# else:\n",
    "#     optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# if init_from == 'resume':\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7004cf3-0a9f-4b45-b92a-d8d5725d5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = create_spoofed_input(device=\"cuda\", batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544ececb-6201-434a-81fa-130646d1e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:optimizing 3.3M parameters\n",
      "INFO:__main__:using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.05\n",
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999#0.95\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "optimizer = configure_optimizers_lora(\n",
    "    model,\n",
    "    weight_decay,\n",
    "    learning_rate,\n",
    "    (beta1, beta2),\n",
    "    device_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9e934-62e0-4b3f-bffe-4c7a2d05e074",
   "metadata": {},
   "source": [
    "test backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205909ba-1de1-4e2f-854d-e760b47e7561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** First Pass ***\n",
      "Initial gradients: A: 0/195, B: 194/195\n",
      "Trainable parameters with gradients: 808,320\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "Total A matrices without gradients: 195\n",
      "Total B matrices without gradients: 1\n",
      "\n",
      "Gradients after step 1:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 2:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 3:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 4:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 5:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from minlora import LoRAParametrization\n",
    "\n",
    "\n",
    "def inspect_lora_gradients(model, x, num_steps=5):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    def check_grads():\n",
    "        a_no_grad, b_no_grad = [], []\n",
    "        a_with_grad, b_with_grad = 0, 0\n",
    "        total_a, total_b = 0, 0\n",
    "        trainable_params_with_grad = 0\n",
    "        frozen_params = 0\n",
    "        total_params = 0\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            total_params += param.numel()\n",
    "            if not param.requires_grad:\n",
    "                frozen_params += param.numel()\n",
    "            elif param.grad is not None and torch.any(param.grad != 0):\n",
    "                trainable_params_with_grad += param.numel()\n",
    "\n",
    "            if 'lora_A' in name:\n",
    "                total_a += 1\n",
    "                if param.grad is None or torch.all(param.grad == 0):\n",
    "                    a_no_grad.append(name)\n",
    "                else:\n",
    "                    a_with_grad += 1\n",
    "            elif 'lora_B' in name:\n",
    "                total_b += 1\n",
    "                if param.grad is None or torch.all(param.grad == 0):\n",
    "                    b_no_grad.append(name)\n",
    "                else:\n",
    "                    b_with_grad += 1\n",
    "\n",
    "        return (a_with_grad, b_with_grad, total_a, total_b, a_no_grad, b_no_grad, \n",
    "                trainable_params_with_grad, frozen_params, total_params)\n",
    "\n",
    "    # Initial forward and backward pass\n",
    "    y = model(x)\n",
    "    loss = y[\"loss\"].sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    results = check_grads()\n",
    "    (\n",
    "        a_grad,\n",
    "        b_grad,\n",
    "        total_a,\n",
    "        total_b,\n",
    "        a_no_grad,\n",
    "        b_no_grad,\n",
    "        trainable_grad,\n",
    "        frozen,\n",
    "        total\n",
    "    ) = results\n",
    "\n",
    "    print(\"*** First Pass ***\")\n",
    "    print(f\"Initial gradients: A: {a_grad}/{total_a}, B: {b_grad}/{total_b}\")\n",
    "    print(f\"Trainable parameters with gradients: {trainable_grad:,}\")\n",
    "    print(f\"Frozen parameters: {frozen:,}\")\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    if a_no_grad:\n",
    "        print(f\"Total A matrices without gradients: {len(a_no_grad)}\")\n",
    "    if b_no_grad:\n",
    "        print(f\"Total B matrices without gradients: {len(b_no_grad)}\")\n",
    "\n",
    "    # Perform several optimization steps\n",
    "    for i in range(num_steps):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y = model(x)\n",
    "        loss = y[\"loss\"].sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        results = check_grads()\n",
    "        a_grad, b_grad, total_a, total_b, a_no_grad, b_no_grad, trainable_grad, frozen, total = results\n",
    "\n",
    "        print(f\"\\nGradients after step {i+1}:\")\n",
    "        print(f\"A: {a_grad}/{total_a}, B: {b_grad}/{total_b}\")\n",
    "        print(f\"Trainable parameters with gradients: {trainable_grad:,}\")\n",
    "        print(f\"Frozen parameters: {frozen:,}\")\n",
    "        print(f\"Total parameters: {total:,}\")\n",
    "        if a_no_grad:\n",
    "            print(f\"A matrices without gradients: {a_no_grad}\")\n",
    "        if b_no_grad:\n",
    "            print(f\"B matrices without gradients: {b_no_grad}\")\n",
    "            \n",
    "X = create_spoofed_input(device=\"cuda\", batch_size=16)\n",
    "inspect_lora_gradients(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18bc7e3-4dcf-4b95-b1e7-23067c80a7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daeedf06-df3a-4c76-ad3e-17b409fa9fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:\n",
      "Total LoRA parameters: 3,314,890\n",
      "INFO:__main__:Total non-LoRA parameters: 97,447,089\n",
      "INFO:__main__:Total parameters: 100,761,979\n",
      "INFO:__main__:Percentage of LoRA parameters: 3.29%\n"
     ]
    }
   ],
   "source": [
    "def inspect_lora_params(model):\n",
    "    lora_param_count = 0\n",
    "    non_lora_param_count = 0\n",
    "    lora_param_sizes = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        param_size = param.numel()\n",
    "        if 'lora_A' in name or 'lora_B' in name:\n",
    "            lora_param_count += param_size\n",
    "            lora_param_sizes[name] = param_size\n",
    "        else:\n",
    "            non_lora_param_count += param_size\n",
    "    \n",
    "    logger.debug(\"LoRA parameters:\")\n",
    "    for name, size in lora_param_sizes.items():\n",
    "        logger.debug(f\"  {name}: {size:,} elements\")\n",
    "    \n",
    "    logger.info(f\"\\nTotal LoRA parameters: {lora_param_count:,}\")\n",
    "    logger.info(f\"Total non-LoRA parameters: {non_lora_param_count:,}\")\n",
    "    logger.info(f\"Total parameters: {lora_param_count + non_lora_param_count:,}\")\n",
    "    \n",
    "    lora_percentage = (lora_param_count / (lora_param_count + non_lora_param_count)) * 100\n",
    "    logger.info(f\"Percentage of LoRA parameters: {lora_percentage:.2f}%\")\n",
    "    \n",
    "    return lora_param_count, non_lora_param_count\n",
    "\n",
    "# Usage\n",
    "lora_params, non_lora_params = inspect_lora_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d35664-7b6a-4cc0-b199-3dd4d138c760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])=\n",
      "feat.shape=torch.Size([16000, 512])\n",
      "self.class_embedding.shape=torch.Size([13, 512])\n",
      "sim.shape=torch.Size([16000, 13])\n"
     ]
    }
   ],
   "source": [
    "loss = model(X)\n",
    "\n",
    "loss[\"loss\"].backward()\n",
    "optimizer.step()\n",
    "#optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aa32727-0770-491e-8527-13c201299ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])=\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacity of 23.55 GiB of which 62.38 MiB is free. Including non-PyTorch memory, this process has 22.06 GiB memory in use. Of the allocated memory 21.17 GiB is allocated by PyTorch, and 435.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGradients after step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m         check_gradients()\n\u001b[0;32m---> 33\u001b[0m \u001b[43mtest_lora_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mtest_lora_gradients\u001b[0;34m(model, X, num_steps)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  B grad:\u001b[39m\u001b[38;5;124m\"\u001b[39m, param\u001b[38;5;241m.\u001b[39mlora_B\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Initial forward and backward pass\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m y[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/rvt_parametrix/point_cloud_deep_learning/Pointcept/pointcept/models/point_prompt_training/point_prompt_training_v1m1_language_guided.py:258\u001b[0m, in \u001b[0;36mPointPromptTraining.forward\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# raise ValueError\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Get features from backbone\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m feat \u001b[38;5;241m=\u001b[39m point\u001b[38;5;241m.\u001b[39mfeat \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(point, Point) \u001b[38;5;28;01melse\u001b[39;00m point\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone_mode:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/rvt_parametrix/point_cloud_deep_learning/Pointcept/pointcept/models/point_transformer_v3/point_transformer_v3m1_base.py:1035\u001b[0m, in \u001b[0;36mPointTransformerV3.forward\u001b[0;34m(self, data_dict)\u001b[0m\n\u001b[1;32m   1032\u001b[0m point\u001b[38;5;241m.\u001b[39msparsify()\n\u001b[1;32m   1034\u001b[0m point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(point)\n\u001b[0;32m-> 1035\u001b[0m point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_mode:\n\u001b[1;32m   1037\u001b[0m     point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec(point)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/rvt_parametrix/point_cloud_deep_learning/Pointcept/pointcept/models/modules.py:62\u001b[0m, in \u001b[0;36mPointSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Point module\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, PointModule):\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Spconv module\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m spconv\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mis_spconv_module(module):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/rvt_parametrix/point_cloud_deep_learning/Pointcept/pointcept/models/modules.py:62\u001b[0m, in \u001b[0;36mPointSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Point module\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, PointModule):\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Spconv module\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m spconv\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mis_spconv_module(module):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/rvt_parametrix/point_cloud_deep_learning/Pointcept/pointcept/models/point_transformer_v3/point_transformer_v3m1_base.py:333\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, point)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_norm:\n\u001b[1;32m    332\u001b[0m     point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(point)\n\u001b[0;32m--> 333\u001b[0m point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    334\u001b[0m point\u001b[38;5;241m.\u001b[39mfeat \u001b[38;5;241m=\u001b[39m shortcut \u001b[38;5;241m+\u001b[39m point\u001b[38;5;241m.\u001b[39mfeat\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_norm:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/rvt_parametrix/point_cloud_deep_learning/Pointcept/pointcept/models/modules.py:73\u001b[0m, in \u001b[0;36mPointSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# PyTorch module\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, Point):\n\u001b[0;32m---> 73\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mfeat \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_conv_feat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msparse_conv_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msparse_conv_feat\u001b[38;5;241m.\u001b[39mreplace_feature(\n\u001b[1;32m     76\u001b[0m                 \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mfeat\n\u001b[1;32m     77\u001b[0m             )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/rvt_parametrix/point_cloud_deep_learning/Pointcept/pointcept/models/point_transformer_v3/point_transformer_v3m1_base.py:243\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 243\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m    245\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacity of 23.55 GiB of which 62.38 MiB is free. Including non-PyTorch memory, this process has 22.06 GiB memory in use. Of the allocated memory 21.17 GiB is allocated by PyTorch, and 435.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from minlora import LoRAParametrization\n",
    "\n",
    "def test_lora_gradients(model, X, num_steps=5):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    def check_gradients():\n",
    "        for name, param in model.named_parameters():\n",
    "            if isinstance(param, LoRAParametrization):\n",
    "                print(f\"{name}:\")\n",
    "                print(\"  A grad:\", param.lora_A.grad)\n",
    "                print(\"  B grad:\", param.lora_B.grad)\n",
    "\n",
    "    # Initial forward and backward pass\n",
    "    y = model(X)\n",
    "    loss = y[\"loss\"].sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"Initial gradients:\")\n",
    "    check_gradients()\n",
    "\n",
    "    # Perform several optimization steps\n",
    "    for i in range(num_steps):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y = model(x)\n",
    "        loss = y[\"loss\"].sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        print(f\"\\nGradients after step {i+1}:\")\n",
    "        check_gradients()\n",
    "\n",
    "test_lora_gradients(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac5a9e93-435c-4b13-b531-709996eab8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256])=\n",
      "feat.shape=torch.Size([16000, 512])\n",
      "self.class_embedding.shape=torch.Size([13, 512])\n",
      "sim.shape=torch.Size([16000, 13])\n",
      "LoRA parameter: backbone.embedding.stem.conv.parametrizations.weight.0.lora_A (7,500 elements) - without grad\n",
      "LoRA parameter: backbone.embedding.stem.conv.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.cpe.0.parametrizations.weight.0.lora_A (12,960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.cpe.0.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.cpe.1.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.cpe.1.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.attn.qkv.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.attn.qkv.parametrizations.weight.0.lora_B (1,440 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.attn.proj.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.attn.proj.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.mlp.0.fc1.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.mlp.0.fc1.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.mlp.0.fc2.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block0.mlp.0.fc2.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.cpe.0.parametrizations.weight.0.lora_A (12,960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.cpe.0.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.cpe.1.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.cpe.1.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.attn.qkv.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.attn.qkv.parametrizations.weight.0.lora_B (1,440 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.attn.proj.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.attn.proj.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.mlp.0.fc1.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.mlp.0.fc1.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.mlp.0.fc2.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block1.mlp.0.fc2.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.cpe.0.parametrizations.weight.0.lora_A (12,960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.cpe.0.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.cpe.1.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.cpe.1.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.attn.qkv.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.attn.qkv.parametrizations.weight.0.lora_B (1,440 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.attn.proj.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.attn.proj.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.mlp.0.fc1.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.mlp.0.fc1.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.mlp.0.fc2.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc0.block2.mlp.0.fc2.parametrizations.weight.0.lora_B (480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.down.proj.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.down.proj.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.cpe.0.parametrizations.weight.0.lora_A (25,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.cpe.0.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.cpe.1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.cpe.1.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.attn.qkv.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.attn.qkv.parametrizations.weight.0.lora_B (2,880 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.attn.proj.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.attn.proj.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.mlp.0.fc1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.mlp.0.fc1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.mlp.0.fc2.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block0.mlp.0.fc2.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.cpe.0.parametrizations.weight.0.lora_A (25,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.cpe.0.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.cpe.1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.cpe.1.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.attn.qkv.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.attn.qkv.parametrizations.weight.0.lora_B (2,880 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.attn.proj.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.attn.proj.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.mlp.0.fc1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.mlp.0.fc1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.mlp.0.fc2.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block1.mlp.0.fc2.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.cpe.0.parametrizations.weight.0.lora_A (25,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.cpe.0.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.cpe.1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.cpe.1.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.attn.qkv.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.attn.qkv.parametrizations.weight.0.lora_B (2,880 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.attn.proj.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.attn.proj.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.mlp.0.fc1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.mlp.0.fc1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.mlp.0.fc2.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc1.block2.mlp.0.fc2.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.down.proj.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.down.proj.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.cpe.0.parametrizations.weight.0.lora_A (51,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.cpe.0.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.cpe.1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.cpe.1.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.attn.qkv.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.attn.qkv.parametrizations.weight.0.lora_B (5,760 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.attn.proj.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.attn.proj.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.mlp.0.fc1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.mlp.0.fc1.parametrizations.weight.0.lora_B (7,680 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.mlp.0.fc2.parametrizations.weight.0.lora_A (7,680 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block0.mlp.0.fc2.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.cpe.0.parametrizations.weight.0.lora_A (51,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.cpe.0.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.cpe.1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.cpe.1.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.attn.qkv.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.attn.qkv.parametrizations.weight.0.lora_B (5,760 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.attn.proj.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.attn.proj.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.mlp.0.fc1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.mlp.0.fc1.parametrizations.weight.0.lora_B (7,680 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.mlp.0.fc2.parametrizations.weight.0.lora_A (7,680 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block1.mlp.0.fc2.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.cpe.0.parametrizations.weight.0.lora_A (51,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.cpe.0.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.cpe.1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.cpe.1.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.attn.qkv.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.attn.qkv.parametrizations.weight.0.lora_B (5,760 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.attn.proj.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.attn.proj.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.mlp.0.fc1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.mlp.0.fc1.parametrizations.weight.0.lora_B (7,680 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.mlp.0.fc2.parametrizations.weight.0.lora_A (7,680 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc2.block2.mlp.0.fc2.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.down.proj.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.down.proj.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.cpe.0.parametrizations.weight.0.lora_A (103,680 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.cpe.0.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.cpe.1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.cpe.1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.attn.qkv.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.attn.qkv.parametrizations.weight.0.lora_B (11,520 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.attn.proj.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.attn.proj.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.mlp.0.fc1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.mlp.0.fc1.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.mlp.0.fc2.parametrizations.weight.0.lora_A (15,360 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block0.mlp.0.fc2.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.cpe.0.parametrizations.weight.0.lora_A (103,680 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.cpe.0.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.cpe.1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.cpe.1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.attn.qkv.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.attn.qkv.parametrizations.weight.0.lora_B (11,520 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.attn.proj.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.attn.proj.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.mlp.0.fc1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.mlp.0.fc1.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.mlp.0.fc2.parametrizations.weight.0.lora_A (15,360 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block1.mlp.0.fc2.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.cpe.0.parametrizations.weight.0.lora_A (103,680 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.cpe.0.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.cpe.1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.cpe.1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.attn.qkv.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.attn.qkv.parametrizations.weight.0.lora_B (11,520 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.attn.proj.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.attn.proj.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.mlp.0.fc1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.mlp.0.fc1.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.mlp.0.fc2.parametrizations.weight.0.lora_A (15,360 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block2.mlp.0.fc2.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.cpe.0.parametrizations.weight.0.lora_A (103,680 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.cpe.0.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.cpe.1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.cpe.1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.attn.qkv.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.attn.qkv.parametrizations.weight.0.lora_B (11,520 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.attn.proj.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.attn.proj.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.mlp.0.fc1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.mlp.0.fc1.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.mlp.0.fc2.parametrizations.weight.0.lora_A (15,360 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block3.mlp.0.fc2.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.cpe.0.parametrizations.weight.0.lora_A (103,680 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.cpe.0.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.cpe.1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.cpe.1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.attn.qkv.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.attn.qkv.parametrizations.weight.0.lora_B (11,520 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.attn.proj.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.attn.proj.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.mlp.0.fc1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.mlp.0.fc1.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.mlp.0.fc2.parametrizations.weight.0.lora_A (15,360 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block4.mlp.0.fc2.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.cpe.0.parametrizations.weight.0.lora_A (103,680 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.cpe.0.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.cpe.1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.cpe.1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.attn.qkv.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.attn.qkv.parametrizations.weight.0.lora_B (11,520 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.attn.proj.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.attn.proj.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.mlp.0.fc1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.mlp.0.fc1.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.mlp.0.fc2.parametrizations.weight.0.lora_A (15,360 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc3.block5.mlp.0.fc2.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.down.proj.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.down.proj.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.cpe.0.parametrizations.weight.0.lora_A (138,240 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.cpe.0.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.cpe.1.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.cpe.1.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.attn.qkv.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.attn.qkv.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.attn.proj.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.attn.proj.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.mlp.0.fc1.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.mlp.0.fc1.parametrizations.weight.0.lora_B (20,480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.mlp.0.fc2.parametrizations.weight.0.lora_A (20,480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block0.mlp.0.fc2.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.cpe.0.parametrizations.weight.0.lora_A (138,240 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.cpe.0.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.cpe.1.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.cpe.1.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.attn.qkv.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.attn.qkv.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.attn.proj.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.attn.proj.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.mlp.0.fc1.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.mlp.0.fc1.parametrizations.weight.0.lora_B (20,480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.mlp.0.fc2.parametrizations.weight.0.lora_A (20,480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block1.mlp.0.fc2.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.cpe.0.parametrizations.weight.0.lora_A (138,240 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.cpe.0.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.cpe.1.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.cpe.1.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.attn.qkv.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.attn.qkv.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.attn.proj.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.attn.proj.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.mlp.0.fc1.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.mlp.0.fc1.parametrizations.weight.0.lora_B (20,480 elements) - with grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.mlp.0.fc2.parametrizations.weight.0.lora_A (20,480 elements) - without grad\n",
      "LoRA parameter: backbone.enc.enc4.block2.mlp.0.fc2.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.up.proj.0.parametrizations.weight.0.lora_A (5,120 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.up.proj.0.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.up.proj_skip.0.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.up.proj_skip.0.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.cpe.0.parametrizations.weight.0.lora_A (103,680 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.cpe.0.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.cpe.1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.cpe.1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.attn.qkv.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.attn.qkv.parametrizations.weight.0.lora_B (11,520 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.attn.proj.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.attn.proj.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.mlp.0.fc1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.mlp.0.fc1.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.mlp.0.fc2.parametrizations.weight.0.lora_A (15,360 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block0.mlp.0.fc2.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.cpe.0.parametrizations.weight.0.lora_A (103,680 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.cpe.0.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.cpe.1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.cpe.1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.attn.qkv.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.attn.qkv.parametrizations.weight.0.lora_B (11,520 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.attn.proj.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.attn.proj.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.mlp.0.fc1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.mlp.0.fc1.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.mlp.0.fc2.parametrizations.weight.0.lora_A (15,360 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block1.mlp.0.fc2.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.cpe.0.parametrizations.weight.0.lora_A (103,680 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.cpe.0.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.cpe.1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.cpe.1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.attn.qkv.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.attn.qkv.parametrizations.weight.0.lora_B (11,520 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.attn.proj.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.attn.proj.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.mlp.0.fc1.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.mlp.0.fc1.parametrizations.weight.0.lora_B (15,360 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.mlp.0.fc2.parametrizations.weight.0.lora_A (15,360 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec3.block2.mlp.0.fc2.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.up.proj.0.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.up.proj.0.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.up.proj_skip.0.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.up.proj_skip.0.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.cpe.0.parametrizations.weight.0.lora_A (51,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.cpe.0.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.cpe.1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.cpe.1.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.attn.qkv.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.attn.qkv.parametrizations.weight.0.lora_B (5,760 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.attn.proj.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.attn.proj.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.mlp.0.fc1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.mlp.0.fc1.parametrizations.weight.0.lora_B (7,680 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.mlp.0.fc2.parametrizations.weight.0.lora_A (7,680 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block0.mlp.0.fc2.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.cpe.0.parametrizations.weight.0.lora_A (51,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.cpe.0.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.cpe.1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.cpe.1.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.attn.qkv.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.attn.qkv.parametrizations.weight.0.lora_B (5,760 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.attn.proj.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.attn.proj.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.mlp.0.fc1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.mlp.0.fc1.parametrizations.weight.0.lora_B (7,680 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.mlp.0.fc2.parametrizations.weight.0.lora_A (7,680 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block1.mlp.0.fc2.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.cpe.0.parametrizations.weight.0.lora_A (51,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.cpe.0.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.cpe.1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.cpe.1.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.attn.qkv.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.attn.qkv.parametrizations.weight.0.lora_B (5,760 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.attn.proj.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.attn.proj.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.mlp.0.fc1.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.mlp.0.fc1.parametrizations.weight.0.lora_B (7,680 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.mlp.0.fc2.parametrizations.weight.0.lora_A (7,680 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec2.block2.mlp.0.fc2.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.up.proj.0.parametrizations.weight.0.lora_A (1,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.up.proj.0.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.up.proj_skip.0.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.up.proj_skip.0.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.cpe.0.parametrizations.weight.0.lora_A (25,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.cpe.0.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.cpe.1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.cpe.1.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.attn.qkv.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.attn.qkv.parametrizations.weight.0.lora_B (2,880 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.attn.proj.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.attn.proj.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.mlp.0.fc1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.mlp.0.fc1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.mlp.0.fc2.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block0.mlp.0.fc2.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.cpe.0.parametrizations.weight.0.lora_A (25,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.cpe.0.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.cpe.1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.cpe.1.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.attn.qkv.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.attn.qkv.parametrizations.weight.0.lora_B (2,880 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.attn.proj.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.attn.proj.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.mlp.0.fc1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.mlp.0.fc1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.mlp.0.fc2.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block1.mlp.0.fc2.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.cpe.0.parametrizations.weight.0.lora_A (25,920 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.cpe.0.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.cpe.1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.cpe.1.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.attn.qkv.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.attn.qkv.parametrizations.weight.0.lora_B (2,880 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.attn.proj.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.attn.proj.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.mlp.0.fc1.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.mlp.0.fc1.parametrizations.weight.0.lora_B (3,840 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.mlp.0.fc2.parametrizations.weight.0.lora_A (3,840 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec1.block2.mlp.0.fc2.parametrizations.weight.0.lora_B (960 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.up.proj.0.parametrizations.weight.0.lora_A (960 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.up.proj.0.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.up.proj_skip.0.parametrizations.weight.0.lora_A (480 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.up.proj_skip.0.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.cpe.0.parametrizations.weight.0.lora_A (17,280 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.cpe.0.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.cpe.1.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.cpe.1.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.attn.qkv.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.attn.qkv.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.attn.proj.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.attn.proj.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.mlp.0.fc1.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.mlp.0.fc1.parametrizations.weight.0.lora_B (2,560 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.mlp.0.fc2.parametrizations.weight.0.lora_A (2,560 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block0.mlp.0.fc2.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.cpe.0.parametrizations.weight.0.lora_A (17,280 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.cpe.0.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.cpe.1.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.cpe.1.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.attn.qkv.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.attn.qkv.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.attn.proj.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.attn.proj.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.mlp.0.fc1.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.mlp.0.fc1.parametrizations.weight.0.lora_B (2,560 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.mlp.0.fc2.parametrizations.weight.0.lora_A (2,560 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block1.mlp.0.fc2.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.cpe.0.parametrizations.weight.0.lora_A (17,280 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.cpe.0.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.cpe.1.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.cpe.1.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.attn.qkv.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.attn.qkv.parametrizations.weight.0.lora_B (1,920 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.attn.proj.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.attn.proj.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.mlp.0.fc1.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.mlp.0.fc1.parametrizations.weight.0.lora_B (2,560 elements) - with grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.mlp.0.fc2.parametrizations.weight.0.lora_A (2,560 elements) - without grad\n",
      "LoRA parameter: backbone.dec.dec0.block2.mlp.0.fc2.parametrizations.weight.0.lora_B (640 elements) - with grad\n",
      "LoRA parameter: embedding_table.parametrizations.weight.0.lora_A (30 elements) - without grad\n",
      "LoRA parameter: embedding_table.parametrizations.weight.0.lora_B (2,560 elements) - without grad\n",
      "LoRA parameter: proj_head.parametrizations.weight.0.lora_A (640 elements) - without grad\n",
      "LoRA parameter: proj_head.parametrizations.weight.0.lora_B (5,120 elements) - with grad\n",
      "\n",
      "Total LoRA parameters: 3,314,890\n",
      "LoRA parameters with gradients: 808,320\n",
      "Total non-LoRA parameters: 97,447,089\n",
      "Non-LoRA parameters with gradients: 97,350,832\n",
      "Total parameters: 100,761,979\n",
      "Percentage of LoRA parameters: 3.29%\n"
     ]
    }
   ],
   "source": [
    "# def inspect_lora_params_and_gradients(model, X):\n",
    "#     # Forward and backward pass\n",
    "#     loss = model(X)\n",
    "#     loss[\"loss\"].backward()\n",
    "    \n",
    "#     lora_param_count = 0\n",
    "#     lora_param_with_grad_count = 0\n",
    "#     non_lora_param_count = 0\n",
    "#     non_lora_param_with_grad_count = 0\n",
    "    \n",
    "#     for name, param in model.named_parameters():\n",
    "#         param_size = param.numel()\n",
    "#         is_lora = 'lora_A' in name or 'lora_B' in name\n",
    "        \n",
    "#         if is_lora:\n",
    "#             lora_param_count += param_size\n",
    "#             if param.grad is not None and param.grad.abs().sum().item() > 0:\n",
    "#                 lora_param_with_grad_count += param_size\n",
    "#         else:\n",
    "#             non_lora_param_count += param_size\n",
    "#             if param.grad is not None and param.grad.abs().sum().item() > 0:\n",
    "#                 non_lora_param_with_grad_count += param_size\n",
    "        \n",
    "#         if is_lora:\n",
    "#             grad_status = \"with grad\" if param.grad is not None and param.grad.abs().sum().item() > 0 else \"without grad\"\n",
    "#             print(f\"LoRA parameter: {name} ({param_size:,} elements) - {grad_status}\")\n",
    "    \n",
    "#     print(f\"\\nTotal LoRA parameters: {lora_param_count:,}\")\n",
    "#     print(f\"LoRA parameters with gradients: {lora_param_with_grad_count:,}\")\n",
    "#     print(f\"Total non-LoRA parameters: {non_lora_param_count:,}\")\n",
    "#     print(f\"Non-LoRA parameters with gradients: {non_lora_param_with_grad_count:,}\")\n",
    "#     print(f\"Total parameters: {lora_param_count + non_lora_param_count:,}\")\n",
    "    \n",
    "#     lora_percentage = (lora_param_count / (lora_param_count + non_lora_param_count)) * 100\n",
    "#     print(f\"Percentage of LoRA parameters: {lora_percentage:.2f}%\")\n",
    "    \n",
    "#     # Optional: zero out gradients\n",
    "#     model.zero_grad()\n",
    "    \n",
    "#     return lora_param_count, lora_param_with_grad_count, non_lora_param_count, non_lora_param_with_grad_count\n",
    "\n",
    "# # Usage\n",
    "# lora_params, lora_grads, non_lora_params, non_lora_grads = inspect_lora_params_and_gradients(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4582c4b7-2f5a-4d6c-a8b0-965af4a508f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff3c868-354c-4251-9d3c-de175ac71b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module backbone.enc.enc0.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module proj_head:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "def showlora(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d, nn.MultiheadAttention)):\n",
    "            print(f\"Module {name}:\")\n",
    "            if hasattr(module, 'parametrizations'):\n",
    "                for param_name, param in module.parametrizations.items():\n",
    "                    print(f\"  - {param_name} LoRA parameters:\")\n",
    "                    for lora_name, lora_param in param.named_parameters():\n",
    "                        print(f\"    - {lora_name}: device = {lora_param.device}\")\n",
    "            elif isinstance(module, nn.MultiheadAttention):\n",
    "                if hasattr(module.out_proj, 'parametrizations'):\n",
    "                    for param_name, param in module.out_proj.parametrizations.items():\n",
    "                        print(f\"  - out_proj.{param_name} LoRA parameters:\")\n",
    "                        for lora_name, lora_param in param.named_parameters():\n",
    "                            print(f\"    - {lora_name}: device = {lora_param.device}\")\n",
    "\n",
    "showlora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd82726-a2ce-4fc5-bcb2-54535a6d7460",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Serialization of parametrized modules is only supported through state_dict(). See:\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_minlora.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/serialization.py:629\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 629\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/serialization.py:841\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    839\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m    840\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m--> 841\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    843\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/utils/parametrize.py:305\u001b[0m, in \u001b[0;36m_inject_new_class.<locals>.getstate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSerialization of parametrized modules is only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported through state_dict(). See:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/tutorials/beginner/saving_loading_models.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Serialization of parametrized modules is only supported through state_dict(). See:\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"model_minlora.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7011af-0d97-46b4-94a0-e4b2707e46c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba4a57-ab4c-47d7-b9bf-bb32267bab67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8478a-94cd-41cc-bb66-168c02e22551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca56452-a63a-41a3-ad42-a7afc0df6d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd04c52-fdca-464d-943c-3373fc528132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef7074-15d7-463d-942b-c6f1facae827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18bd2de-5b12-45c3-9f35-abcdc24c64b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516088c-9a06-457e-8b55-df4ca94f2d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df44cae0-ba50-4ca3-ad1a-3e8cdd5cd0d1",
   "metadata": {},
   "source": [
    "### custom implementation (claude, unchecked but it runs lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56f4925-2b1d-4961-afc5-6c98cf7a39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.scale = 0.01\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.lora_A.T @ self.lora_B.T) * self.scale\n",
    "\n",
    "class AdaptiveLoRAWrapper(nn.Module):\n",
    "    def __init__(self, base_layer, rank=4):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        if hasattr(base_layer, 'weight'):\n",
    "            weight = base_layer.weight\n",
    "            in_features, out_features = weight.shape[1], weight.shape[0]\n",
    "        elif hasattr(base_layer, 'in_features') and hasattr(base_layer, 'out_features'):\n",
    "            in_features, out_features = base_layer.in_features, base_layer.out_features\n",
    "        else:\n",
    "            raise ValueError(f\"Unable to determine in_features and out_features for {type(base_layer)}\")\n",
    "        self.lora = LoRALayer(in_features, out_features, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_layer(x) + self.lora(x)\n",
    "\n",
    "def get_in_out_features(layer):\n",
    "    if hasattr(layer, 'in_features') and hasattr(layer, 'out_features'):\n",
    "        return layer.in_features, layer.out_features\n",
    "    elif hasattr(layer, 'weight'):\n",
    "        return layer.weight.shape[1], layer.weight.shape[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to determine in_features and out_features for {type(layer)}\")\n",
    "\n",
    "class LoRAQKV(nn.Module):\n",
    "    def __init__(self, qkv_layer, rank=4):\n",
    "        super().__init__()\n",
    "        self.qkv_layer = qkv_layer\n",
    "        in_features, out_features = get_in_out_features(qkv_layer)\n",
    "        self.lora = LoRALayer(in_features, out_features, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.qkv_layer(x) + self.lora(x)\n",
    "\n",
    "def apply_lora_to_ptv3(model, rank=4):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, SerializedAttention):\n",
    "            module.qkv = LoRAQKV(module.qkv, rank)\n",
    "            module.proj = AdaptiveLoRAWrapper(module.proj, rank)\n",
    "        elif isinstance(module, MLP):\n",
    "            module.fc1 = AdaptiveLoRAWrapper(module.fc1, rank)\n",
    "            module.fc2 = AdaptiveLoRAWrapper(module.fc2, rank)\n",
    "\n",
    "def apply_lora_to_ppt(model, rank=4):\n",
    "    # Apply LoRA to PT-v3 backbone\n",
    "    apply_lora_to_ptv3(model.backbone, rank)\n",
    "    \n",
    "    # Apply LoRA to the projection head\n",
    "    model.proj_head = AdaptiveLoRAWrapper(model.proj_head, rank)\n",
    "\n",
    "    def freeze_non_lora_params(model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'lora' not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    freeze_non_lora_params(model)\n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "# ppt_model = PointPromptTraining(...)\n",
    "# ppt_model_with_lora = apply_lora_to_ppt(ppt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63cb43c1-07d4-4dbd-8750-996c067d5c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_model_with_lora = apply_lora_to_ppt(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e371a72-40b1-4b14-b25f-849320d726a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointcept.models.point_transformer_v3 import SerializedAttention, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49b62fda-35af-416b-bb18-00942c4fa9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453888"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_parameters(ppt_model_with_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b67cec-fe12-465d-92e6-b09f227082d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97979580"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d061f4e-b415-4cce-b5ce-68fb2f8b1a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
