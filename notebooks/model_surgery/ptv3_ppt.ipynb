{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97aa1631-bdf1-446f-8a7a-12eb0de74245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-23 21:15:46,222 INFO test.py line 41 81188] => Loading config ...\n",
      "[2024-08-23 21:15:46,223 INFO test.py line 48 81188] => Building model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proj_head shape says Linear(in_features=64, out_features=512, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-23 21:15:49,005 INFO test.py line 61 81188] Num params: 97447088\n",
      "[2024-08-23 21:15:49,213 INFO test.py line 68 81188] Loading weight at: ../../models/PointTransformerV3/scannet-semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth\n",
      "[2024-08-23 21:15:49,859 INFO test.py line 84 81188] => Loaded weight '../../models/PointTransformerV3/scannet-semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth' (epoch 94)\n",
      "[2024-08-23 21:15:49,864 INFO test.py line 53 81188] => Building test dataset & dataloader ...\n",
      "[2024-08-23 21:15:49,866 INFO scannet.py line 72 81188] Totally 0 x 1 samples in val set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DITCHING CLASS EMBEDDING\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "import graphviz\n",
    "graphviz.set_jupyter_format('svg')\n",
    "from lora_pytorch import LoRA\n",
    "assert torch.cuda.is_available()\n",
    "from torchview import draw_graph\n",
    "from torchviz import make_dot\n",
    "from graphviz import Digraph\n",
    "\n",
    "from pointcept.engines.defaults import (\n",
    "    default_argument_parser,\n",
    "    default_config_parser,\n",
    "    default_setup,\n",
    ")\n",
    "from pointcept.engines.test import TESTERS\n",
    "from pointcept.engines.launch import launch\n",
    "from pointcept.engines.test import TesterBase, SemSegTester\n",
    "\n",
    "repo_root = Path(\"../..\")\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def create_spoofed_input(batch_size=2, num_points=1000, n_classes=5, num_features=6, device='cpu'):\n",
    "    return {\n",
    "        'coord': torch.rand(num_points * batch_size, num_features, device=device),\n",
    "        'feat': torch.rand(num_points * batch_size, num_features, device=device),\n",
    "        'grid_coord': torch.randint(0, 100, (num_points * batch_size, 3), device=device),\n",
    "        'batch': torch.arange(batch_size, device=device).repeat_interleave(num_points),\n",
    "        'offset': torch.tensor([num_points * i for i in range(1, batch_size + 1)], device=device),\n",
    "        'condition': ['ScanNet'] * batch_size,\n",
    "        'grid_size': torch.tensor([0.01], device=device),\n",
    "        'segment': torch.randint(low=0, high=n_classes-1, size=(num_points * batch_size,), device=device)\n",
    "    }\n",
    "\n",
    "\n",
    "def patch_cfg(cfg: dict, repo_root: Path = repo_root) -> dict:\n",
    "    cfg = cfg.copy()\n",
    "    cfg[\"my_data_root\"] = repo_root / cfg[\"my_data_root\"]\n",
    "    cfg[\"weight\"] = repo_root / cfg[\"weight\"]\n",
    "    cfg[\"batch_size_test_per_gpu\"] = 1\n",
    "    return cfg\n",
    "\n",
    "\n",
    "repo_root = Path(\"../..\")\n",
    "cfg_file = Path(\"../../test/custom-ppt-config.py\"); assert cfg_file.exists\n",
    "\n",
    "args = default_argument_parser().parse_args(args=[\"--config-file\", f\"{cfg_file}\"])\n",
    "cfg = default_config_parser(args.config_file, args.options); cfg = patch_cfg(cfg)\n",
    "\n",
    "tester = TESTERS.build(dict(type=cfg.test.type, cfg=cfg))\n",
    "model = tester.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4c725-2926-43b7-aed2-9f348530cc91",
   "metadata": {},
   "source": [
    "# Visualise netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f17df69-85bb-4481-9c82-d4b36bd058b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece78c4e-df11-4a38-9175-75309a337b1b",
   "metadata": {},
   "source": [
    "Now install netron and open this file:\n",
    "\n",
    "```bash\n",
    "snap install netron\n",
    "snap run netron\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92870c6-b661-422a-8542-a3b7f0e4bac7",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301b9e8-b406-44e0-9e8e-0ed4f5a8ba81",
   "metadata": {},
   "source": [
    "Compare implementations, need to understand wtf is going on in lora-pytorch with the custom MHA implementation and why this isn't necessary in the pytora + claude-generated solutions\n",
    "\n",
    "maybe need to compare param counts and see the difference in their behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac3631-feea-49f9-9dc2-d8ac19297e51",
   "metadata": {},
   "source": [
    "### lora-pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf019940-a66c-402d-ae71-f164b94cf3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bare model:  110759388\n",
      "lora: 13312300\n"
     ]
    }
   ],
   "source": [
    "lora_model = LoRA.from_module(model, rank=50)\n",
    "print(\"bare model: \", count_trainable_parameters(model))\n",
    "print(\"lora:\", count_trainable_parameters(lora_model))\n",
    "torch.save(model, \"model_lora.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76140c1e-d0ec-4dbf-bb16-2f62dc4fc47c",
   "metadata": {},
   "source": [
    "### pytora implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6fbb5-2525-4ba2-8bd5-eaaed1102c1c",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df44cae0-ba50-4ca3-ad1a-3e8cdd5cd0d1",
   "metadata": {},
   "source": [
    "### custom implementation (claude, unchecked but it runs lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b56f4925-2b1d-4961-afc5-6c98cf7a39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=4):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.scale = 0.01\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.lora_A.T @ self.lora_B.T) * self.scale\n",
    "\n",
    "class AdaptiveLoRAWrapper(nn.Module):\n",
    "    def __init__(self, base_layer, rank=4):\n",
    "        super().__init__()\n",
    "        self.base_layer = base_layer\n",
    "        if hasattr(base_layer, 'weight'):\n",
    "            weight = base_layer.weight\n",
    "            in_features, out_features = weight.shape[1], weight.shape[0]\n",
    "        elif hasattr(base_layer, 'in_features') and hasattr(base_layer, 'out_features'):\n",
    "            in_features, out_features = base_layer.in_features, base_layer.out_features\n",
    "        else:\n",
    "            raise ValueError(f\"Unable to determine in_features and out_features for {type(base_layer)}\")\n",
    "        self.lora = LoRALayer(in_features, out_features, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_layer(x) + self.lora(x)\n",
    "\n",
    "def get_in_out_features(layer):\n",
    "    if hasattr(layer, 'in_features') and hasattr(layer, 'out_features'):\n",
    "        return layer.in_features, layer.out_features\n",
    "    elif hasattr(layer, 'weight'):\n",
    "        return layer.weight.shape[1], layer.weight.shape[0]\n",
    "    else:\n",
    "        raise ValueError(f\"Unable to determine in_features and out_features for {type(layer)}\")\n",
    "\n",
    "class LoRAQKV(nn.Module):\n",
    "    def __init__(self, qkv_layer, rank=4):\n",
    "        super().__init__()\n",
    "        self.qkv_layer = qkv_layer\n",
    "        in_features, out_features = get_in_out_features(qkv_layer)\n",
    "        self.lora = LoRALayer(in_features, out_features, rank)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.qkv_layer(x) + self.lora(x)\n",
    "\n",
    "def apply_lora_to_ptv3(model, rank=4):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, SerializedAttention):\n",
    "            module.qkv = LoRAQKV(module.qkv, rank)\n",
    "            module.proj = AdaptiveLoRAWrapper(module.proj, rank)\n",
    "        elif isinstance(module, MLP):\n",
    "            module.fc1 = AdaptiveLoRAWrapper(module.fc1, rank)\n",
    "            module.fc2 = AdaptiveLoRAWrapper(module.fc2, rank)\n",
    "\n",
    "def apply_lora_to_ppt(model, rank=4):\n",
    "    # Apply LoRA to PT-v3 backbone\n",
    "    apply_lora_to_ptv3(model.backbone, rank)\n",
    "    \n",
    "    # Apply LoRA to the projection head\n",
    "    model.proj_head = AdaptiveLoRAWrapper(model.proj_head, rank)\n",
    "\n",
    "    def freeze_non_lora_params(model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'lora' not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    freeze_non_lora_params(model)\n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "# ppt_model = PointPromptTraining(...)\n",
    "# ppt_model_with_lora = apply_lora_to_ppt(ppt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63cb43c1-07d4-4dbd-8750-996c067d5c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt_model_with_lora = apply_lora_to_ppt(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e371a72-40b1-4b14-b25f-849320d726a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pointcept.models.point_transformer_v3 import SerializedAttention, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49b62fda-35af-416b-bb18-00942c4fa9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "453888"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_trainable_parameters(ppt_model_with_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67928735-707c-4784-9f52-e3201b8bbd75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointPromptTraining(\n",
       "  (backbone): PointTransformerV3(\n",
       "    (embedding): Embedding(\n",
       "      (stem): PointSequential(\n",
       "        (conv): SubMConv3d(6, 48, kernel_size=[5, 5, 5], stride=[1, 1, 1], padding=[1, 1, 1], dilation=[1, 1, 1], output_padding=[0, 0, 0], bias=False, algo=ConvAlgo.Native)\n",
       "        (norm): PDNorm(\n",
       "          (norm): ModuleList(\n",
       "            (0-2): 3 x BatchNorm1d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "    (enc): PointSequential(\n",
       "      (enc0): PointSequential(\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=48, out_features=144, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=48, out_features=192, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=192, out_features=48, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=48, out_features=144, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=48, out_features=192, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=192, out_features=48, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.018)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(48, 48, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=48, out_features=48, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=48, out_features=144, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=48, out_features=48, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((48,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=48, out_features=192, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=192, out_features=48, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.035)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (enc1): PointSequential(\n",
       "        (down): SerializedPooling(\n",
       "          (proj): Linear(in_features=48, out_features=96, bias=True)\n",
       "          (norm): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (act): PointSequential(\n",
       "            (0): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.053)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.071)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.088)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (enc2): PointSequential(\n",
       "        (down): SerializedPooling(\n",
       "          (proj): Linear(in_features=96, out_features=192, bias=True)\n",
       "          (norm): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (act): PointSequential(\n",
       "            (0): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.106)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.124)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.141)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (enc3): PointSequential(\n",
       "        (down): SerializedPooling(\n",
       "          (proj): Linear(in_features=192, out_features=384, bias=True)\n",
       "          (norm): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (act): PointSequential(\n",
       "            (0): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.159)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.176)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.194)\n",
       "          )\n",
       "        )\n",
       "        (block3): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.212)\n",
       "          )\n",
       "        )\n",
       "        (block4): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.229)\n",
       "          )\n",
       "        )\n",
       "        (block5): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.247)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (enc4): PointSequential(\n",
       "        (down): SerializedPooling(\n",
       "          (proj): Linear(in_features=384, out_features=512, bias=True)\n",
       "          (norm): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(512, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (act): PointSequential(\n",
       "            (0): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(512, 512, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.265)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(512, 512, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.282)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(512, 512, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((512,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.300)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dec): PointSequential(\n",
       "      (dec3): PointSequential(\n",
       "        (up): SerializedUnpooling(\n",
       "          (proj): PointSequential(\n",
       "            (0): Linear(in_features=512, out_features=384, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (proj_skip): PointSequential(\n",
       "            (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(384, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.300)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.273)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(384, 384, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((384,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.245)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dec2): PointSequential(\n",
       "        (up): SerializedUnpooling(\n",
       "          (proj): PointSequential(\n",
       "            (0): Linear(in_features=384, out_features=192, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (proj_skip): PointSequential(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.218)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.191)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(192, 192, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=192, out_features=576, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=192, out_features=192, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((192,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.164)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dec1): PointSequential(\n",
       "        (up): SerializedUnpooling(\n",
       "          (proj): PointSequential(\n",
       "            (0): Linear(in_features=192, out_features=96, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (proj_skip): PointSequential(\n",
       "            (0): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.136)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.109)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(96, 96, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=96, out_features=288, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=96, out_features=96, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((96,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.082)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (dec0): PointSequential(\n",
       "        (up): SerializedUnpooling(\n",
       "          (proj): PointSequential(\n",
       "            (0): Linear(in_features=96, out_features=64, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "          (proj_skip): PointSequential(\n",
       "            (0): Linear(in_features=48, out_features=64, bias=True)\n",
       "            (1): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x BatchNorm1d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (2): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (block0): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.055)\n",
       "          )\n",
       "        )\n",
       "        (block1): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): DropPath(drop_prob=0.027)\n",
       "          )\n",
       "        )\n",
       "        (block2): Block(\n",
       "          (cpe): PointSequential(\n",
       "            (0): SubMConv3d(64, 64, kernel_size=[3, 3, 3], stride=[1, 1, 1], padding=[0, 0, 0], dilation=[1, 1, 1], output_padding=[0, 0, 0], algo=ConvAlgo.MaskImplicitGemm)\n",
       "            (1): Linear(in_features=64, out_features=64, bias=True)\n",
       "            (2): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (norm1): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (attn): SerializedAttention(\n",
       "            (qkv): LoRAQKV(\n",
       "              (qkv_layer): Linear(in_features=64, out_features=192, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj): AdaptiveLoRAWrapper(\n",
       "              (base_layer): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (lora): LoRALayer()\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm2): PointSequential(\n",
       "            (0): PDNorm(\n",
       "              (norm): ModuleList(\n",
       "                (0-2): 3 x LayerNorm((64,), eps=0.001, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): PointSequential(\n",
       "            (0): MLP(\n",
       "              (fc1): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (act): GELU(approximate='none')\n",
       "              (fc2): AdaptiveLoRAWrapper(\n",
       "                (base_layer): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (lora): LoRALayer()\n",
       "              )\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): PointSequential(\n",
       "            (0): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (embedding_table): Embedding(3, 256)\n",
       "  (proj_head): AdaptiveLoRAWrapper(\n",
       "    (base_layer): Linear(in_features=64, out_features=512, bias=True)\n",
       "    (lora): LoRALayer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppt_model_with_lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b67cec-fe12-465d-92e6-b09f227082d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
