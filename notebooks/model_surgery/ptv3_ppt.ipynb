{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97aa1631-bdf1-446f-8a7a-12eb0de74245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-09-02 17:11:05,584 INFO test.py line 41 36491] => Loading config ...\n",
      "[2024-09-02 17:11:05,585 INFO test.py line 48 36491] => Building model ...\n",
      "[2024-09-02 17:11:08,260 INFO test.py line 61 36491] Num params: 97447088\n",
      "[2024-09-02 17:11:08,462 INFO test.py line 68 36491] Loading weight at: ../../models/PointTransformerV3/scannet-semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth\n",
      "[2024-09-02 17:11:08,961 INFO test.py line 84 36491] => Loaded weight '../../models/PointTransformerV3/scannet-semseg-pt-v3m1-1-ppt-extreme/model/model_best.pth' (epoch 94)\n",
      "[2024-09-02 17:11:08,965 INFO test.py line 53 36491] => Building test dataset & dataloader ...\n",
      "[2024-09-02 17:11:08,967 INFO scannet.py line 72 36491] Totally 0 x 1 samples in val set.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import inspect\n",
    "import logging\n",
    "import typing as ty\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import webbrowser\n",
    "import graphviz\n",
    "import minlora\n",
    "from minlora import (\n",
    "    LoRAParametrization,\n",
    "    add_lora,\n",
    "    merge_lora,\n",
    "    remove_lora\n",
    ")\n",
    "from minlora.utils import get_params_by_name, name_is_lora\n",
    "from minlora.model import add_lora_by_name, apply_lora\n",
    "from torch.optim import AdamW\n",
    "from spconv.pytorch.conv import SubMConv3d\n",
    "graphviz.set_jupyter_format('svg')\n",
    "from lora_pytorch import LoRA\n",
    "assert torch.cuda.is_available()\n",
    "from torchview import draw_graph\n",
    "from torchviz import make_dot\n",
    "from graphviz import Digraph\n",
    "\n",
    "from pointcept.engines.defaults import (\n",
    "    default_argument_parser,\n",
    "    default_config_parser,\n",
    "    default_setup,\n",
    ")\n",
    "from pointcept.engines.test import TESTERS\n",
    "from pointcept.engines.launch import launch\n",
    "from pointcept.engines.test import TesterBase, SemSegTester\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "repo_root = Path(\"../..\")\n",
    "\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    return dict(\n",
    "        trainable=sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        frozen=sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "    )\n",
    "\n",
    "def named_trainable_parameters(model):\n",
    "    return dict(\n",
    "        trainable=[n for n, p in model.named_parameters() if p.requires_grad],\n",
    "        frozen=[n for n, p in model.named_parameters() if not p.requires_grad]\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_named_params(model, name_filter: ty.Callable) -> tuple[str, nn.Parameter]:\n",
    "    \"\"\"\n",
    "    generator which returns (parameter_name, weight tensor)\n",
    "    for all tensors whose names match the filter function\n",
    "    \"\"\"\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_filter(n):\n",
    "            yield n, p\n",
    "\n",
    "get_named_lora_params = partial(filter_named_params, name_filter=name_is_lora)\n",
    "get_named_non_lora_params = partial(filter_named_params, name_filter=(lambda x: not name_is_lora(x)))\n",
    "\n",
    "def count_lora_parameters(model):\n",
    "    \"\"\"use minlora directly\"\"\"\n",
    "    return sum(p.numel() for p in minlora.get_lora_params(model))\n",
    "    \n",
    "def count_lora_params_manual(model):\n",
    "    \"\"\"just looking at weight tensor names manually as a cross check\"\"\"\n",
    "    return sum(p.numel() for n, p in get_named_lora_params(model))\n",
    "\n",
    "def check_lora_trainable(model):\n",
    "    for param in minlora.get_lora_params(model):\n",
    "        assert param.requires_grad\n",
    "\n",
    "def freeze_non_lora_params(model):\n",
    "    print(\"freezing non-lora parameters\")\n",
    "    frozen_param_names = set()\n",
    "    for name, param in get_named_non_lora_params(model):\n",
    "        param.requires_grad = False\n",
    "        frozen_param_names.add(name)\n",
    "    yield\n",
    "    print(\"unfreezing\")\n",
    "\n",
    "def unfreeze_all_params(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "def create_spoofed_input(batch_size=2, num_points=1000, n_classes=5, num_features=6, device='cpu'):\n",
    "    return {\n",
    "        'coord': torch.rand(num_points * batch_size, num_features, device=device),\n",
    "        'feat': torch.rand(num_points * batch_size, num_features, device=device),\n",
    "        'grid_coord': torch.randint(0, 100, (num_points * batch_size, 3), device=device),\n",
    "        'batch': torch.arange(batch_size, device=device).repeat_interleave(num_points),\n",
    "        'offset': torch.tensor([num_points * i for i in range(1, batch_size + 1)], device=device),\n",
    "        'condition': ['ScanNet'] * batch_size,\n",
    "        'grid_size': torch.tensor([0.01], device=device),\n",
    "        'segment': torch.randint(low=0, high=n_classes-1, size=(num_points * batch_size,), device=device)\n",
    "    }\n",
    "\n",
    "\n",
    "def patch_cfg(cfg: dict, repo_root: Path = repo_root) -> dict:\n",
    "    cfg = cfg.copy()\n",
    "    cfg[\"my_data_root\"] = repo_root / cfg[\"my_data_root\"]\n",
    "    cfg[\"weight\"] = repo_root / cfg[\"weight\"]\n",
    "    cfg[\"batch_size_test_per_gpu\"] = 1\n",
    "    return cfg\n",
    "\n",
    "\n",
    "repo_root = Path(\"../..\")\n",
    "cfg_file = Path(\"../../test/custom-ppt-config.py\"); assert cfg_file.exists\n",
    "device = \"cuda\"\n",
    "\n",
    "args = default_argument_parser().parse_args(args=[\"--config-file\", f\"{cfg_file}\"])\n",
    "cfg = default_config_parser(args.config_file, args.options); cfg = patch_cfg(cfg)\n",
    "\n",
    "tester = TESTERS.build(dict(type=cfg.test.type, cfg=cfg))\n",
    "model = tester.model\n",
    "model.to(device)\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d4c725-2926-43b7-aed2-9f348530cc91",
   "metadata": {},
   "source": [
    "# Visualise netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f17df69-85bb-4481-9c82-d4b36bd058b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece78c4e-df11-4a38-9175-75309a337b1b",
   "metadata": {},
   "source": [
    "Now install netron and open this file:\n",
    "\n",
    "```bash\n",
    "snap install netron\n",
    "snap run netron\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92870c6-b661-422a-8542-a3b7f0e4bac7",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac3631-feea-49f9-9dc2-d8ac19297e51",
   "metadata": {},
   "source": [
    "### lora-pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf019940-a66c-402d-ae71-f164b94cf3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bare model:  110759388\n",
      "lora: 13312300\n"
     ]
    }
   ],
   "source": [
    "# lora_model = LoRA.from_module(model, rank=50)\n",
    "# print(\"bare model: \", count_trainable_parameters(model))\n",
    "# print(\"lora:\", count_trainable_parameters(lora_model))\n",
    "# torch.save(model, \"model_lora.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf183b0f-3960-4586-ace2-567cb7365ded",
   "metadata": {},
   "source": [
    "### minlora implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27230259-4488-4bb1-a727-34fe57fab909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before LoRA: {'trainable': 97447088, 'frozen': 1}\n",
      "after LoRA: {'trainable': 100761978, 'frozen': 1}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'named_lora_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m minlora\u001b[38;5;241m.\u001b[39madd_lora(model, lora_config\u001b[38;5;241m=\u001b[39mlora_config)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter LoRA:\u001b[39m\u001b[38;5;124m\"\u001b[39m, count_trainable_parameters(model))\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m count_lora_parameters(model) \u001b[38;5;241m==\u001b[39m \u001b[43mcount_lora_params_manual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m minlora\u001b[38;5;241m.\u001b[39mremove_lora(model)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mafter removing lora:\u001b[39m\u001b[38;5;124m\"\u001b[39m, count_trainable_parameters(model))\n",
      "Cell \u001b[0;32mIn[1], line 80\u001b[0m, in \u001b[0;36mcount_lora_params_manual\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount_lora_params_manual\u001b[39m(model):\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"just looking at weight tensor names manually as a cross check\"\"\"\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnamed_lora_params\u001b[49m(model))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'named_lora_params' is not defined"
     ]
    }
   ],
   "source": [
    "# optimizer\n",
    "def configure_optimizers_lora(\n",
    "    model,\n",
    "    weight_decay: float = 0.05,\n",
    "    learning_rate: float = 0.005,\n",
    "    betas: tuple[float, float] = (0.9, 0.999),\n",
    "    device_type: str = \"cuda\"\n",
    "):\n",
    "    # we apply weight decay to all lora params\n",
    "    optim_groups = [\n",
    "        # note: .get_lora_params() returns a generator\n",
    "        # we need to wrap it in a list so we can consume it twice\n",
    "        {\"params\": list(minlora.get_lora_params(model)) , \"weight_decay\": weight_decay},\n",
    "        # you can also add biases for fine-tuning,\n",
    "        # but I want to make sure lora alone works\n",
    "        # {\"params\": minlora.get_bias_params(model), \"weight_decay\": 0.0}, # bias params don't get weight decay\n",
    "    ]\n",
    "\n",
    "    def parameter_count(optim_groups):\n",
    "        n = sum(p.numel() for group in optim_groups for p in group[\"params\"])\n",
    "        if n < 1e6:\n",
    "            return f\"{n/1e3:.1f}k\"\n",
    "        else:\n",
    "            return f\"{n/1e6:.1f}M\"\n",
    "\n",
    "    logger.info(f\"optimizing {parameter_count(optim_groups)} parameters\")\n",
    "\n",
    "    # new PyTorch nightly has a new 'fused' option for AdamW that is much faster\n",
    "    use_fused = (device_type == \"cuda\") and (\"fused\" in inspect.signature(torch.optim.AdamW).parameters)\n",
    "    logger.info(f\"using fused AdamW: {use_fused}\")\n",
    "    extra_args = dict(fused=True) if use_fused else dict()\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optim_groups,\n",
    "        lr=learning_rate,\n",
    "        betas=betas,\n",
    "        **extra_args\n",
    "    )\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "lora_hparams = dict(\n",
    "    lora_dropout_p = 0.0,\n",
    "    rank=10,\n",
    "    lora_alpha = 64\n",
    ")\n",
    "\n",
    "lora_config = {\n",
    "    torch.nn.Embedding: {\n",
    "        \"weight\": partial(LoRAParametrization.from_embedding, **lora_hparams),\n",
    "    },\n",
    "    torch.nn.Linear: {\n",
    "        \"weight\": partial(LoRAParametrization.from_linear, **lora_hparams),\n",
    "    },\n",
    "    SubMConv3d: {\n",
    "        \"weight\": partial(LoRAParametrization.from_sparseconv3d, **lora_hparams),\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"before LoRA:\", count_trainable_parameters(model))\n",
    "named_trainable_initial = named_trainable_parameters(model)\n",
    "\n",
    "\n",
    "freeze_non_lora_params(model)\n",
    "\n",
    "minlora.add_lora(model, lora_config=lora_config)\n",
    "print(\"after LoRA:\", count_trainable_parameters(model))\n",
    "\n",
    "assert count_lora_parameters(model) == count_lora_params_manual(model)\n",
    "\n",
    "minlora.remove_lora(model)\n",
    "print(\"after removing lora:\", count_trainable_parameters(model))\n",
    "\n",
    "#unfreeze_all_params(model)\n",
    "#print(\"After unfreezing:\", count_trainable_parameters(model))\n",
    "# if use_lora:\n",
    "#     optimizer = configure_optimizers_lora(model, weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# else:\n",
    "#     optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
    "# if init_from == 'resume':\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c23b7b6-b23c-4918-b909-79a69b7183fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "named_trainable_initial[\"frozen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "845b9fa3-f784-4297-92ec-360fbee44d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for item in minlora.get_lora_params(model):\n",
    "    print(type(item))\n",
    "    print(item.requires_grad)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78e4c334-3f90-404f-b5ff-5985ffb321a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for item in get_params_by_name(model, name_filter=name_is_lora):\n",
    "    print(type(item))\n",
    "    print(item.requires_grad)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "387a27b6-a202-4b3e-8eef-61e47f23d6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b17674ac-cd66-442e-90f3-6e190a421211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.embedding.stem.conv.parametrizations.weight.0.lora_A\n",
      "Parameter containing:\n",
      "tensor([[ 0.0015, -0.0013, -0.0098,  ..., -0.0012, -0.0173,  0.0280],\n",
      "        [-0.0241,  0.0332,  0.0262,  ...,  0.0046, -0.0024, -0.0134],\n",
      "        [-0.0098, -0.0252,  0.0011,  ...,  0.0062, -0.0120,  0.0355],\n",
      "        ...,\n",
      "        [ 0.0060,  0.0095,  0.0280,  ...,  0.0277, -0.0291, -0.0006],\n",
      "        [-0.0261,  0.0200,  0.0330,  ..., -0.0213, -0.0310, -0.0165],\n",
      "        [-0.0139, -0.0235, -0.0101,  ...,  0.0103,  0.0028, -0.0225]],\n",
      "       device='cuda:0', requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n, p in named_lora_params(model):\n",
    "    print(n)\n",
    "    print(p, \"\\n\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d0d3f2e-09d9-4820-b789-3549bac93204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0015, -0.0013, -0.0098,  ..., -0.0012, -0.0173,  0.0280],\n",
       "         [-0.0241,  0.0332,  0.0262,  ...,  0.0046, -0.0024, -0.0134],\n",
       "         [-0.0098, -0.0252,  0.0011,  ...,  0.0062, -0.0120,  0.0355],\n",
       "         ...,\n",
       "         [ 0.0060,  0.0095,  0.0280,  ...,  0.0277, -0.0291, -0.0006],\n",
       "         [-0.0261,  0.0200,  0.0330,  ..., -0.0213, -0.0310, -0.0165],\n",
       "         [-0.0139, -0.0235, -0.0101,  ...,  0.0103,  0.0028, -0.0225]],\n",
       "        device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-2.4972e-02,  1.4469e-02, -5.3538e-03,  ...,  1.7494e-02,\n",
       "          -7.5978e-03, -4.9841e-03],\n",
       "         [-1.8805e-02, -1.4388e-02,  8.9047e-03,  ...,  8.3569e-05,\n",
       "           1.3736e-02,  4.8615e-03],\n",
       "         [ 2.1049e-02,  1.7829e-02,  7.9850e-03,  ...,  1.0299e-02,\n",
       "           2.2313e-02, -2.5554e-02],\n",
       "         ...,\n",
       "         [-2.2818e-02, -1.5946e-02,  2.6613e-02,  ..., -1.7899e-02,\n",
       "           1.7108e-02,  4.9381e-03],\n",
       "         [ 2.5884e-02,  2.6989e-02,  9.2511e-03,  ..., -3.9559e-03,\n",
       "          -2.3899e-02, -2.3583e-02],\n",
       "         [-2.1103e-02, -1.3243e-02, -6.2689e-03,  ..., -1.1380e-02,\n",
       "           2.6363e-02,  8.5329e-03]], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[-0.0196,  0.0828, -0.0645, -0.0890, -0.0984, -0.0772,  0.0877, -0.0307,\n",
       "           0.0710,  0.0654, -0.0199,  0.0675,  0.0621,  0.1332, -0.0110,  0.0580,\n",
       "           0.1260,  0.1004, -0.1371, -0.0276, -0.1376, -0.0642,  0.1386, -0.0698,\n",
       "          -0.1208, -0.1024,  0.0878, -0.0205,  0.0928, -0.0096, -0.0417, -0.0800,\n",
       "           0.1371,  0.1256,  0.1192, -0.1235, -0.0173, -0.1180,  0.1043,  0.0824,\n",
       "          -0.0214, -0.1154,  0.0081,  0.0633,  0.1022, -0.0288,  0.0486,  0.1121],\n",
       "         [ 0.0906, -0.0937, -0.0635, -0.0866, -0.0514, -0.0347, -0.0149, -0.0976,\n",
       "           0.0124, -0.1139, -0.1398, -0.0957,  0.0874,  0.0852, -0.0655,  0.0859,\n",
       "           0.0286, -0.1180,  0.0582,  0.0137,  0.1426, -0.1262, -0.0544,  0.0145,\n",
       "          -0.0654, -0.0794,  0.0533,  0.1229, -0.0569,  0.0412,  0.0537, -0.0822,\n",
       "           0.0691,  0.0028,  0.1016, -0.0249, -0.0738, -0.0645,  0.0207, -0.0454,\n",
       "           0.0359, -0.0967,  0.0994, -0.0158,  0.0216,  0.1319,  0.0791, -0.1426],\n",
       "         [ 0.1233,  0.0471, -0.0370, -0.0348,  0.0799,  0.1242, -0.1384,  0.0854,\n",
       "           0.1246, -0.0914,  0.0256, -0.0668,  0.1020, -0.0586, -0.1096, -0.0146,\n",
       "          -0.0452,  0.0549,  0.1283, -0.0397,  0.0724,  0.0623,  0.0801, -0.0575,\n",
       "          -0.0128,  0.1006, -0.0665,  0.0317, -0.1257,  0.0162,  0.1030, -0.0499,\n",
       "          -0.0552,  0.0826,  0.1240, -0.0151, -0.1366, -0.0795,  0.1443,  0.1282,\n",
       "           0.0360, -0.0308, -0.1170,  0.0484, -0.0758,  0.0938, -0.0390, -0.0545],\n",
       "         [ 0.1003, -0.0522, -0.1370,  0.0572, -0.0821,  0.0556,  0.0829, -0.0067,\n",
       "          -0.1053,  0.1052, -0.0934, -0.1136, -0.1408, -0.0732, -0.0723, -0.1302,\n",
       "          -0.0885, -0.0197, -0.0783,  0.0115, -0.0349, -0.1238, -0.0632, -0.1201,\n",
       "          -0.0957, -0.1043,  0.0038, -0.0215,  0.1120,  0.1315, -0.0533,  0.0907,\n",
       "           0.0821,  0.0291,  0.0178,  0.1212, -0.1040, -0.0452, -0.0413,  0.0194,\n",
       "           0.1255,  0.1144,  0.1349,  0.0588,  0.0958, -0.1107,  0.0360, -0.1148],\n",
       "         [-0.0255, -0.0184, -0.1135,  0.0472, -0.1283, -0.0391, -0.1097, -0.1158,\n",
       "           0.0845, -0.0328, -0.0196,  0.1033, -0.0799,  0.1230, -0.1417, -0.0818,\n",
       "           0.0574, -0.0162,  0.1177,  0.1175,  0.1021,  0.0152,  0.1191,  0.0299,\n",
       "          -0.0859,  0.0660,  0.1074, -0.0435,  0.0097,  0.1361, -0.0766,  0.0031,\n",
       "          -0.0514, -0.1226,  0.1113, -0.0389, -0.0132, -0.0291,  0.1423,  0.1435,\n",
       "           0.0265, -0.1009,  0.1181, -0.0699, -0.0998,  0.1111, -0.1167, -0.0472],\n",
       "         [ 0.0851, -0.0709,  0.0637,  0.0325,  0.0827,  0.0781,  0.0471,  0.0094,\n",
       "          -0.1292,  0.1130,  0.0877,  0.0089,  0.1310,  0.1254,  0.0316,  0.0007,\n",
       "           0.0831, -0.0806,  0.0604,  0.1299, -0.0177,  0.1357, -0.0452, -0.0090,\n",
       "          -0.0210,  0.0924,  0.0116, -0.1194, -0.0576,  0.0455,  0.0809, -0.1092,\n",
       "           0.1029,  0.0371, -0.1303, -0.0840, -0.1283,  0.0187, -0.1181, -0.0995,\n",
       "           0.0281, -0.0740, -0.0324, -0.0189, -0.1226,  0.0806,  0.1436,  0.0200],\n",
       "         [-0.0963, -0.0998,  0.0462, -0.0036,  0.0397, -0.1395,  0.0325, -0.0201,\n",
       "           0.1211,  0.0451,  0.0218, -0.1223,  0.1425,  0.0547, -0.0517, -0.0018,\n",
       "           0.1385, -0.0971, -0.1315, -0.1375,  0.1009, -0.1342,  0.1305, -0.0702,\n",
       "           0.0684, -0.0765, -0.0214,  0.0851, -0.1082, -0.0100,  0.0412, -0.0186,\n",
       "           0.1215, -0.1083, -0.0329, -0.0204, -0.0917, -0.0412, -0.0123, -0.0448,\n",
       "           0.0851, -0.0329, -0.1066, -0.0210,  0.1338, -0.0025, -0.1276,  0.1106],\n",
       "         [ 0.0878,  0.1247, -0.0836,  0.0377,  0.0256, -0.0885, -0.1003, -0.0852,\n",
       "          -0.0060, -0.0955,  0.1200,  0.0488,  0.0600,  0.0109, -0.1413,  0.0884,\n",
       "           0.1247,  0.0513,  0.0775,  0.0632, -0.0312,  0.1096, -0.1281, -0.0665,\n",
       "          -0.1442, -0.0232, -0.0769,  0.1356,  0.1301, -0.0100,  0.0877,  0.0614,\n",
       "           0.1270,  0.1177,  0.0085, -0.0056, -0.1334,  0.0595,  0.1264,  0.0611,\n",
       "           0.0095,  0.1189,  0.0964,  0.1406, -0.0667, -0.0163,  0.1222, -0.0114],\n",
       "         [-0.1415,  0.0389, -0.0300,  0.0235,  0.0328,  0.0863,  0.1312,  0.0133,\n",
       "           0.0323, -0.1402,  0.0246,  0.1429,  0.0234,  0.0152, -0.0514,  0.0081,\n",
       "           0.0921,  0.0647,  0.1239, -0.0580,  0.0827,  0.0041,  0.0350, -0.1055,\n",
       "          -0.0753,  0.0743,  0.0603,  0.0785,  0.0419,  0.0449, -0.1392, -0.0731,\n",
       "           0.1198,  0.0571, -0.1297,  0.0880, -0.0827, -0.0171,  0.0746, -0.1240,\n",
       "           0.1176,  0.0100,  0.1074, -0.0972,  0.0035,  0.0550,  0.0504,  0.0139],\n",
       "         [-0.1012,  0.0852,  0.1057, -0.0549, -0.1428, -0.0888, -0.0602,  0.0505,\n",
       "          -0.0942,  0.0682,  0.0906,  0.0677,  0.1138, -0.0667, -0.0917,  0.0119,\n",
       "           0.0958, -0.1224, -0.1167, -0.0767,  0.0127,  0.1176,  0.0225,  0.0376,\n",
       "          -0.1112, -0.0165, -0.1113, -0.0897,  0.0323,  0.0097,  0.0293,  0.0906,\n",
       "           0.0395,  0.1412,  0.0012, -0.0448,  0.1183, -0.1079, -0.0936,  0.1110,\n",
       "           0.0725,  0.0306, -0.0642,  0.1037,  0.0713, -0.1280, -0.0493, -0.1071]],\n",
       "        device='cuda:0', requires_grad=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_params_by_name(model, name_filter=name_is_lora))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45a40d36-8faf-4934-8b1a-2b0c18339d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params_by_name(model, print_shapes=False, name_filter=None):\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_filter is None or name_filter(n):\n",
    "            if print_shapes:\n",
    "                print(n, p.shape)\n",
    "            yield p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1152d216-dd38-4e38-a987-0dc2bb04647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_lora_trainable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7004cf3-0a9f-4b45-b92a-d8d5725d5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = create_spoofed_input(device=\"cuda\", batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "544ececb-6201-434a-81fa-130646d1e39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:optimizing 3.3M parameters\n",
      "INFO:__main__:using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 0.05\n",
    "learning_rate = 0.005\n",
    "beta1, beta2 = 0.9, 0.999#0.95\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "\n",
    "optimizer = configure_optimizers_lora(\n",
    "    model,\n",
    "    weight_decay,\n",
    "    learning_rate,\n",
    "    (beta1, beta2),\n",
    "    device_type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9e934-62e0-4b3f-bffe-4c7a2d05e074",
   "metadata": {},
   "source": [
    "test backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205909ba-1de1-4e2f-854d-e760b47e7561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** First Pass ***\n",
      "Initial gradients: A: 0/195, B: 194/195\n",
      "Trainable parameters with gradients: 808,320\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "Total A matrices without gradients: 195\n",
      "Total B matrices without gradients: 1\n",
      "\n",
      "Gradients after step 1:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 2:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 3:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 4:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n",
      "\n",
      "Gradients after step 5:\n",
      "A: 194/195, B: 194/195\n",
      "Trainable parameters with gradients: 3,312,300\n",
      "Frozen parameters: 97,447,089\n",
      "Total parameters: 100,761,979\n",
      "A matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_A']\n",
      "B matrices without gradients: ['embedding_table.parametrizations.weight.0.lora_B']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from minlora import LoRAParametrization\n",
    "\n",
    "\n",
    "def inspect_lora_gradients(model, x, num_steps=5):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "    def check_grads():\n",
    "        a_no_grad, b_no_grad = [], []\n",
    "        a_with_grad, b_with_grad = 0, 0\n",
    "        total_a, total_b = 0, 0\n",
    "        trainable_params_with_grad = 0\n",
    "        frozen_params = 0\n",
    "        total_params = 0\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            total_params += param.numel()\n",
    "            if not param.requires_grad:\n",
    "                frozen_params += param.numel()\n",
    "            elif param.grad is not None and torch.any(param.grad != 0):\n",
    "                trainable_params_with_grad += param.numel()\n",
    "\n",
    "            if 'lora_A' in name:\n",
    "                total_a += 1\n",
    "                if param.grad is None or torch.all(param.grad == 0):\n",
    "                    a_no_grad.append(name)\n",
    "                else:\n",
    "                    a_with_grad += 1\n",
    "            elif 'lora_B' in name:\n",
    "                total_b += 1\n",
    "                if param.grad is None or torch.all(param.grad == 0):\n",
    "                    b_no_grad.append(name)\n",
    "                else:\n",
    "                    b_with_grad += 1\n",
    "\n",
    "        return (a_with_grad, b_with_grad, total_a, total_b, a_no_grad, b_no_grad, \n",
    "                trainable_params_with_grad, frozen_params, total_params)\n",
    "\n",
    "    # Initial forward and backward pass\n",
    "    y = model(x)\n",
    "    loss = y[\"loss\"].sum()\n",
    "    loss.backward()\n",
    "    \n",
    "    results = check_grads()\n",
    "    (\n",
    "        a_grad,\n",
    "        b_grad,\n",
    "        total_a,\n",
    "        total_b,\n",
    "        a_no_grad,\n",
    "        b_no_grad,\n",
    "        trainable_grad,\n",
    "        frozen,\n",
    "        total\n",
    "    ) = results\n",
    "\n",
    "    print(\"*** First Pass ***\")\n",
    "    print(f\"Initial gradients: A: {a_grad}/{total_a}, B: {b_grad}/{total_b}\")\n",
    "    print(f\"Trainable parameters with gradients: {trainable_grad:,}\")\n",
    "    print(f\"Frozen parameters: {frozen:,}\")\n",
    "    print(f\"Total parameters: {total:,}\")\n",
    "    if a_no_grad:\n",
    "        print(f\"Total A matrices without gradients: {len(a_no_grad)}\")\n",
    "    if b_no_grad:\n",
    "        print(f\"Total B matrices without gradients: {len(b_no_grad)}\")\n",
    "\n",
    "    # Perform several optimization steps\n",
    "    for i in range(num_steps):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y = model(x)\n",
    "        loss = y[\"loss\"].sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        results = check_grads()\n",
    "        a_grad, b_grad, total_a, total_b, a_no_grad, b_no_grad, trainable_grad, frozen, total = results\n",
    "\n",
    "        print(f\"\\nGradients after step {i+1}:\")\n",
    "        print(f\"A: {a_grad}/{total_a}, B: {b_grad}/{total_b}\")\n",
    "        print(f\"Trainable parameters with gradients: {trainable_grad:,}\")\n",
    "        print(f\"Frozen parameters: {frozen:,}\")\n",
    "        print(f\"Total parameters: {total:,}\")\n",
    "        if a_no_grad:\n",
    "            print(f\"A matrices without gradients: {a_no_grad}\")\n",
    "        if b_no_grad:\n",
    "            print(f\"B matrices without gradients: {b_no_grad}\")\n",
    "            \n",
    "X = create_spoofed_input(device=\"cuda\", batch_size=16)\n",
    "inspect_lora_gradients(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18bc7e3-4dcf-4b95-b1e7-23067c80a7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4582c4b7-2f5a-4d6c-a8b0-965af4a508f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff3c868-354c-4251-9d3c-de175ac71b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module backbone.enc.enc0.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc0.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc1.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc2.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block3.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block4.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc3.block5.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.down.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.enc.enc4.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec3.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec2.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec1.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.up.proj.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.up.proj_skip.0:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block0.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block1.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.cpe.1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.attn.qkv:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.attn.proj:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.mlp.0.fc1:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module backbone.dec.dec0.block2.mlp.0.fc2:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n",
      "Module proj_head:\n",
      "  - weight LoRA parameters:\n",
      "    - original: device = cuda:0\n",
      "    - 0.lora_A: device = cuda:0\n",
      "    - 0.lora_B: device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "def showlora(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d, nn.MultiheadAttention)):\n",
    "            print(f\"Module {name}:\")\n",
    "            if hasattr(module, 'parametrizations'):\n",
    "                for param_name, param in module.parametrizations.items():\n",
    "                    print(f\"  - {param_name} LoRA parameters:\")\n",
    "                    for lora_name, lora_param in param.named_parameters():\n",
    "                        print(f\"    - {lora_name}: device = {lora_param.device}\")\n",
    "            elif isinstance(module, nn.MultiheadAttention):\n",
    "                if hasattr(module.out_proj, 'parametrizations'):\n",
    "                    for param_name, param in module.out_proj.parametrizations.items():\n",
    "                        print(f\"  - out_proj.{param_name} LoRA parameters:\")\n",
    "                        for lora_name, lora_param in param.named_parameters():\n",
    "                            print(f\"    - {lora_name}: device = {lora_param.device}\")\n",
    "\n",
    "showlora(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd82726-a2ce-4fc5-bcb2-54535a6d7460",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Serialization of parametrized modules is only supported through state_dict(). See:\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_minlora.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/serialization.py:629\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 629\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/serialization.py:841\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    839\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[1;32m    840\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[0;32m--> 841\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    843\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/torch/nn/utils/parametrize.py:305\u001b[0m, in \u001b[0;36m_inject_new_class.<locals>.getstate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetstate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSerialization of parametrized modules is only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported through state_dict(). See:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/tutorials/beginner/saving_loading_models.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Serialization of parametrized modules is only supported through state_dict(). See:\nhttps://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"model_minlora.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7011af-0d97-46b4-94a0-e4b2707e46c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba4a57-ab4c-47d7-b9bf-bb32267bab67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8478a-94cd-41cc-bb66-168c02e22551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca56452-a63a-41a3-ad42-a7afc0df6d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd04c52-fdca-464d-943c-3373fc528132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ef7074-15d7-463d-942b-c6f1facae827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18bd2de-5b12-45c3-9f35-abcdc24c64b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2516088c-9a06-457e-8b55-df4ca94f2d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
