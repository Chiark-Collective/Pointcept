{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f478db6-17fc-4d56-8a26-ea8589f34645",
   "metadata": {},
   "source": [
    "# TODO: put me in library code plx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582bebf7-a509-410c-b4b1-be7cdb4a4cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import typing as ty\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import xarray as xr\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    f1_score\n",
    ")\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def clf_report(y_true: pd.Series | np.ndarray, y_pred: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    assert isinstance(report, dict)\n",
    "    return pd.DataFrame(report).transpose()\n",
    "\n",
    "\n",
    "def save_classification_report(\n",
    "    y_true: pd.Series,\n",
    "    y_pred: pd.Series,\n",
    "    output_dir: Path,\n",
    "    log_mlflow: bool = False\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Generates a classification report and saves it to a CSV file.\n",
    "\n",
    "    :param y_true: Series containing the true labels.\n",
    "    :param y_pred: Series containing the predicted labels.\n",
    "    :param encoder: HierarchicalEncoder object used for label encoding.\n",
    "    :param classification_level: The level of classification used by the encoder.\n",
    "    :param output_dir: Directory to save the classification report.\n",
    "    :return: Path to the saved classification report CSV file.\n",
    "    \"\"\"\n",
    "    report_df = clf_report(y_true, y_pred)\n",
    "    report_csv_path = output_dir / 'classification_report.csv'\n",
    "    report_df.to_csv(report_csv_path, index=True)\n",
    "    if log_mlflow:\n",
    "        # Logging all metrics in classification_report\n",
    "        mlflow.log_metric(\"accuracy\", report.pop(\"accuracy\"))\n",
    "        for class_or_avg, metrics_dict in report.items():\n",
    "            for metric, value in metrics_dict.items():\n",
    "                mlflow.log_metric(class_or_avg + '_' + metric,value)\n",
    "    return report_csv_path\n",
    "\n",
    "\n",
    "# Function to update indices using Pandas Index intersection\n",
    "def update_indices_pandas(indices, sampled_indices):\n",
    "    return pd.Index(indices).intersection(sampled_indices)\n",
    "\n",
    "\n",
    "def precision_recall_dataset(\n",
    "    Y_true: pd.Series | np.ndarray,\n",
    "    Y_pred: pd.Series | np.ndarray,\n",
    "    pos_label = None\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Builds a PR curve dataset from predictions and ground truth\n",
    "    \"\"\"\n",
    "    precision, recall, thresh_pr = precision_recall_curve(Y_true, Y_pred, pos_label=pos_label)\n",
    "    average_precision = average_precision_score(Y_true, Y_pred, pos_label=pos_label)\n",
    "    ds_pr = pd.DataFrame(\n",
    "        data=np.stack([precision, recall], axis=-1),\n",
    "        columns=[\"Precision\", \"Recall\"],\n",
    "    ).to_xarray().rename(index=\"pr_curve_index\")\n",
    "    ds_pr[\"thresh_pr\"] = (\n",
    "        \"pr_curve_index\",\n",
    "        np.pad(\n",
    "            thresh_pr,\n",
    "            pad_width=(1, 0),\n",
    "            mode=\"constant\",\n",
    "            constant_values=0.\n",
    "        )\n",
    "    )\n",
    "    ds_pr[\"F1_score\"] = (\"pr_curve_index\", 2*precision*recall / (precision+recall))\n",
    "    ds_pr[\"AP\"] = average_precision\n",
    "    return ds_pr\n",
    "\n",
    "\n",
    "def roc_curve_dataset(\n",
    "    Y_true: pd.Series | np.ndarray,\n",
    "    Y_pred: pd.Series | np.ndarray,\n",
    "    pos_label=None\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Builds a ROC curve dataset from predictions and ground truth\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresh_roc = roc_curve(Y_true, Y_pred, pos_label=pos_label)\n",
    "    auc_score = roc_auc_score(Y_true, Y_pred)\n",
    "    ds_roc = pd.DataFrame(\n",
    "        data=np.stack([fpr, tpr], axis=-1),\n",
    "        columns=[\"FPR\", \"TPR\"]\n",
    "    ).to_xarray().rename(index=\"roc_curve_index\")\n",
    "    ds_roc[\"thresh_roc\"] = (\n",
    "        \"roc_curve_index\",\n",
    "        thresh_roc\n",
    "    )\n",
    "    ds_roc[\"balanced_accuracy\"] = (\"roc_curve_index\", (tpr + (1-fpr))/2)\n",
    "    ds_roc[\"AUC\"] = auc_score\n",
    "    return ds_roc\n",
    "\n",
    "\n",
    "def confusion_matrix_dataframe(\n",
    "    y_pred: np.ndarray | pd.Series,\n",
    "    y_true: np.ndarray | pd.Series,\n",
    "    classes: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a confusion matrix and embed it into a DataFrame\n",
    "    \n",
    "    Calculates precision/recall/specificity/NPV and pred/real totals\n",
    "    \"\"\"\n",
    "    # make cm, swapping pred/true to get our preferred convention (opposite to sklearn)\n",
    "    cm = confusion_matrix(y_true=y_pred, y_pred=y_true)\n",
    "    # stick the CM in a dataframe\n",
    "    df_cm = pd.DataFrame(index=classes, columns=classes, data=cm)\n",
    "    # calculate total predicted/true in each class\n",
    "    df_cm.loc[\"total\", :] = df_cm.sum()\n",
    "    df_cm['total'] = df_cm.sum(axis=1)\n",
    "    # calculate recalls and precisions for each class\n",
    "    for i in range(len(classes)):\n",
    "        rec = np.round(df_cm.iloc[i, i]/df_cm.loc[\"total\", df_cm.columns[i]],2)\n",
    "        prec = np.round(df_cm.iloc[i, i]/df_cm.loc[df_cm.index[i], 'total'],2)\n",
    "        df_cm.loc['recall', df_cm.columns[i]] = rec\n",
    "        df_cm.loc[df_cm.index[i], 'precision'] = prec\n",
    "    return df_cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd48c21a-3814-4241-b47e-632146cb358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve\n",
    ")\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(\n",
    "    y_true: pd.Series,\n",
    "    y_pred: pd.Series,\n",
    "    output_dir: Path = Path(\".\"),\n",
    "    file_name: str | None = \"confusion_matrix.png\"\n",
    ") -> Path | go.Figure:\n",
    "    \"\"\"\n",
    "    Generates and saves the confusion matrix plot to a file.\n",
    "\n",
    "    :param y_true: Series containing the true labels.\n",
    "    :param y_pred: Series containing the predicted labels.\n",
    "    :param output_dir: Directory to save the confusion matrix plot.\n",
    "    :return: Path to the saved confusion matrix plot file.\n",
    "    \"\"\"\n",
    "    logger.info(\"Plot confusion matrix\")\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    ConfusionMatrixDisplay.from_predictions(y_true, y_pred, ax=ax)\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "    if file_name is None:\n",
    "        return fig\n",
    "    confusion_matrix_path = output_dir / file_name \n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    plt.close()\n",
    "    \n",
    "    return confusion_matrix_path\n",
    "\n",
    "\n",
    "def plot_predicted_vs_real_counts(\n",
    "    y_true: pd.Series,\n",
    "    y_pred: pd.Series,\n",
    "    output_dir: Path  = Path(\".\"),\n",
    "    file_name: str | None = \"predicted_vs_real_counts.png\"\n",
    ") -> Path | go.Figure:\n",
    "    \"\"\"\n",
    "    Plots and saves the predicted vs real counts.\n",
    "\n",
    "    :param y_true: Series containing the true labels.\n",
    "    :param y_pred: Series containing the predicted labels.\n",
    "    :param encoder: HierarchicalEncoder object used for label encoding.\n",
    "    :param classification_level: The level of classification used by the encoder.\n",
    "    :param output_dir: Directory to save the plot.\n",
    "    :return: Path to the saved plot file.\n",
    "    \"\"\"\n",
    "    logger.info(\"Plot counts\")\n",
    "    value_counts = (\n",
    "        pd.concat(\n",
    "            [\n",
    "                y_true.value_counts().rename(\"true\"),\n",
    "                y_pred.value_counts().rename(\"predicted\")\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        .stack()\n",
    "        .reset_index()\n",
    "        .rename(\n",
    "            {\n",
    "                \"level_0\": \"class\",\n",
    "                \"level_1\": \"set\",\n",
    "                0: \"count\"\n",
    "            },\n",
    "            axis=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig = px.bar(\n",
    "        value_counts,\n",
    "        x=\"class\",\n",
    "        y=\"count\",\n",
    "        color=\"set\",\n",
    "        barmode=\"group\",\n",
    "        title=\"Predicted vs real counts\"\n",
    "    )\n",
    "    if file_name is None:\n",
    "        return fig\n",
    "    counts_plot_path = output_dir / file_name \n",
    "    fig.write_image(str(counts_plot_path), engine=\"kaleido\")\n",
    "   \n",
    "    return counts_plot_path\n",
    "\n",
    "\n",
    "def plot_multiclass_roc_auc_save(\n",
    "    y_true, y_proba, output_dir: Path = Path(\".\"), file_name: str | None = \"roc_curve.png\"\n",
    ") -> Path | go.Figure:\n",
    "    \"\"\"\n",
    "    Generates, saves, and returns the ROC curve plot with AUC for multi-class.\n",
    "\n",
    "    :param y_true: Series containing the true labels.\n",
    "    :param y_proba: DataFrame containing the predicted probabilities.\n",
    "    :param output_dir: Directory to save the ROC curve plot.\n",
    "    :param file_name: File name for saving the plot.\n",
    "    :return: Path to the saved ROC curve plot file.\n",
    "    \"\"\"\n",
    "    logger.info(\"Plot multiclass ROC\")\n",
    "    # Create a figure for the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    for i, class_label in enumerate(y_proba.columns):\n",
    "        fpr, tpr, _ = roc_curve(y_true == class_label, y_proba[class_label])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax.plot(fpr, tpr, label=f'ROC curve of class {class_label} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title('Receiver Operating Characteristic (ROC) with AUC for Multi-Class')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    if file_name is None:\n",
    "        return fig\n",
    "    \n",
    "    # Save the plot to the specified directory\n",
    "    roc_curve_path = output_dir / file_name\n",
    "    fig.savefig(roc_curve_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return roc_curve_path\n",
    "\n",
    "def plot_multiclass_pr_curve_save_with_auc(\n",
    "    y_true: pd.Series, \n",
    "    y_proba: pd.DataFrame, \n",
    "    output_dir: Path = Path(\".\"), \n",
    "    file_name: str | None = \"pr_curve.png\"\n",
    ") -> Path | go.Figure:\n",
    "    \"\"\"\n",
    "    Generates, saves, and returns the Precision-Recall curve plot with AUC for multi-class.\n",
    "\n",
    "    :param y_true: Series containing the true labels.\n",
    "    :param y_proba: DataFrame containing the predicted probabilities.\n",
    "    :param output_dir: Directory to save the PR curve plot.\n",
    "    :param file_name: File name for saving the plot.\n",
    "    :return: Path to the saved PR curve plot file.\n",
    "    \"\"\"\n",
    "    logger.info(\"Plot multiclass PR curve\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "    # Compute and plot PR curve and AUC for each class\n",
    "    for i, class_label in enumerate(y_proba.columns):\n",
    "        precision, recall, _ = precision_recall_curve(y_true == class_label, y_proba[class_label])\n",
    "        pr_auc = average_precision_score(y_true == class_label, y_proba[class_label])\n",
    "        ax.plot(recall, precision, label=f'Class {class_label} (AUC PR = {pr_auc:.2f})')\n",
    "\n",
    "    ax.set_xlabel('Recall')\n",
    "    ax.set_ylabel('Precision')\n",
    "    ax.set_title('Precision-Recall Curve for Multi-Class with AUC')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "    if file_name is None:\n",
    "        return fig\n",
    "    \n",
    "    # Save the plot\n",
    "    pr_curve_path = output_dir / file_name\n",
    "    fig.savefig(pr_curve_path)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return pr_curve_path\n",
    "\n",
    "\n",
    "def evaluate_hard_label_metrics(\n",
    "    y_true: pd.Series, \n",
    "    y_pred: pd.Series, \n",
    "    metrics: list[str] = [\n",
    "        \"accuracy\",\n",
    "        \"balanced_accuracy\",\n",
    "        \"f1_macro\",\n",
    "        \"f1_micro\",\n",
    "        \"f1_weighted\",\n",
    "        \"precision_macro\",\n",
    "        \"precision_micro\",\n",
    "        \"precision_weighted\",\n",
    "        \"recall_macro\",\n",
    "        \"recall_micro\",\n",
    "        \"recall_weighted\"\n",
    "    ]\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluates various metrics using hard labels.\n",
    "\n",
    "    :param y_true: Series containing the true labels.\n",
    "    :param y_pred: Series containing the predicted labels.\n",
    "    :param metrics: List of metric names to evaluate.\n",
    "    :return: Dictionary with metric names as keys and their corresponding scores as values.\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluate hard label metrics\")\n",
    "    results = {}\n",
    "    for metric in metrics:\n",
    "        logger.info(f\"Evaluating {metric=}\")\n",
    "        if metric == \"accuracy\":\n",
    "            results[metric] = accuracy_score(y_true, y_pred)\n",
    "        elif metric == \"balanced_accuracy\":\n",
    "            results[metric] = balanced_accuracy_score(y_true, y_pred)\n",
    "        elif metric.startswith(\"f1\"):\n",
    "            average_type = metric.split(\"_\")[1]  # e.g., macro, micro, weighted\n",
    "            results[metric] = f1_score(y_true, y_pred, average=average_type)\n",
    "        elif metric.startswith(\"precision\"):\n",
    "            average_type = metric.split(\"_\")[1]\n",
    "            results[metric] = precision_score(y_true, y_pred, average=average_type, zero_division=0)\n",
    "        elif metric.startswith(\"recall\"):\n",
    "            average_type = metric.split(\"_\")[1]\n",
    "            results[metric] = recall_score(y_true, y_pred, average=average_type, zero_division=0)\n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_probability_metrics(\n",
    "    y_true: pd.Series, \n",
    "    y_proba: pd.DataFrame, \n",
    "    metrics: list[str] = [\"roc_auc_ovr\", \"average_precision\"]\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluates various metrics using probability scores for multiclass classification.\n",
    "\n",
    "    :param y_true: Series containing the true labels.\n",
    "    :param y_proba: DataFrame containing the predicted probabilities for each class.\n",
    "    :param metrics: List of metric names to evaluate.\n",
    "    :return: Dictionary with metric names as keys and their corresponding scores as values.\n",
    "    \"\"\"\n",
    "    logger.info(\"Evaluate probability metrics\")\n",
    "    results = {}\n",
    "    y_true_cat = y_true\n",
    "\n",
    "    for metric in metrics:\n",
    "        if metric == \"roc_auc_ovr\":\n",
    "            # Handle binary classification differently\n",
    "            if y_proba.shape[1] == 2: \n",
    "                # Assuming the second column is the one of interest for binary classification\n",
    "                y_proba_binary = y_proba.iloc[:, 1]\n",
    "                results[metric] = roc_auc_score(y_true_cat, y_proba_binary)\n",
    "            else:\n",
    "                # Multiclass case\n",
    "                results[metric] = roc_auc_score(\n",
    "                    y_true_cat, y_proba, multi_class=\"ovr\", labels=sorted(y_proba.columns)\n",
    "                )\n",
    "        elif metric == \"average_precision\":\n",
    "            # Compute Average Precision for each class and take the mean\n",
    "            ap_scores = [\n",
    "                average_precision_score(\n",
    "                    y_true_cat == class_label,\n",
    "                    y_proba[class_label]\n",
    "                )\n",
    "                for class_label in y_proba.columns\n",
    "            ]\n",
    "            results[metric] = sum(ap_scores) / len(ap_scores)\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802aecd2-bcf8-4209-99a9-ee33692e5491",
   "metadata": {},
   "source": [
    "# Actual plotting shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f3f584d-fa27-4fef-8504-52c167f305b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "all_exp_dir = Path(\"../../exp\")\n",
    "heritage_exp_dir = all_exp_dir / \"heritage\"\n",
    "data_dir = Path(\"../../data\")\n",
    "\n",
    "experiment_names = [\n",
    "    \"library\",\n",
    "    \"maritime_and_park_v1\",\n",
    "    \"rog_and_foundry_v2\",\n",
    "    \"maritime_park_library_v1\",\n",
    "]\n",
    "\n",
    "dataset_names = [\n",
    "    \"library\",\n",
    "    \"maritime_and_park\",\n",
    "    \"rog_and_foundry\",\n",
    "    \"maritime_park_library\",\n",
    "]\n",
    "\n",
    "class_labels = pd.Series({\n",
    "    1 : \"wall\",\n",
    "    2 : \"floor\",\n",
    "    3 : \"roof\",\n",
    "    4 : \"ceiling\",\n",
    "    5 : \"footpath\",\n",
    "    6 : \"grass\",\n",
    "    7 : \"column\",\n",
    "    8 : \"door\",\n",
    "    9 : \"window\",\n",
    "    10 : \"stair\",\n",
    "    11 : \"railing\",\n",
    "    12 : \"rainwater_pipe\",\n",
    "    13 : \"other\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c378b94-dbb7-4d72-9923-fd56ad99ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_filename_to_scene_id(dataset_filename: str) -> str:\n",
    "    assert dataset_filename.endswith(\".pth\")\n",
    "    return dataset_filename[:-len(\".pth\")]\n",
    "\n",
    "def predictions_filename_to_scene_id(preds_filename: str) -> str:\n",
    "    assert preds_filename.endswith(\"_pred.npy\")\n",
    "    return preds_filename[:-len(\"_pred.npy\")]\n",
    "\n",
    "def load_predictions(exp_dir: Path) -> dict[str, np.ndarray]:\n",
    "    assert (files := list((exp_dir / \"result\").glob(\"*.npy\")))\n",
    "    return {predictions_filename_to_scene_id(f.name): np.load(f) for f in files}\n",
    "\n",
    "def load_clouds(dataset_dir: Path, subset: str | Path | None = \"test\") -> dict[str, torch.Tensor]:\n",
    "    glob_prefix = \"*\" if subset is None else f\"{subset}/\"\n",
    "    files = dataset_dir.glob(f\"{glob_prefix}*.pth\")\n",
    "    return {dataset_filename_to_scene_id(f.name): torch.load(f) for f in files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d5f2441-fbed-4417-9b5c-e4b5ed6ad08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_name='library'\n",
      "  scene_key='combined_library_test_sceneid2'\n",
      "exp_name='maritime_and_park_v1'\n",
      "  scene_key='combined_maritime_museum_test_sceneid2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liam/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/tmp/ipykernel_147635/3500622346.py:132: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prec = np.round(df_cm.iloc[i, i]/df_cm.loc[df_cm.index[i], 'total'],2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scene_key='combined_maritime_museum_test_sceneid3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liam/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/tmp/ipykernel_147635/3500622346.py:132: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prec = np.round(df_cm.iloc[i, i]/df_cm.loc[df_cm.index[i], 'total'],2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scene_key='combined_park_row_test_sceneid2'\n",
      "exp_name='rog_and_foundry_v2'\n",
      "  scene_key='combined_brass_foundry_test_sceneid2'\n",
      "  scene_key='combined_rog_south_test_sceneid2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147635/3500622346.py:131: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  rec = np.round(df_cm.iloc[i, i]/df_cm.loc[\"total\", df_cm.columns[i]],2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scene_key='combined_rog_south_test_sceneid3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147635/3500622346.py:131: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  rec = np.round(df_cm.iloc[i, i]/df_cm.loc[\"total\", df_cm.columns[i]],2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scene_key='combined_rog_north_test_sceneid2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liam/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/tmp/ipykernel_147635/3500622346.py:132: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prec = np.round(df_cm.iloc[i, i]/df_cm.loc[df_cm.index[i], 'total'],2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_name='maritime_park_library_v1'\n",
      "  scene_key='combined_maritime_museum_test_sceneid2'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liam/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/tmp/ipykernel_147635/3500622346.py:132: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prec = np.round(df_cm.iloc[i, i]/df_cm.loc[df_cm.index[i], 'total'],2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scene_key='combined_maritime_museum_test_sceneid3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liam/.cache/pypoetry/virtualenvs/pointcept-yysez_FY-py3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:2480: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
      "/tmp/ipykernel_147635/3500622346.py:132: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  prec = np.round(df_cm.iloc[i, i]/df_cm.loc[df_cm.index[i], 'total'],2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  scene_key='combined_park_row_test_sceneid2'\n"
     ]
    }
   ],
   "source": [
    "metrics = {}\n",
    "\n",
    "for exp_name, dataset_name in zip(experiment_names, dataset_names):\n",
    "    print(f\"{exp_name=}\")\n",
    "    metrics[dataset_name] = {}\n",
    "    exp_preds = load_predictions(heritage_exp_dir / exp_name)\n",
    "    exp_clouds = load_clouds(data_dir / dataset_name)\n",
    "    assert set(exp_preds.keys()) == set(exp_clouds.keys()) # all match up to common scene id\n",
    "    # collect individual scene arrays /results as we go to collect global metrics at the end\n",
    "    y_trues_all, y_preds_all = [], []\n",
    "    # evaluate per scene in the experiment\n",
    "    for scene_key in exp_preds.keys():\n",
    "        metrics[dataset_name][scene_key] = {}\n",
    "        print(f\"  {scene_key=}\")\n",
    "        y_pred = exp_preds[scene_key]; y_preds_all.append(y_pred)\n",
    "        y_true = exp_clouds[scene_key][\"gt\"]; y_trues_all.append(y_true)\n",
    "        unique_labels_pred = np.unique(y_pred)\n",
    "        unique_labels_true = np.unique(y_true)\n",
    "        unique_labels = set(unique_labels_pred).union(set(unique_labels_true))\n",
    "        metrics[dataset_name][scene_key] = dict(\n",
    "            hard_label_metrics = evaluate_hard_label_metrics(y_true, y_pred),\n",
    "            df_clf_report = clf_report(y_true, y_pred),\n",
    "            df_cm = confusion_matrix_dataframe(y_true, y_pred, classes=class_labels.loc[list(unique_labels)])\n",
    "        )\n",
    "    # now globally across all scenes in expt\n",
    "    y_true_all, y_pred_all = np.concatenate(y_trues_all), np.concatenate(y_preds_all)\n",
    "    unique_labels_pred_all = np.unique(y_pred_all)\n",
    "    unique_labels_true_all = np.unique(y_true_all)\n",
    "    unique_labels_all = set(unique_labels_pred_all).union(set(unique_labels_true_all))\n",
    "    metrics[dataset_name][\"global\"] = dict(\n",
    "        hard_label_metrics = evaluate_hard_label_metrics(y_true_all, y_pred_all),\n",
    "        df_clf_report = clf_report(y_true_all, y_pred_all),\n",
    "        df_cm = confusion_matrix_dataframe(y_true_all, y_pred_all, classes=class_labels.loc[list(unique_labels_all)])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8d698c-a446-4503-bab5-c43b7e5ca171",
   "metadata": {},
   "source": [
    "# Plot shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16d313e1-652f-4b49-8cd5-74a7c8273e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, dataset_metrics in metrics.items():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d24bbb-0524-4a2b-9c05-cf34209603db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
