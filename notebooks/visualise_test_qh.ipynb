{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d61c53-d178-48cf-81d3-7e2410c8dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing as ty\n",
    "from pathlib import Path\n",
    "\n",
    "import laspy\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pyntcloud import PyntCloud\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d3b4d8-e014-4790-8be9-cc91f523afc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to read PCD data - this needs to be implemented\n",
    "def read_pcd(file_path):\n",
    "    cloud = PyntCloud.from_file(\n",
    "        file_path\n",
    "    )\n",
    "    return cloud.points\n",
    "\n",
    "def create_pc_dataset(pc_dict: dict[str, ty.Any]) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Create an Xarray dataset from a point cloud dictionary.\n",
    "\n",
    "    Args:\n",
    "        pc_dict (Dict[str, Any]): A dictionary containing point cloud data with the following keys:\n",
    "            - 'coord': The point coordinates as a 2D numpy array of shape (num_points, 3).\n",
    "            - 'color': The point colors as a 2D numpy array of shape (num_points, 3).\n",
    "            - 'normal': The point normals as a 2D numpy array of shape (num_points, 3).\n",
    "            - 'semantic_gt20': The semantic labels (20 classes) as a 1D numpy array of shape (num_points,).\n",
    "            - 'semantic_gt200': The semantic labels (200 classes) as a 1D numpy array of shape (num_points,).\n",
    "            - 'instance_gt': The instance labels as a 1D numpy array of shape (num_points,).\n",
    "            - 'scene_id': The scene ID as a string.\n",
    "\n",
    "    Returns:\n",
    "        xr.Dataset: An Xarray dataset representing the point cloud data.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If any of the required keys are missing from the input dictionary.\n",
    "        ValueError: If the input arrays have inconsistent shapes or if the 'scene_id' is not a string.\n",
    "\n",
    "    Example:\n",
    "        pc_dict = {\n",
    "            'coord': np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]),\n",
    "            'color': np.array([[255, 0, 0], [0, 255, 0]]),\n",
    "            'normal': np.array([[0.0, 0.0, 1.0], [0.0, 1.0, 0.0]]),\n",
    "            'semantic_gt20': np.array([1, 2]),\n",
    "            'semantic_gt200': np.array([10, 20]),\n",
    "            'instance_gt': np.array([1, 1]),\n",
    "            'scene_id': 'scene_0001'\n",
    "        }\n",
    "        pc_dataset = create_pc_dataset(pc_dict)\n",
    "    \"\"\"\n",
    "    # Check if all required keys are present in the input dictionary\n",
    "    # required_keys = [\n",
    "    #     'coord', 'color', 'normal', 'semantic_gt20', 'semantic_gt200', \n",
    "    #     'instance_gt', 'scene_id'\n",
    "    # ]\n",
    "    required_keys = [\n",
    "        'coord', 'color', 'normal', 'semantic_gt20', 'scene_id'\n",
    "    ]\n",
    "    for key in required_keys:\n",
    "        if key not in pc_dict:\n",
    "            raise KeyError(f\"Missing required key '{key}' in the input dict\")\n",
    "\n",
    "    # Check if the input arrays have consistent shapes\n",
    "    num_points = len(pc_dict['coord'])\n",
    "    for key in ['color', 'normal']:\n",
    "        if pc_dict[key].shape != (num_points, 3):\n",
    "            raise ValueError(f\"Array '{key}' has an inconsistent shape. Expected ({num_points}, 3).\")\n",
    "    for key in ['semantic_gt20']: #, 'semantic_gt200', 'instance_gt']:\n",
    "        if pc_dict[key].shape != (num_points,):\n",
    "            raise ValueError(f\"Array '{key}' has an inconsistent shape. Expected ({num_points},).\")\n",
    "\n",
    "    # Create an Xarray dataset from the point cloud dictionary\n",
    "    pc_dataset = xr.Dataset(\n",
    "        data_vars=dict(\n",
    "            coord=(['point', 'coord_dim'], pc_dict['coord']),\n",
    "            color=(['point', 'color_dim'], pc_dict['color']),\n",
    "            normal=(['point', 'normal_dim'], pc_dict['normal']),\n",
    "            semantic_gt20=(['point'], pc_dict['semantic_gt20']),\n",
    "            # semantic_gt200=(['point'], pc_dict['semantic_gt200']),\n",
    "            # instance_gt=(['point'], pc_dict['instance_gt'])\n",
    "        ),\n",
    "        coords=dict(\n",
    "            point=np.arange(num_points),\n",
    "            coord_dim=['x', 'y', 'z'],\n",
    "            color_dim=['r', 'g', 'b'],\n",
    "            normal_dim=['nx', 'ny', 'nz']\n",
    "        ),\n",
    "        attrs=dict(scene_id=pc_dict['scene_id'])\n",
    "    )\n",
    "\n",
    "    return pc_dataset\n",
    "\n",
    "\n",
    "# Define the mappings from Polars to PCD data types\n",
    "POLARS_PCD_TYPE_MAPPINGS = [\n",
    "    (pl.Float32, ('F', 4)),\n",
    "    (pl.Float64, ('F', 8)),\n",
    "    (pl.UInt8, ('U', 1)),\n",
    "    (pl.UInt16, ('U', 2)),\n",
    "    (pl.UInt32, ('U', 4)),\n",
    "    (pl.UInt64, ('U', 8)),\n",
    "    (pl.Int16, ('I', 2)),\n",
    "    (pl.Int32, ('I', 4)),\n",
    "    (pl.Int64, ('I', 8)),\n",
    "]\n",
    "POLARS_TYPE_TO_PCD_TYPE = {dtype: mapping for dtype, mapping in POLARS_PCD_TYPE_MAPPINGS}\n",
    "\n",
    "\n",
    "def write_pcd(filename, data, metadata=None, batch_size=500000, binary=False):\n",
    "    \"\"\"\n",
    "    Writes a PCD file from a Polars DataFrame or LazyFrame in batches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        Path to the output PCD file.\n",
    "    data: Polars DataFrame or LazyFrame\n",
    "        DataFrame or LazyFrame containing the point cloud data.\n",
    "    metadata: dict, optional\n",
    "        Dictionary containing PCD metadata. If not provided, it will be\n",
    "        generated from the data.\n",
    "    batch_size: int, optional\n",
    "        Size of each batch to be processed.\n",
    "    binary: bool, optional\n",
    "        If True, writes the PCD file in binary format. Otherwise, writes in ASCII format.\n",
    "    \"\"\"\n",
    "    # Check if data is a LazyFrame\n",
    "    is_lazy = isinstance(data, pl.LazyFrame)\n",
    "\n",
    "    # Rename columns if they exist\n",
    "    # rename_map = {'X': 'x', 'Y': 'y', 'Z': 'z'}\n",
    "    # data = data.rename(rename_map)\n",
    "\n",
    "    # Fill nans for eigenentropy\n",
    "    if is_lazy:\n",
    "        data = data.with_columns([\n",
    "            pl.col(\"x\").cast(pl.Float32),  # open3d seems to req single precision float\n",
    "            pl.col(\"y\").cast(pl.Float32),\n",
    "            pl.col(\"z\").cast(pl.Float32)\n",
    "        ])\n",
    "        data = data.select(pl.exclude(pl.String))\n",
    "    else:\n",
    "        data = data.with_columns([\n",
    "            pl.col(\"x\").cast(pl.Float32),  # open3d seems to req single precision float\n",
    "            pl.col(\"y\").cast(pl.Float32),\n",
    "            pl.col(\"z\").cast(pl.Float32)\n",
    "        ])\n",
    "        data = data.select(pl.exclude(pl.Utf8))\n",
    "\n",
    "    if metadata is None:\n",
    "        n_rows = data.height if not is_lazy else data.select(pl.col(\"*\")).count().item()\n",
    "        # Generate metadata from the first batch of data\n",
    "        first_batch = data.limit(batch_size).collect() if is_lazy else data.head(batch_size)\n",
    "        metadata = {\n",
    "            'version': '.7',\n",
    "            'fields': first_batch.columns,\n",
    "            'size': [POLARS_TYPE_TO_PCD_TYPE[dtype][1] for dtype in first_batch.dtypes],\n",
    "            'type': [POLARS_TYPE_TO_PCD_TYPE[dtype][0] for dtype in first_batch.dtypes],\n",
    "            'count': [1] * len(first_batch.columns),\n",
    "            'width': n_rows,\n",
    "            'height': 1,\n",
    "            'viewpoint': [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n",
    "            'points': n_rows,\n",
    "            'data': 'binary' if binary else 'ascii'\n",
    "        }\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        # Write metadata\n",
    "        f.write(f\"VERSION {metadata['version']}\\n\".encode())\n",
    "        f.write(f\"FIELDS {' '.join(metadata['fields'])}\\n\".encode())\n",
    "        f.write(f\"SIZE {' '.join(map(str, metadata['size']))}\\n\".encode())\n",
    "        f.write(f\"TYPE {' '.join(metadata['type'])}\\n\".encode())\n",
    "        f.write(f\"COUNT {' '.join(map(str, metadata['count']))}\\n\".encode())\n",
    "        f.write(f\"WIDTH {metadata['width']}\\n\".encode())\n",
    "        f.write(f\"HEIGHT {metadata['height']}\\n\".encode())\n",
    "        f.write(f\"VIEWPOINT {' '.join(map(str, metadata['viewpoint']))}\\n\".encode())\n",
    "        f.write(f\"POINTS {metadata['points']}\\n\".encode())\n",
    "        f.write(f\"DATA {metadata['data']}\\n\".encode())\n",
    "\n",
    "        # Process and write data in batches\n",
    "        for start_row in range(0, metadata[\"points\"], batch_size):\n",
    "            batch_df = data.slice(start_row, batch_size).collect() if is_lazy else data[start_row:start_row+batch_size]\n",
    "            if binary:\n",
    "                # Write batch in binary format\n",
    "                data_buffer = batch_df.to_pandas().to_records(index=False).tobytes()\n",
    "                f.write(data_buffer)\n",
    "            else:\n",
    "                # Write batch in ASCII format\n",
    "                np.savetxt(f, batch_df.to_numpy(), fmt=' '.join(['%s'] * batch_df.width))\n",
    "\n",
    "    return filename\n",
    "\n",
    "def convert_pcd_to_las(\n",
    "    pcd_file_path: Path,\n",
    "    las_file_path: Path,\n",
    "    pred_col: str = \"pred\",\n",
    "    gt_col: str = \"gt\"\n",
    ") -> Path:\n",
    "    # Read the PCD file\n",
    "    pcd_data = read_pcd(str(pcd_file_path))\n",
    "\n",
    "    # Create a new LAS file header\n",
    "    header = laspy.LasHeader(version=\"1.4\", point_format=2)\n",
    "    \n",
    "    # Add extra dimensions for predicted_class and predicted_proba\n",
    "    header.add_extra_dim(laspy.ExtraBytesParams(name=\"predicted_class\", type=\"f\"))\n",
    "    header.add_extra_dim(laspy.ExtraBytesParams(name=\"gt_class\", type=\"f\"))\n",
    "    \n",
    "    # Create an empty LAS file with the header\n",
    "    las = laspy.LasData(header)\n",
    "\n",
    "    # Fill the LAS file with data\n",
    "    las.x, las.y, las.z = pcd_data['x'], pcd_data['y'], pcd_data['z']\n",
    "    if \"intensity\" in pcd_data.columns:\n",
    "        las.intensity = pcd_data['intensity']\n",
    "    las.red, las.green, las.blue = pcd_data[\"r\"], pcd_data[\"g\"], pcd_data[\"b\"]\n",
    "\n",
    "    # Set values for the extra dimensions\n",
    "    las.predicted_class = pcd_data[pred_col]\n",
    "    las.gt_class = pcd_data[gt_col]\n",
    "\n",
    "    # Write the LAS file to disk\n",
    "    las.write(las_file_path)\n",
    "    return las_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb3dd6-64fc-4b88-a567-042ae8fb37fe",
   "metadata": {},
   "source": [
    "Load the scenes and the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a572b0be-0e74-4008-b4b3-d0339689d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = Path(\"..\")\n",
    "\n",
    "scenes_dir = Path(\n",
    "    \"/data/sdd/qh/vox6res0.2/val\"\n",
    ")\n",
    "results_dir = Path(\n",
    "    \"/data/sdd/results_qh/vox6res0.2/result\"\n",
    ")\n",
    "\n",
    "predictions = {\n",
    "    path.stem.rstrip(\"_pred\"): np.load(path) \n",
    "    for path in results_dir.glob(\"*.npy\")\n",
    "}\n",
    "scenes = {\n",
    "    path.stem: torch.load(path) \n",
    "    for path in scenes_dir.glob(\"*.pth\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76408e2f-9e66-432b-9821-24710901387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa', 'table', 'door',\n",
    "    'window', 'bookshelf', 'picture', 'counter', 'desk', 'curtain',\n",
    "    'refridgerator', 'shower curtain', 'toilet', 'sink', 'bathtub',\n",
    "    'otherfurniture'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743b9bc-22cc-49d2-9920-a5d8a4b2e98a",
   "metadata": {},
   "source": [
    "Write everything out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "637712e6-5361-465b-b70a-8f3f99f5254b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write: voxel_0_2_0.las\n",
      "Write: voxel_0_2_1.las\n",
      "Write: voxel_0_2_2.las\n",
      "Write: voxel_0_2_3.las\n",
      "Write: voxel_0_2_4.las\n",
      "Write: voxel_0_3_0.las\n",
      "Write: voxel_0_3_1.las\n",
      "Write: voxel_0_3_2.las\n",
      "Write: voxel_0_3_3.las\n",
      "Write: voxel_0_3_4.las\n",
      "Write: voxel_1_0_0.las\n",
      "Write: voxel_1_0_1.las\n",
      "Write: voxel_1_0_2.las\n",
      "Write: voxel_1_0_3.las\n",
      "Write: voxel_1_2_0.las\n",
      "Write: voxel_1_2_1.las\n",
      "Write: voxel_1_2_2.las\n",
      "Write: voxel_1_2_3.las\n",
      "Write: voxel_1_2_4.las\n",
      "Write: voxel_1_3_0.las\n",
      "Write: voxel_1_3_1.las\n",
      "Write: voxel_1_3_2.las\n",
      "Write: voxel_1_3_3.las\n",
      "Write: voxel_1_3_4.las\n",
      "Write: voxel_2_0_0.las\n",
      "Write: voxel_2_0_1.las\n",
      "Write: voxel_2_0_2.las\n",
      "Write: voxel_2_0_3.las\n",
      "Write: voxel_2_0_4.las\n",
      "Write: voxel_2_2_1.las\n",
      "Write: voxel_2_2_2.las\n",
      "Write: voxel_2_2_3.las\n",
      "Write: voxel_2_2_4.las\n",
      "Write: voxel_2_2_5.las\n",
      "Write: voxel_2_3_0.las\n",
      "Write: voxel_2_3_1.las\n",
      "Write: voxel_2_3_2.las\n",
      "Write: voxel_2_3_3.las\n",
      "Write: voxel_2_3_4.las\n",
      "Write: voxel_2_3_5.las\n",
      "Write: voxel_2_4_0.las\n",
      "Write: voxel_2_4_1.las\n",
      "Write: voxel_3_2_0.las\n",
      "Write: voxel_3_2_1.las\n",
      "Write: voxel_3_2_3.las\n",
      "Write: voxel_3_2_4.las\n",
      "Write: voxel_3_4_0.las\n",
      "Write: voxel_3_4_1.las\n",
      "Write: voxel_4_0_0.las\n",
      "Write: voxel_4_0_1.las\n",
      "Write: voxel_4_0_2.las\n",
      "Write: voxel_4_0_3.las\n",
      "Write: voxel_4_0_4.las\n",
      "Write: voxel_4_0_5.las\n",
      "Write: voxel_4_1_1.las\n",
      "Write: voxel_4_1_2.las\n",
      "Write: voxel_4_1_3.las\n",
      "Write: voxel_4_1_4.las\n",
      "Write: voxel_4_2_0.las\n",
      "Write: voxel_1_0_4.las\n",
      "Write: voxel_2_2_0.las\n",
      "Write: voxel_3_2_2.las\n",
      "Write: voxel_4_2_1.las\n",
      "Write: voxel_4_2_2.las\n",
      "Write: voxel_4_2_3.las\n",
      "Write: voxel_4_2_4.las\n",
      "Write: voxel_4_2_5.las\n",
      "Write: voxel_5_0_0.las\n",
      "Write: voxel_5_0_1.las\n",
      "Write: voxel_5_0_2.las\n",
      "Write: voxel_5_0_3.las\n",
      "Write: voxel_5_0_4.las\n",
      "Write: voxel_5_0_5.las\n",
      "Write: voxel_5_1_1.las\n",
      "Write: voxel_5_1_2.las\n",
      "Write: voxel_5_1_3.las\n",
      "Write: voxel_5_1_4.las\n",
      "Write: voxel_5_3_0.las\n",
      "Write: voxel_5_3_1.las\n",
      "Write: voxel_5_3_2.las\n",
      "Write: voxel_5_3_3.las\n",
      "Write: voxel_5_3_4.las\n",
      "Write: voxel_5_5_0.las\n"
     ]
    }
   ],
   "source": [
    "for scene_id, pred in predictions.items():\n",
    "    points = scenes[scene_id]\n",
    "    labels = points[\"semantic_gt20\"]\n",
    "    ds = create_pc_dataset(points)\n",
    "    ds = ds.assign_coords(classes_scannet=xr.DataArray(np.array(classes), dims=[\"classes_scannet\"]))\n",
    "    ds[\"pred\"] =((\"point\",), pred)\n",
    "    ds[\"color\"] = ds[\"color\"].astype(np.uint8)\n",
    "    df = pd.concat(\n",
    "        [\n",
    "            ds[\"coord\"].to_pandas(),\n",
    "            ds[\"color\"].to_pandas(),\n",
    "            ds[\"pred\"].to_pandas().rename(\"pred\"),\n",
    "            ds[\"semantic_gt20\"].to_pandas().rename(\"gt\")\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "    lf = pl.from_pandas(df)\n",
    "    pcd_out = f\"{scene_id}.pcd\"\n",
    "    las_out = f\"{scene_id}.las\"\n",
    "    write_pcd(pcd_out, lf, binary=True)\n",
    "    print(f\"Write: {las_out}\")\n",
    "    convert_pcd_to_las(pcd_out, las_out)\n",
    "    os.remove(pcd_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fc8ddf-db02-4aa8-bdfa-812fa1e017a4",
   "metadata": {},
   "source": [
    "Confusion matrix stuff\n",
    "\n",
    "need to figure out labelling first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e90cba4-2d72-46cc-bb6b-17b7b5e6ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    df = pd.DataFrame(dict(pred=pred, gt=labels))\n",
    "    \n",
    "    # Extract the predicted and ground truth labels from the DataFrame\n",
    "    y_pred = df['pred']\n",
    "    y_true = df['gt']\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))  # Adjust the figure size as needed\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    \n",
    "    # Add a color bar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Define the class names\n",
    "    #classes = class_name\n",
    "    classes = [\n",
    "        'wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa', 'table', 'door',\n",
    "        'window', 'bookshelf', 'picture', 'counter', 'desk', 'curtain',\n",
    "        'refridgerator', 'shower curtain', 'toilet', 'sink', 'bathtub',\n",
    "        'otherfurniture'\n",
    "    ]\n",
    "    \n",
    "    # Set the tick labels and positions\n",
    "    ax.set(xticks=np.arange(len(classes)),\n",
    "           yticks=np.arange(len(classes)),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           xlabel='Predicted Labels',\n",
    "           ylabel='True Labels')\n",
    "    \n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Loop over data dimensions and create text annotations\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    # Set the title\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    \n",
    "    # Adjust the layout and display the plot\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd15ce0-d301-4dba-b34c-e8184d6eef1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m full_series\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Assuming your .npy files are stored in 'path_to_your_directory'\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m result_series \u001b[38;5;241m=\u001b[39m \u001b[43mnpy_to_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(result_series)\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mnpy_to_series\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     22\u001b[0m         series_list\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mSeries(data))  \u001b[38;5;66;03m# Append data as a pandas Series to the list\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Concatenate all series in the list into a single Series\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m full_series \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m full_series\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-1jndxwDi-py3.11/lib/python3.11/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-1jndxwDi-py3.11/lib/python3.11/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/pointcept-1jndxwDi-py3.11/lib/python3.11/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def npy_to_series(directory):\n",
    "    \"\"\"\n",
    "    Load all .npy files from the specified directory and collapse them into a single pandas Series.\n",
    "    \n",
    "    Args:\n",
    "    directory (str): The path to the directory containing .npy files.\n",
    "    \n",
    "    Returns:\n",
    "    pd.Series: A Pandas Series containing all elements from the .npy files.\n",
    "    \"\"\"\n",
    "    series_list = []  # Initialize an empty list to hold arrays from each file\n",
    "    \n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.npy'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            data = np.load(file_path)  # Load .npy file\n",
    "            series_list.append(pd.Series(data))  # Append data as a pandas Series to the list\n",
    "            \n",
    "    # Concatenate all series in the list into a single Series\n",
    "    full_series = pd.concat(series_list, ignore_index=True)\n",
    "    \n",
    "    return full_series\n",
    "\n",
    "# Example usage\n",
    "# Assuming your .npy files are stored in 'path_to_your_directory'\n",
    "result_series = npy_to_series(results_dir)\n",
    "print(result_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1da91b-b97a-4cab-bffb-e409b04eaf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_series.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886395b-c6b5-4b5f-83e8-18bd882c26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_series.value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef3a02-f8ec-4a1b-877b-57ddf4596005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pointcept)",
   "language": "python",
   "name": "pointcept"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
